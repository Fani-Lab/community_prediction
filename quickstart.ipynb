{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickstart.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --config python3 #3.6 due to dynamimcgem"
      ],
      "metadata": {
        "id": "faeGOx7swFSY",
        "outputId": "96eae877-db15-4d1a-cc81-c3968227b781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.7   2         auto mode\n",
            "  1            /usr/bin/python3.6   1         manual mode\n",
            "  2            /usr/bin/python3.7   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/bin/python3.6 to provide /usr/bin/python3 (python3) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3-pip\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install ipykernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hMhgG7g5OYpo",
        "outputId": "00ac2a84-d123-490f-d144-6af33abb2258"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pkg-resources python3-secretstorage python3-setuptools python3-six\n",
            "  python3-wheel python3-xdg\n",
            "Suggested packages:\n",
            "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
            "  python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
            "  python3-six python3-wheel python3-xdg\n",
            "0 upgraded, 15 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 2,882 kB of archives.\n",
            "After this operation, 8,886 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.5 [1,653 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.4 [220 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.5 [114 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-xdg all 0.25-4ubuntu1.1 [31.3 kB]\n",
            "Fetched 2,882 kB in 1s (2,945 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 15.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../01-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../02-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../03-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../04-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../05-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../06-python3-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../07-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../08-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../09-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../10-python3-pip_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../11-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../12-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../13-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../14-python3-xdg_0.25-4ubuntu1.1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pip\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/6d/6463d49a933f547439d6b5b98b46af8742cc03ae83543e4d7688c2420f8b/pip-21.3.1-py3-none-any.whl (1.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.7MB 718kB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 9.0.1\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "Successfully installed pip-21.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\n",
            "     |████████████████████████████████| 121 kB 28.6 MB/s            \n",
            "\u001b[?25hCollecting traitlets>=4.1.0\n",
            "  Downloading traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
            "     |████████████████████████████████| 75 kB 4.4 MB/s             \n",
            "\u001b[?25hCollecting jupyter-client\n",
            "  Downloading jupyter_client-7.1.2-py3-none-any.whl (130 kB)\n",
            "     |████████████████████████████████| 130 kB 51.3 MB/s            \n",
            "\u001b[?25hCollecting tornado>=4.2\n",
            "  Downloading tornado-6.1-cp36-cp36m-manylinux2010_x86_64.whl (427 kB)\n",
            "     |████████████████████████████████| 427 kB 53.5 MB/s            \n",
            "\u001b[?25hCollecting ipython>=5.0.0\n",
            "  Downloading ipython-7.16.3-py3-none-any.whl (783 kB)\n",
            "     |████████████████████████████████| 783 kB 33.3 MB/s            \n",
            "\u001b[?25hCollecting ipython-genutils\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "     |████████████████████████████████| 381 kB 47.5 MB/s            \n",
            "\u001b[?25hCollecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython>=5.0.0->ipykernel) (39.0.1)\n",
            "Collecting jedi<=0.17.2,>=0.10\n",
            "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 38.0 MB/s            \n",
            "\u001b[?25hCollecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting pexpect\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "     |████████████████████████████████| 59 kB 5.7 MB/s             \n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting pygments\n",
            "  Downloading Pygments-2.12.0-py3-none-any.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 52.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from traitlets>=4.1.0->ipykernel) (1.11.0)\n",
            "Collecting nest-asyncio>=1.5\n",
            "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
            "Collecting jupyter-core>=4.6.0\n",
            "  Downloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\n",
            "     |████████████████████████████████| 86 kB 5.1 MB/s             \n",
            "\u001b[?25hCollecting python-dateutil>=2.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     |████████████████████████████████| 247 kB 39.4 MB/s            \n",
            "\u001b[?25hCollecting pyzmq>=13\n",
            "  Downloading pyzmq-23.0.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 42.4 MB/s            \n",
            "\u001b[?25hCollecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting parso<0.8.0,>=0.7.0\n",
            "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
            "     |████████████████████████████████| 109 kB 48.2 MB/s            \n",
            "\u001b[?25hCollecting wcwidth\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting ptyprocess>=0.5\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ipython-genutils, decorator, wcwidth, traitlets, ptyprocess, parso, tornado, pyzmq, python-dateutil, pygments, prompt-toolkit, pickleshare, pexpect, nest-asyncio, jupyter-core, jedi, entrypoints, backcall, jupyter-client, ipython, ipykernel\n",
            "Successfully installed backcall-0.2.0 decorator-5.1.1 entrypoints-0.4 ipykernel-5.5.6 ipython-7.16.3 ipython-genutils-0.2.0 jedi-0.17.2 jupyter-client-7.1.2 jupyter-core-4.9.2 nest-asyncio-1.5.5 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.29 ptyprocess-0.7.0 pygments-2.12.0 python-dateutil-2.8.2 pyzmq-23.0.0 tornado-6.1 traitlets-4.3.3 wcwidth-0.2.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "ipython_genutils",
                  "pexpect",
                  "pickleshare",
                  "wcwidth",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -R SEERa/\n",
        "!git clone https://github.com/fani-lab/SEERa.git\n",
        "%cd SEERa/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTadusMKXPRJ",
        "outputId": "7c099cde-a439-4e1a-d8a4-c20d1038e9eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SEERa\n",
            "rm: cannot remove 'SEERa/': No such file or directory\n",
            "Cloning into 'SEERa'...\n",
            "remote: Enumerating objects: 775, done.\u001b[K\n",
            "remote: Counting objects: 100% (149/149), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 775 (delta 112), reused 119 (delta 86), pack-reused 626\u001b[K\n",
            "Receiving objects: 100% (775/775), 45.18 MiB | 15.90 MiB/s, done.\n",
            "Resolving deltas: 100% (403/403), done.\n",
            "/content/SEERa/SEERa\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2.2.4)\n",
            "Requirement already satisfied: matplotlib==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.0.1)\n",
            "Requirement already satisfied: tensorflow-gpu==1.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.11.0)\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.8.3)\n",
            "Requirement already satisfied: networkx==2.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (2.5.1)\n",
            "Requirement already satisfied: nltk==3.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.6.2)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.19.5)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn==0.24.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.24.2)\n",
            "Requirement already satisfied: scikit-network in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.20.0)\n",
            "Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (1.5.4)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages/sklearn-0.0-py3.6.egg (from -r requirements.txt (line 15)) (0.0)\n",
            "Requirement already satisfied: tagme==0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.1.3)\n",
            "Requirement already satisfied: igraph in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (0.9.10)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.1->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.1->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.1->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (1.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (0.8.1)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/lib/python3/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (39.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (3.19.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (0.30.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (1.46.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->-r requirements.txt (line 6)) (6.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.6/dist-packages (from networkx==2.5.1->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.6.2->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.6.2->-r requirements.txt (line 8)) (8.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk==3.6.2->-r requirements.txt (line 8)) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.6.2->-r requirements.txt (line 8)) (2022.4.24)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.1.5->-r requirements.txt (line 10)) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.24.2->-r requirements.txt (line 11)) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tagme==0.1.3->-r requirements.txt (line 16)) (2.27.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tagme==0.1.3->-r requirements.txt (line 16)) (0.18.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.6/dist-packages (from igraph->-r requirements.txt (line 17)) (1.6.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from click->nltk==3.6.2->-r requirements.txt (line 8)) (4.8.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.6/dist-packages (from h5py->Keras==2.2.4->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests->tagme==0.1.3->-r requirements.txt (line 16)) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tagme==0.1.3->-r requirements.txt (line 16)) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->tagme==0.1.3->-r requirements.txt (line 16)) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tagme==0.1.3->-r requirements.txt (line 16)) (2022.5.18.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm->nltk==3.6.2->-r requirements.txt (line 8)) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->click->nltk==3.6.2->-r requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->click->nltk==3.6.2->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from werkzeug>=0.11.10->tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0->-r requirements.txt (line 4)) (0.8)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/palash1992/DynamicGEM.git\n",
        "%cd DynamicGEM/\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "RrkSxnOJ_2qD",
        "outputId": "ce8b4c58-4151-4eca-db9e-55a39f345a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating dynamicgem.egg-info\n",
            "writing dynamicgem.egg-info/PKG-INFO\n",
            "writing dependency_links to dynamicgem.egg-info/dependency_links.txt\n",
            "writing requirements to dynamicgem.egg-info/requires.txt\n",
            "writing top-level names to dynamicgem.egg-info/top_level.txt\n",
            "writing manifest file 'dynamicgem.egg-info/SOURCES.txt'\n",
            "reading manifest file 'dynamicgem.egg-info/SOURCES.txt'\n",
            "writing manifest file 'dynamicgem.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/dynamicgem\n",
            "copying dynamicgem/version.py -> build/lib/dynamicgem\n",
            "copying dynamicgem/__init__.py -> build/lib/dynamicgem\n",
            "creating build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dnn_utils.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/graphFac_dynamic.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynRNN.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynamic_graph_embedding.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/static_graph_embedding.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/__init__.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynAE.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynamicTriad.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/sdne_dynamic.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/sdne_utils.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynSDNE.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynAERNN.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/ae_static.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/TIMERS.py -> build/lib/dynamicgem/embedding\n",
            "creating build/lib/dynamicgem/dynamictriad\n",
            "copying dynamicgem/dynamictriad/__init__.py -> build/lib/dynamicgem/dynamictriad\n",
            "copying dynamicgem/dynamictriad/__main__.py -> build/lib/dynamicgem/dynamictriad\n",
            "creating build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/plot_static_embedding.py -> build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/plot_dynamic_sbm_embedding.py -> build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/plot_dynamic_embedding.py -> build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/__init__.py -> build/lib/dynamicgem/visualization\n",
            "creating build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/static_military_call_graph_v1.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/SBM_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/testgraphgen.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/static_military_call_graph_v2.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/createHepTHCollabNet_nx.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/dynamic_military_call_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/dynamic_military_call_graph_v1.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/SBM_node_migration.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/__init__.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/motivation.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/getAS_nx.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/dynamic_SBM_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/static_military_call_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "creating build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/embed_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/__init__.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/dataprep_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/graph_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/plot_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/evaluation_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/fig_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/ts_utils.py -> build/lib/dynamicgem/utils\n",
            "creating build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/visualize_embedding.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/evaluate_graph_reconstruction.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/metrics.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/__init__.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/evaluate_link_prediction.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/standardTest.py -> build/lib/dynamicgem/evaluation\n",
            "creating build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/gconfig.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/gconv.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/mygraph_utils.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/__init__.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/utils_py.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/graphtool_utils.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/utils.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "creating build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/stdtests.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/__init__.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/academic2adjlist.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/test.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "creating build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "copying dynamicgem/dynamictriad/core/algorithm/embutils.py -> build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "copying dynamicgem/dynamictriad/core/algorithm/__init__.py -> build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "copying dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py -> build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "creating build/lib/dynamicgem/dynamictriad/core/cython_src\n",
            "copying dynamicgem/dynamictriad/core/cython_src/__init__.py -> build/lib/dynamicgem/dynamictriad/core/cython_src\n",
            "creating build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/dataset_utils.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/adjlist.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/__init__.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/citation.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "creating build/lib/dynamicgem/dynamictriad/core/kerasext\n",
            "copying dynamicgem/dynamictriad/core/kerasext/__init__.py -> build/lib/dynamicgem/dynamictriad/core/kerasext\n",
            "creating build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "creating build/lib/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying dynamicgem/dynamictriad/core/kerasext/debug/__init__.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "creating build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dnn_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/graphFac_dynamic.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynRNN.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynamic_graph_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/static_graph_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynAE.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynamicTriad.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/sdne_dynamic.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/sdne_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynSDNE.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynAERNN.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/ae_static.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/TIMERS.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/gconfig.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/gconv.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/mygraph_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/embutils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/utils_py.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/graphtool_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/cython_src\n",
            "copying build/lib/dynamicgem/dynamictriad/core/cython_src/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/cython_src\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/dataset_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/adjlist.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/citation.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/debug/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad\n",
            "copying build/lib/dynamicgem/dynamictriad/__main__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/stdtests.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/academic2adjlist.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/test.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_static_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_dynamic_sbm_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_dynamic_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph_v1.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/SBM_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/testgraphgen.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph_v2.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/createHepTHCollabNet_nx.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_military_call_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_military_call_graph_v1.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/SBM_node_migration.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/motivation.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/getAS_nx.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_SBM_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/version.py -> build/bdist.linux-x86_64/egg/dynamicgem\n",
            "copying build/lib/dynamicgem/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/embed_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/dataprep_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/graph_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/plot_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/evaluation_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/fig_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/ts_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/visualize_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/evaluate_graph_reconstruction.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/metrics.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/evaluate_link_prediction.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/standardTest.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dnn_utils.py to dnn_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/graphFac_dynamic.py to graphFac_dynamic.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynRNN.py to dynRNN.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynamic_graph_embedding.py to dynamic_graph_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/static_graph_embedding.py to static_graph_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynAE.py to dynAE.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynamicTriad.py to dynamicTriad.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/sdne_dynamic.py to sdne_dynamic.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/sdne_utils.py to sdne_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynSDNE.py to dynSDNE.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynAERNN.py to dynAERNN.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/ae_static.py to ae_static.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/TIMERS.py to TIMERS.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/gconfig.py to gconfig.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/gconv.py to gconv.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/mygraph_utils.py to mygraph_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py to sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py to pos_neg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py to pos_neg_tri.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/embutils.py to embutils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py to dynamic_triad.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/utils_py.py to utils_py.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/graphtool_utils.py to graphtool_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/cython_src/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/dataset_utils.py to dataset_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/adjlist.py to adjlist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/citation.py to citation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py to finite_number_check.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py to theano_patches.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py to tensorflow_patches.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/stdtests.py to stdtests.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/academic2adjlist.py to academic2adjlist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/test.py to test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/plot_static_embedding.py to plot_static_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/plot_dynamic_sbm_embedding.py to plot_dynamic_sbm_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/plot_dynamic_embedding.py to plot_dynamic_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/static_military_call_graph_v1.py to static_military_call_graph_v1.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/SBM_graph.py to SBM_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/testgraphgen.py to testgraphgen.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/static_military_call_graph_v2.py to static_military_call_graph_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/createHepTHCollabNet_nx.py to createHepTHCollabNet_nx.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/dynamic_military_call_graph.py to dynamic_military_call_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/dynamic_military_call_graph_v1.py to dynamic_military_call_graph_v1.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/SBM_node_migration.py to SBM_node_migration.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/motivation.py to motivation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/getAS_nx.py to getAS_nx.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/dynamic_SBM_graph.py to dynamic_SBM_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/static_military_call_graph.py to static_military_call_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/version.py to version.cpython-36.pyc\n",
            "Sorry: IndentationError: unexpected indent (version.py, line 3)\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/embed_util.py to embed_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/dataprep_util.py to dataprep_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/graph_util.py to graph_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/plot_util.py to plot_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/evaluation_util.py to evaluation_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/fig_util.py to fig_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/ts_utils.py to ts_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/visualize_embedding.py to visualize_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/evaluate_graph_reconstruction.py to evaluate_graph_reconstruction.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/evaluate_link_prediction.py to evaluate_link_prediction.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/standardTest.py to standardTest.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "dynamicgem.dynamictriad.core.__pycache__.__init__.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.core.__pycache__.utils.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.scripts.__pycache__.academic2adjlist.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.scripts.__pycache__.stdtests.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.scripts.__pycache__.test.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.TIMERS.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.ae_static.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.dynSDNE.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.dynamicTriad.cpython-36: module references __file__\n",
            "dynamicgem.evaluation.__pycache__.standardTest.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/dynamicgem-1.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing dynamicgem-1.0.0-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem-1.0.0-py3.6.egg\n",
            "Extracting dynamicgem-1.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Sorry: IndentationError: unexpected indent (version.py, line 3)\n",
            "Adding dynamicgem 1.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/dynamicgem-1.0.0-py3.6.egg\n",
            "Processing dependencies for dynamicgem==1.0.0\n",
            "Searching for tensorflow==1.11.0\n",
            "Reading https://pypi.python.org/simple/tensorflow/\n",
            "Downloading https://files.pythonhosted.org/packages/ce/d5/38cd4543401708e64c9ee6afa664b936860f4630dd93a49ab863f9998cd2/tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl#sha256=8e70949273d52fe85c2f85c3159e0fcc666773beaae75a52ae9d90d71f599cc6\n",
            "Best match: tensorflow 1.11.0\n",
            "Processing tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl to /usr/local/lib/python3.6/dist-packages\n",
            "writing requirements to /usr/local/lib/python3.6/dist-packages/tensorflow-1.11.0-py3.6-linux-x86_64.egg/EGG-INFO/requires.txt\n",
            "Adding tensorflow 1.11.0 to easy-install.pth file\n",
            "Installing freeze_graph script to /usr/local/bin\n",
            "Installing saved_model_cli script to /usr/local/bin\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "Installing tflite_convert script to /usr/local/bin\n",
            "Installing toco script to /usr/local/bin\n",
            "Installing toco_from_protos script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/tensorflow-1.11.0-py3.6-linux-x86_64.egg\n",
            "Searching for sklearn>=0.0\n",
            "Reading https://pypi.python.org/simple/sklearn/\n",
            "Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz#sha256=e23001573aa194b834122d2b9562459bf5ae494a2d59ca6b8aa22c85a44c0e31\n",
            "Best match: sklearn 0.0\n",
            "Processing sklearn-0.0.tar.gz\n",
            "Writing /tmp/easy_install-bvn75t8b/sklearn-0.0/setup.cfg\n",
            "Running sklearn-0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-bvn75t8b/sklearn-0.0/egg-dist-tmp-8r6u_r4s\n",
            "file wheel-platform-tag-is-broken-on-empty-wheels-see-issue-141.py (for module wheel-platform-tag-is-broken-on-empty-wheels-see-issue-141) not found\n",
            "file wheel-platform-tag-is-broken-on-empty-wheels-see-issue-141.py (for module wheel-platform-tag-is-broken-on-empty-wheels-see-issue-141) not found\n",
            "file wheel-platform-tag-is-broken-on-empty-wheels-see-issue-141.py (for module wheel-platform-tag-is-broken-on-empty-wheels-see-issue-141) not found\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating /usr/local/lib/python3.6/dist-packages/sklearn-0.0-py3.6.egg\n",
            "Extracting sklearn-0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sklearn 0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sklearn-0.0-py3.6.egg\n",
            "Searching for seaborn>=0.9.0\n",
            "Reading https://pypi.python.org/simple/seaborn/\n",
            "Downloading https://files.pythonhosted.org/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl#sha256=85a6baa9b55f81a0623abddc4a26b334653ff4c6b18c418361de19dbba0ef283\n",
            "Best match: seaborn 0.11.2\n",
            "Processing seaborn-0.11.2-py3-none-any.whl\n",
            "Installing seaborn-0.11.2-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "writing requirements to /usr/local/lib/python3.6/dist-packages/seaborn-0.11.2-py3.6.egg/EGG-INFO/requires.txt\n",
            "Adding seaborn 0.11.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/seaborn-0.11.2-py3.6.egg\n",
            "Searching for scipy>=1.1.0\n",
            "Reading https://pypi.python.org/simple/scipy/\n",
            "Downloading https://files.pythonhosted.org/packages/26/b5/9330f004b9a3b2b6a31f59f46f1617ce9ca15c0e7fe64288c20385a05c9d/scipy-1.8.1.tar.gz#sha256=9e3fb1b0e896f14a85aa9a28d5f755daaeeb54c897b746df7a55ccb02b340f33\n",
            "Best match: scipy 1.8.1\n",
            "Processing scipy-1.8.1.tar.gz\n",
            "Writing /tmp/easy_install-x9nlsekf/scipy-1.8.1/setup.cfg\n",
            "Running scipy-1.8.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-x9nlsekf/scipy-1.8.1/egg-dist-tmp-i2cgekas\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 154, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 195, in setup_context\n",
            "    yield\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 250, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 45, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-x9nlsekf/scipy-1.8.1/setup.py\", line 31, in <module>\n",
            "    'network embedding', 'data mining', 'machine learning']\n",
            "RuntimeError: Python version >= 3.8 required.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"setup.py\", line 185, in <module>\n",
            "    setup_package()\n",
            "  File \"setup.py\", line 164, in setup_package\n",
            "    'Programming Language :: Python :: 3.5', ],\n",
            "  File \"/usr/lib/python3.6/distutils/core.py\", line 148, in setup\n",
            "    dist.run_commands()\n",
            "  File \"/usr/lib/python3.6/distutils/dist.py\", line 955, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/install.py\", line 67, in run\n",
            "    self.do_egg_install()\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/install.py\", line 117, in do_egg_install\n",
            "    cmd.run()\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 437, in run\n",
            "    self.easy_install(spec, not self.no_deps)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 679, in easy_install\n",
            "    return self.install_item(None, spec, tmpdir, deps, True)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 726, in install_item\n",
            "    self.process_distribution(spec, dist, deps)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 771, in process_distribution\n",
            "    [requirement], self.local_index, self.easy_install\n",
            "  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 774, in resolve\n",
            "    replace_conflicting=replace_conflicting\n",
            "  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 1057, in best_match\n",
            "    return self.obtain(req, installer)\n",
            "  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 1069, in obtain\n",
            "    return installer(requirement)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 698, in easy_install\n",
            "    return self.install_item(spec, dist.location, tmpdir, deps)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 724, in install_item\n",
            "    dists = self.install_eggs(spec, download, tmpdir)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 909, in install_eggs\n",
            "    return self.build_and_install(setup_script, setup_base)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 1177, in build_and_install\n",
            "    self.run_setup(setup_script, setup_base, args)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/command/easy_install.py\", line 1163, in run_setup\n",
            "    run_setup(setup_script, args)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 253, in run_setup\n",
            "    raise\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 195, in setup_context\n",
            "    yield\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 166, in save_modules\n",
            "    saved_exc.resume()\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 141, in resume\n",
            "    six.reraise(type, exc, self._tb)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/_vendor/six.py\", line 685, in reraise\n",
            "    raise value.with_traceback(tb)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 154, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 195, in setup_context\n",
            "    yield\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 250, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/lib/python3/dist-packages/setuptools/sandbox.py\", line 45, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-x9nlsekf/scipy-1.8.1/setup.py\", line 31, in <module>\n",
            "    'network embedding', 'data mining', 'machine learning']\n",
            "RuntimeError: Python version >= 3.8 required.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../SEERa/src\n",
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFfsQl5qtaIf",
        "outputId": "d276b8d0-4ad2-4d04-eb79-113ab9a3719c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SEERa/SEERa/src\n",
            "1. Data Reading & Preparation ...\n",
            "Loading perprocessed files ...\n",
            "Loading perprocessed files failed! Generating files ...\n",
            "dataset.shape: (217108, 6)\n",
            "dataset.keys: Index(['TweetId', 'Text', 'CreationDate', 'UserId', 'ModificationTimestamp',\n",
            "       'Tokens'],\n",
            "      dtype='object')\n",
            "Data Preparation ...\n",
            "DataPreperation: userModeling=True, timeModeling=True, preProcessing=False, TagME=False\n",
            "DataPreperation: Length of the dataset after applying groupby: 37063 \n",
            "\n",
            "DataPreparation: Processed docs shape: (37063,)\n",
            "processed_docs.shape: (37063,)\n",
            "documents.shape: (37063, 3)\n",
            "2. Topic modeling ...\n",
            "Loading LDA model ...\n",
            "loading Dictionary object from ../output/1/tml/gensim_30topics_TopicModelingDictionary.mm\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'rb', 'uri': '../output/1/tml/gensim_30topics_TopicModelingDictionary.mm'}\n",
            "Loading LDA model failed! Training LDA model ...\n",
            "TopicModeling: num_topics=30,  filterExtremes=True, library=gensim\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "adding document #10000 to Dictionary(119303 unique tokens: ['&', '1,500', '13th', '2,', '2010']...)\n",
            "adding document #20000 to Dictionary(195293 unique tokens: ['&', '1,500', '13th', '2,', '2010']...)\n",
            "adding document #30000 to Dictionary(284096 unique tokens: ['&', '1,500', '13th', '2,', '2010']...)\n",
            "built Dictionary(331114 unique tokens: ['&', '1,500', '13th', '2,', '2010']...) from 37063 documents (total 2995253 corpus positions)\n",
            "discarding 231114 tokens: [('and', 8827), ('for', 9324), ('in', 10692), ('on', 8180), ('the', 15281), ('to', 13453), ('is', 8377), ('#freewillie', 1), ('http://tinyurl.com/28fvxrx', 1), ('of', 10697)]...\n",
            "keeping 100000 tokens which were in no less than 1 and no more than 7412 (=20.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(100000 unique tokens: ['&', '1,500', '13th', '2,', '2010']...)\n",
            "using symmetric alpha at 0.03333333333333333\n",
            "using symmetric eta at 0.03333333333333333\n",
            "using serial LDA version on this node\n",
            "running online (multi-pass) LDA training, 30 topics, 5 passes over the supplied corpus of 37063 documents, updating model once every 2000 documents, evaluating perplexity every 20000 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "PROGRESS: pass 0, at document #2000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1351/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #11 (0.033): 0.010*\"de\" + 0.008*\"...\" + 0.005*\"with\" + 0.005*\"new\" + 0.005*\"world\" + 0.005*\"at\" + 0.004*\"el\" + 0.004*\"que\" + 0.004*\"from\" + 0.004*\"as\"\n",
            "topic #19 (0.033): 0.006*\"this\" + 0.006*\"you\" + 0.005*\"...\" + 0.005*\"wikileaks\" + 0.004*\"be\" + 0.004*\"with\" + 0.004*\"by\" + 0.004*\"that\" + 0.004*\"from\" + 0.004*\"are\"\n",
            "topic #6 (0.033): 0.009*\"...\" + 0.006*\"de\" + 0.006*\"-\" + 0.005*\"not\" + 0.004*\"with\" + 0.004*\"no\" + 0.004*\"new\" + 0.004*\"it\" + 0.004*\"la\" + 0.004*\"that\"\n",
            "topic #15 (0.033): 0.010*\"at\" + 0.006*\"by\" + 0.006*\"...\" + 0.006*\"-\" + 0.005*\"as\" + 0.005*\"from\" + 0.005*\"&\" + 0.004*\"new\" + 0.004*\"with\" + 0.004*\"how\"\n",
            "topic #28 (0.033): 0.011*\"-\" + 0.007*\"...\" + 0.005*\"by\" + 0.004*\"it\" + 0.004*\"at\" + 0.004*\"with\" + 0.004*\"as\" + 0.004*\"are\" + 0.004*\"you\" + 0.004*\"from\"\n",
            "topic diff=24.193888, rho=1.000000\n",
            "PROGRESS: pass 0, at document #4000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1711/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #17 (0.033): 0.009*\"by\" + 0.009*\"-\" + 0.008*\"at\" + 0.007*\"via\" + 0.007*\"new\" + 0.006*\"from\" + 0.006*\"as\" + 0.005*\"it\" + 0.004*\"|\" + 0.004*\"that\"\n",
            "topic #1 (0.033): 0.015*\"-\" + 0.012*\"...\" + 0.012*\"at\" + 0.011*\"with\" + 0.008*\"this\" + 0.008*\"it\" + 0.008*\"you\" + 0.006*\"be\" + 0.006*\"from\" + 0.005*\"are\"\n",
            "topic #22 (0.033): 0.039*\"??????\" + 0.038*\"???\" + 0.034*\"????\" + 0.029*\"??\" + 0.028*\"?????\" + 0.016*\"???????\" + 0.009*\"?????????\" + 0.009*\"????????\" + 0.009*\"-\" + 0.006*\"from\"\n",
            "topic #18 (0.033): 0.010*\"you\" + 0.009*\"at\" + 0.009*\"that\" + 0.008*\"new\" + 0.008*\"from\" + 0.008*\"have\" + 0.007*\"my\" + 0.007*\"are\" + 0.007*\"...\" + 0.007*\"world\"\n",
            "topic #14 (0.033): 0.008*\"with\" + 0.007*\"-\" + 0.006*\"by\" + 0.006*\"tax\" + 0.006*\"cuts\" + 0.006*\"you\" + 0.006*\"do\" + 0.004*\"that\" + 0.004*\"be\" + 0.004*\"not\"\n",
            "topic diff=0.797970, rho=0.707107\n",
            "PROGRESS: pass 0, at document #6000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1811/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #16 (0.033): 0.032*\"-\" + 0.009*\"you\" + 0.008*\"with\" + 0.008*\"...\" + 0.006*\"your\" + 0.006*\"from\" + 0.006*\"this\" + 0.006*\"&\" + 0.004*\"no\" + 0.004*\"new\"\n",
            "topic #3 (0.033): 0.015*\"by\" + 0.013*\"...\" + 0.012*\"wikileaks\" + 0.010*\"-\" + 0.008*\"via\" + 0.007*\"with\" + 0.006*\"us\" + 0.005*\"that\" + 0.005*\"new\" + 0.005*\"from\"\n",
            "topic #0 (0.033): 0.020*\"you\" + 0.011*\"this\" + 0.011*\"&\" + 0.011*\"your\" + 0.009*\"my\" + 0.008*\"-\" + 0.008*\"...\" + 0.008*\"at\" + 0.007*\"it\" + 0.007*\"that\"\n",
            "topic #1 (0.033): 0.018*\"-\" + 0.015*\"...\" + 0.014*\"at\" + 0.012*\"with\" + 0.011*\"this\" + 0.009*\"it\" + 0.009*\"you\" + 0.007*\"be\" + 0.007*\"from\" + 0.006*\"your\"\n",
            "topic #23 (0.033): 0.023*\"-\" + 0.010*\"with\" + 0.009*\"be\" + 0.008*\"...\" + 0.008*\"not\" + 0.008*\"that\" + 0.007*\"will\" + 0.006*\"it\" + 0.006*\"at\" + 0.006*\"you\"\n",
            "topic diff=0.763681, rho=0.577350\n",
            "PROGRESS: pass 0, at document #8000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1820/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #22 (0.033): 0.045*\"??????\" + 0.040*\"?????\" + 0.035*\"???\" + 0.033*\"????\" + 0.032*\"??\" + 0.020*\"???????\" + 0.012*\"-\" + 0.012*\"?????????\" + 0.012*\"?\" + 0.009*\"????????\"\n",
            "topic #13 (0.033): 0.008*\"you\" + 0.007*\"le\" + 0.006*\"new\" + 0.006*\"les\" + 0.006*\"at\" + 0.005*\"-\" + 0.005*\"video\" + 0.005*\"that\" + 0.005*\"de\" + 0.005*\"no\"\n",
            "topic #28 (0.033): 0.015*\"-\" + 0.010*\"...\" + 0.008*\"background\" + 0.008*\"are\" + 0.008*\"it\" + 0.006*\"we\" + 0.006*\"by\" + 0.006*\"as\" + 0.006*\"at\" + 0.006*\"you\"\n",
            "topic #19 (0.033): 0.020*\"other.\" + 0.013*\"you\" + 0.011*\"this\" + 0.011*\"each\" + 0.009*\"...\" + 0.007*\"we\" + 0.007*\"-\" + 0.006*\"that\" + 0.006*\"have\" + 0.006*\"are\"\n",
            "topic #14 (0.033): 0.012*\"@huffingtonpost\" + 0.009*\"-\" + 0.008*\"cuts\" + 0.008*\"with\" + 0.007*\"n\" + 0.007*\"tax\" + 0.006*\"#art\" + 0.006*\"by\" + 0.006*\"do\" + 0.006*\"photography\"\n",
            "topic diff=0.754989, rho=0.500000\n",
            "PROGRESS: pass 0, at document #10000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1886/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #23 (0.033): 0.027*\"-\" + 0.010*\"...\" + 0.010*\"with\" + 0.009*\"be\" + 0.007*\"will\" + 0.007*\"that\" + 0.007*\"at\" + 0.007*\"it\" + 0.007*\"not\" + 0.006*\"you\"\n",
            "topic #28 (0.033): 0.018*\"-\" + 0.011*\"...\" + 0.007*\"are\" + 0.007*\"it\" + 0.006*\"as\" + 0.006*\"by\" + 0.006*\"at\" + 0.006*\"with\" + 0.005*\"free\" + 0.005*\"youtube\"\n",
            "topic #13 (0.033): 0.011*\"video\" + 0.007*\"you\" + 0.006*\"new\" + 0.006*\"hanukkah\" + 0.006*\"-\" + 0.006*\"i\" + 0.005*\"at\" + 0.005*\"le\" + 0.005*\"all\" + 0.004*\"wedding\"\n",
            "topic #3 (0.033): 0.018*\"wikileaks\" + 0.017*\"...\" + 0.017*\"by\" + 0.016*\"-\" + 0.009*\"/cc\" + 0.008*\"us\" + 0.008*\"via\" + 0.007*\"#wikileaks\" + 0.007*\"with\" + 0.006*\"#cablegate\"\n",
            "topic #24 (0.033): 0.010*\";-)\" + 0.008*\"-\" + 0.007*\"at\" + 0.007*\"this?\" + 0.006*\"...\" + 0.006*\"this\" + 0.006*\"discovered\" + 0.006*\"with\" + 0.005*\"are\" + 0.005*\"they\"\n",
            "topic diff=0.641917, rho=0.447214\n",
            "PROGRESS: pass 0, at document #12000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1896/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #10 (0.033): 0.010*\"-\" + 0.008*\"from\" + 0.007*\"#wikileaks\" + 0.006*\"–\" + 0.005*\"via\" + 0.005*\"wikileaks\" + 0.005*\"...\" + 0.005*\"#cablegate\" + 0.004*\"&\" + 0.004*\"by\"\n",
            "topic #3 (0.033): 0.018*\"by\" + 0.018*\"wikileaks\" + 0.014*\"-\" + 0.013*\"...\" + 0.009*\"us\" + 0.008*\"#wikileaks\" + 0.007*\"with\" + 0.006*\"via\" + 0.006*\"from\" + 0.006*\"new\"\n",
            "topic #23 (0.033): 0.024*\"-\" + 0.010*\"be\" + 0.009*\"with\" + 0.009*\"...\" + 0.008*\"will\" + 0.008*\"that\" + 0.007*\"not\" + 0.007*\"at\" + 0.006*\"it\" + 0.005*\"&\"\n",
            "topic #9 (0.033): 0.036*\"2000\" + 0.027*\"/\" + 0.017*\"#worldaidsday\" + 0.010*\"????????????????????????????\" + 0.009*\"??????????\" + 0.009*\"#assange\" + 0.008*\"teen\" + 0.007*\"flow\" + 0.007*\"soup\" + 0.007*\"warm\"\n",
            "topic #12 (0.033): 0.013*\"...\" + 0.012*\"de\" + 0.010*\"la\" + 0.009*\":\" + 0.008*\"le\" + 0.008*\"pour\" + 0.007*\"-\" + 0.006*\"du\" + 0.006*\"jam\" + 0.006*\"des\"\n",
            "topic diff=0.555645, rho=0.408248\n",
            "PROGRESS: pass 0, at document #14000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1899/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #21 (0.033): 0.025*\"$\" + 0.017*\"ha\" + 0.012*\"(@\" + 0.010*\"metro\" + 0.010*\"knowing\" + 0.009*\"religious\" + 0.008*\"snowing\" + 0.008*\"gallery\" + 0.008*\"bloody\" + 0.007*\"year's\"\n",
            "topic #27 (0.033): 0.088*\"de\" + 0.029*\"la\" + 0.027*\"que\" + 0.026*\"en\" + 0.022*\"no\" + 0.019*\"el\" + 0.018*\"se\" + 0.015*\"para\" + 0.014*\"do\" + 0.014*\"da\"\n",
            "topic #2 (0.033): 0.026*\"it\" + 0.014*\"be\" + 0.013*\"you\" + 0.013*\"lol\" + 0.013*\"my\" + 0.011*\"not\" + 0.010*\"...\" + 0.009*\"with\" + 0.009*\"just\" + 0.009*\"your\"\n",
            "topic #25 (0.033): 0.023*\"(cont)\" + 0.020*\"aids\" + 0.016*\"&\" + 0.015*\"...\" + 0.014*\"world\" + 0.010*\"you\" + 0.010*\"#ff\" + 0.010*\"today\" + 0.009*\"day\" + 0.007*\"music,\"\n",
            "topic #6 (0.033): 0.028*\"tax\" + 0.025*\"gop\" + 0.019*\"#p2\" + 0.017*\"cuts\" + 0.014*\"#tcot\" + 0.012*\"bush\" + 0.011*\"--\" + 0.010*\"senate\" + 0.010*\"...\" + 0.010*\"vote\"\n",
            "topic diff=0.453877, rho=0.377964\n",
            "PROGRESS: pass 0, at document #16000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1909/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #13 (0.033): 0.008*\"video\" + 0.007*\"une\" + 0.006*\")\" + 0.006*\"hanukkah\" + 0.006*\"new\" + 0.006*\",\" + 0.005*\"on!\" + 0.005*\"#christmas\" + 0.005*\"all\" + 0.005*\"wedding\"\n",
            "topic #19 (0.033): 0.019*\"~\" + 0.017*\"turned\" + 0.009*\"this\" + 0.008*\"you\" + 0.006*\"other.\" + 0.006*\"-\" + 0.006*\"....\" + 0.006*\"...\" + 0.005*\"each\" + 0.005*\"we\"\n",
            "topic #21 (0.033): 0.024*\"$\" + 0.017*\"(@\" + 0.016*\"ha\" + 0.011*\"knowing\" + 0.009*\"atlanta\" + 0.008*\"year's\" + 0.008*\"religious\" + 0.007*\"metro\" + 0.006*\"shuttle\" + 0.006*\"voor\"\n",
            "topic #4 (0.033): 0.011*\"@nytimes:\" + 0.009*\"morning!\" + 0.007*\"meet\" + 0.007*\"economy\" + 0.006*\"up\" + 0.006*\"with\" + 0.006*\"-\" + 0.005*\"plus\" + 0.005*\"nyt\" + 0.005*\"rate\"\n",
            "topic #20 (0.033): 0.019*\"trust\" + 0.015*\"#fb\" + 0.012*\"#football\" + 0.006*\"rose\" + 0.006*\"earn\" + 0.006*\"bowl\" + 0.006*\"chicago\" + 0.006*\"ron\" + 0.005*\"#soccer\" + 0.005*\"wash\"\n",
            "topic diff=0.384146, rho=0.353553\n",
            "PROGRESS: pass 0, at document #18000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1918/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #12 (0.033): 0.023*\":\" + 0.016*\"le\" + 0.015*\"la\" + 0.014*\"...\" + 0.012*\"du\" + 0.012*\".\" + 0.012*\"des\" + 0.011*\"les\" + 0.010*\"de\" + 0.010*\"pour\"\n",
            "topic #28 (0.033): 0.016*\"-\" + 0.014*\"...\" + 0.008*\"free\" + 0.007*\"@breakingnews:\" + 0.007*\"at\" + 0.007*\"as\" + 0.006*\"south\" + 0.006*\"with\" + 0.006*\"are\" + 0.005*\"by\"\n",
            "topic #17 (0.033): 0.027*\"via\" + 0.016*\"|\" + 0.016*\"by\" + 0.014*\"-\" + 0.013*\"top\" + 0.012*\"daily\" + 0.012*\"?\" + 0.010*\"today\" + 0.010*\"@addthis\" + 0.008*\"stories\"\n",
            "topic #15 (0.033): 0.023*\"at\" + 0.016*\"&\" + 0.011*\"...\" + 0.010*\"from\" + 0.009*\"new\" + 0.008*\"by\" + 0.007*\"-\" + 0.005*\"climate\" + 0.005*\"as\" + 0.005*\"holiday\"\n",
            "topic #13 (0.033): 0.011*\"att\" + 0.010*\"hanukkah\" + 0.009*\")\" + 0.008*\"une\" + 0.008*\"video\" + 0.007*\",\" + 0.006*\"royal\" + 0.005*\"all\" + 0.005*\"#christmas\" + 0.005*\"on!\"\n",
            "topic diff=0.293196, rho=0.333333\n",
            "bound: at document #0\n",
            "-15.245 per-word bound, 38820.5 perplexity estimate based on a held-out corpus of 2000 documents with 97368 words\n",
            "PROGRESS: pass 0, at document #20000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1949/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #23 (0.033): 0.023*\"-\" + 0.011*\"...\" + 0.009*\"with\" + 0.009*\"be\" + 0.008*\"will\" + 0.007*\"at\" + 0.007*\"that\" + 0.007*\"not\" + 0.005*\"it\" + 0.005*\"&\"\n",
            "topic #27 (0.033): 0.101*\"de\" + 0.039*\"que\" + 0.028*\"en\" + 0.026*\"la\" + 0.023*\"no\" + 0.022*\"el\" + 0.019*\"do\" + 0.019*\"se\" + 0.018*\"para\" + 0.017*\"da\"\n",
            "topic #4 (0.033): 0.015*\"morning!\" + 0.011*\"@nytimes:\" + 0.008*\"economy\" + 0.006*\"meet\" + 0.006*\"taylor\" + 0.006*\"plus\" + 0.006*\"careful\" + 0.005*\"with\" + 0.005*\"screen\" + 0.005*\"legacy\"\n",
            "topic #3 (0.033): 0.021*\"...\" + 0.020*\"wikileaks\" + 0.016*\"-\" + 0.014*\"by\" + 0.010*\"us\" + 0.008*\"with\" + 0.007*\"#wikileaks\" + 0.007*\"new\" + 0.007*\"from\" + 0.006*\"u.s.\"\n",
            "topic #25 (0.033): 0.029*\"aids\" + 0.023*\"world\" + 0.020*\"...\" + 0.020*\"(cont)\" + 0.018*\"&\" + 0.014*\"day\" + 0.012*\"#ff\" + 0.011*\"today\" + 0.009*\"you\" + 0.007*\"hiv\"\n",
            "topic diff=0.258320, rho=0.316228\n",
            "PROGRESS: pass 0, at document #22000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1927/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #13 (0.033): 0.020*\"une\" + 0.019*\"hanukkah\" + 0.015*\")\" + 0.009*\"chanukah\" + 0.009*\"happy\" + 0.007*\"(\" + 0.007*\"video\" + 0.006*\"royal\" + 0.006*\",\" + 0.006*\"soy\"\n",
            "topic #8 (0.033): 0.022*\"!\" + 0.013*\"#business\" + 0.010*\"...\" + 0.009*\"new\" + 0.008*\"post:\" + 0.007*\"angels\" + 0.006*\"me:\" + 0.006*\"york\" + 0.006*\"-\" + 0.005*\"racist\"\n",
            "topic #21 (0.033): 0.027*\"ha\" + 0.026*\"$\" + 0.018*\"(@\" + 0.011*\"metro\" + 0.010*\"year's\" + 0.010*\"atlanta\" + 0.009*\"knowing\" + 0.008*\"know?\" + 0.008*\"religious\" + 0.007*\"iraqi\"\n",
            "topic #7 (0.033): 0.024*\"#16azar\" + 0.017*\"lol.\" + 0.015*\"rosa\" + 0.013*\"her\" + 0.013*\"morning,\" + 0.011*\"@grooveshark:\" + 0.010*\"parks\" + 0.009*\"#blog\" + 0.007*\"dat\" + 0.007*\"all!\"\n",
            "topic #3 (0.033): 0.028*\"...\" + 0.021*\"wikileaks\" + 0.019*\"-\" + 0.013*\"by\" + 0.011*\"us\" + 0.008*\"with\" + 0.008*\"new\" + 0.008*\"#wikileaks\" + 0.007*\"from\" + 0.006*\"u.s.\"\n",
            "topic diff=0.273226, rho=0.301511\n",
            "PROGRESS: pass 0, at document #24000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1935/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #14 (0.033): 0.019*\"$$\" + 0.019*\"/via\" + 0.016*\"meu\" + 0.011*\"@cnnbrk:\" + 0.008*\"ma\" + 0.007*\"prayer\" + 0.007*\"puerto\" + 0.007*\"okay\" + 0.006*\"photography\" + 0.006*\"jeter\"\n",
            "topic #27 (0.033): 0.113*\"de\" + 0.038*\"que\" + 0.032*\"en\" + 0.030*\"la\" + 0.026*\"no\" + 0.025*\"el\" + 0.020*\"para\" + 0.019*\"do\" + 0.017*\"se\" + 0.016*\"da\"\n",
            "topic #0 (0.033): 0.039*\"you\" + 0.031*\"your\" + 0.019*\"&\" + 0.015*\"this\" + 0.012*\"our\" + 0.012*\"my\" + 0.011*\"love\" + 0.010*\"best\" + 0.009*\"christmas\" + 0.008*\"at\"\n",
            "topic #26 (0.033): 0.031*\"do\" + 0.022*\"-\" + 0.016*\"...\" + 0.014*\"com\" + 0.014*\"é\" + 0.013*\"em\" + 0.012*\"da\" + 0.012*\"#nowplaying\" + 0.009*\"bom\" + 0.009*\"yg\"\n",
            "topic #24 (0.033): 0.020*\";-)\" + 0.016*\"ones\" + 0.014*\"wine\" + 0.009*\"texting\" + 0.007*\"[pic]\" + 0.007*\"trailer\" + 0.006*\"wow\" + 0.006*\"supply\" + 0.006*\"lol,\" + 0.006*\"discovered\"\n",
            "topic diff=0.171323, rho=0.288675\n",
            "PROGRESS: pass 0, at document #26000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1955/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #8 (0.033): 0.016*\"!\" + 0.010*\"#business\" + 0.010*\"post:\" + 0.009*\"...\" + 0.009*\"new\" + 0.006*\"angels\" + 0.006*\"tourism\" + 0.006*\"me:\" + 0.006*\"miles\" + 0.005*\"large\"\n",
            "topic #16 (0.033): 0.071*\"-\" + 0.011*\"social\" + 0.008*\"...\" + 0.007*\"new\" + 0.007*\"facebook\" + 0.007*\"with\" + 0.007*\"justin\" + 0.006*\"marketing\" + 0.006*\"media\" + 0.006*\"your\"\n",
            "topic #13 (0.033): 0.021*\")\" + 0.019*\"une\" + 0.015*\"hanukkah\" + 0.013*\"pattinson\" + 0.011*\"(\" + 0.010*\"kristen\" + 0.010*\"royal\" + 0.010*\"happy\" + 0.008*\"wedding\" + 0.008*\"chanukah\"\n",
            "topic #9 (0.033): 0.065*\"/\" + 0.015*\"bay\" + 0.013*\"????????????????????????????\" + 0.012*\"#glee\" + 0.012*\"teen\" + 0.010*\"boo\" + 0.009*\"flow\" + 0.008*\"???????????????????????????????????????\" + 0.008*\"equipment\" + 0.008*\"2000\"\n",
            "topic #5 (0.033): 0.057*\"ppl\" + 0.011*\"involved\" + 0.009*\"chelsea\" + 0.009*\"gold\" + 0.009*\"everton\" + 0.008*\"3\" + 0.008*\"character\" + 0.007*\"fica\" + 0.007*\"sales\" + 0.006*\"arsenal\"\n",
            "topic diff=0.160631, rho=0.277350\n",
            "PROGRESS: pass 0, at document #28000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1951/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #15 (0.033): 0.026*\"at\" + 0.014*\"&\" + 0.013*\"from\" + 0.013*\"...\" + 0.009*\"new\" + 0.009*\"-\" + 0.009*\"by\" + 0.007*\"snow\" + 0.006*\"green\" + 0.005*\"#auspol\"\n",
            "topic #23 (0.033): 0.024*\"-\" + 0.015*\"...\" + 0.009*\"with\" + 0.008*\"be\" + 0.008*\"will\" + 0.007*\"at\" + 0.007*\"not\" + 0.006*\"that\" + 0.005*\"says\" + 0.004*\"as\"\n",
            "topic #10 (0.033): 0.026*\"#iran\" + 0.024*\"#iranelection\" + 0.009*\"!!\" + 0.008*\"–\" + 0.007*\"-\" + 0.007*\"#usa\" + 0.007*\"!\" + 0.007*\"rights\" + 0.006*\"#humanrights\" + 0.006*\"re\"\n",
            "topic #17 (0.033): 0.038*\"via\" + 0.023*\"by\" + 0.020*\"|\" + 0.019*\"-\" + 0.017*\"?\" + 0.017*\"top\" + 0.017*\"daily\" + 0.013*\"today\" + 0.012*\"@addthis\" + 0.011*\"stories\"\n",
            "topic #6 (0.033): 0.030*\"tax\" + 0.023*\"gop\" + 0.021*\"#p2\" + 0.018*\"#tcot\" + 0.016*\"cuts\" + 0.015*\"...\" + 0.013*\"vote\" + 0.013*\"bush\" + 0.012*\"senate\" + 0.011*\"--\"\n",
            "topic diff=0.138748, rho=0.267261\n",
            "PROGRESS: pass 0, at document #30000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1950/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #11 (0.033): 0.034*\"van\" + 0.023*\"pm\" + 0.019*\"het\" + 0.017*\"op\" + 0.017*\"een\" + 0.014*\"#human\" + 0.012*\":-)\" + 0.011*\"11\" + 0.010*\"er\" + 0.009*\"in!\"\n",
            "topic #22 (0.033): 0.161*\"???\" + 0.119*\"??\" + 0.115*\"????\" + 0.099*\"?????\" + 0.072*\"??????\" + 0.058*\"?\" + 0.049*\"???????\" + 0.041*\"????????\" + 0.030*\"?????????\" + 0.020*\"??????????\"\n",
            "topic #12 (0.033): 0.056*\":\" + 0.034*\"la\" + 0.031*\"le\" + 0.026*\"les\" + 0.025*\"de\" + 0.025*\"et\" + 0.020*\"-\" + 0.019*\"...\" + 0.018*\"à\" + 0.018*\"di\"\n",
            "topic #18 (0.033): 0.021*\"you\" + 0.014*\"that\" + 0.013*\"was\" + 0.012*\"my\" + 0.012*\"...\" + 0.012*\"have\" + 0.011*\"are\" + 0.009*\"be\" + 0.009*\"it\" + 0.008*\"we\"\n",
            "topic #21 (0.033): 0.022*\"$\" + 0.021*\"(@\" + 0.017*\"ha\" + 0.014*\"knowing\" + 0.013*\"uae\" + 0.013*\"year's\" + 0.012*\"religious\" + 0.011*\"metro\" + 0.009*\"classes\" + 0.009*\"zardari\"\n",
            "topic diff=0.167841, rho=0.258199\n",
            "PROGRESS: pass 0, at document #32000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1952/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #25 (0.033): 0.043*\"(cont)\" + 0.032*\"aids\" + 0.031*\"world\" + 0.027*\"...\" + 0.021*\"#ff\" + 0.020*\"day\" + 0.017*\"&\" + 0.011*\"today\" + 0.009*\"hiv\" + 0.008*\"by\"\n",
            "topic #27 (0.033): 0.118*\"de\" + 0.039*\"que\" + 0.035*\"en\" + 0.035*\"la\" + 0.031*\"el\" + 0.026*\"no\" + 0.021*\"para\" + 0.016*\"por\" + 0.016*\"se\" + 0.013*\"los\"\n",
            "topic #12 (0.033): 0.055*\":\" + 0.033*\"la\" + 0.029*\"le\" + 0.025*\"de\" + 0.024*\"les\" + 0.022*\"et\" + 0.020*\"-\" + 0.018*\"...\" + 0.018*\"di\" + 0.018*\"à\"\n",
            "topic #16 (0.033): 0.094*\"ranking\" + 0.062*\"-\" + 0.033*\"week\" + 0.013*\"social\" + 0.009*\"facebook\" + 0.007*\"marketing\" + 0.007*\"media\" + 0.007*\"@mashable:\" + 0.007*\"...\" + 0.006*\"new\"\n",
            "topic #2 (0.033): 0.029*\"it\" + 0.016*\"be\" + 0.015*\"you\" + 0.014*\"my\" + 0.014*\"lol\" + 0.012*\"not\" + 0.010*\"just\" + 0.009*\"...\" + 0.009*\"was\" + 0.009*\"with\"\n",
            "topic diff=0.096059, rho=0.250000\n",
            "PROGRESS: pass 0, at document #34000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1951/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #18 (0.033): 0.022*\"you\" + 0.013*\"that\" + 0.013*\"my\" + 0.012*\"...\" + 0.012*\"have\" + 0.011*\"are\" + 0.010*\"was\" + 0.009*\"be\" + 0.009*\"it\" + 0.008*\"we\"\n",
            "topic #13 (0.033): 0.024*\")\" + 0.019*\"(\" + 0.017*\"hanukkah\" + 0.014*\"une\" + 0.013*\"happy\" + 0.011*\"royal\" + 0.011*\"wedding\" + 0.010*\"soy\" + 0.007*\"cambio\" + 0.007*\"pattinson\"\n",
            "topic #1 (0.033): 0.039*\"...\" + 0.028*\"-\" + 0.022*\"at\" + 0.014*\"home\" + 0.014*\"this\" + 0.014*\"with\" + 0.008*\"out\" + 0.008*\"now\" + 0.008*\"from\" + 0.007*\"your\"\n",
            "topic #21 (0.033): 0.024*\"$\" + 0.022*\"(@\" + 0.015*\"ha\" + 0.013*\"religious\" + 0.011*\"knowing\" + 0.011*\"year's\" + 0.010*\"uae\" + 0.010*\"victoria\" + 0.009*\"metro\" + 0.009*\"atlanta\"\n",
            "topic #8 (0.033): 0.114*\"!\" + 0.018*\"janeiro\" + 0.009*\"@thedailybeast:\" + 0.008*\"cares\" + 0.008*\"post:\" + 0.007*\"new\" + 0.007*\"...\" + 0.007*\"posted:\" + 0.006*\"medicine\" + 0.006*\"miles\"\n",
            "topic diff=0.100371, rho=0.242536\n",
            "PROGRESS: pass 0, at document #36000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1956/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #10 (0.033): 0.012*\"#iran\" + 0.011*\"#iranelection\" + 0.011*\"–\" + 0.008*\"re\" + 0.007*\"!!\" + 0.007*\"itunes\" + 0.007*\"-\" + 0.006*\"#toronto\" + 0.006*\"rights\" + 0.006*\"#music\"\n",
            "topic #1 (0.033): 0.041*\"...\" + 0.029*\"-\" + 0.022*\"at\" + 0.015*\"this\" + 0.014*\"with\" + 0.011*\"home\" + 0.010*\"out\" + 0.008*\"check\" + 0.007*\"now\" + 0.007*\"from\"\n",
            "topic #11 (0.033): 0.029*\"darfur\" + 0.026*\"van\" + 0.022*\"pm\" + 0.016*\"een\" + 0.015*\"promo\" + 0.014*\"dubai\" + 0.013*\"op\" + 0.013*\"het\" + 0.013*\"youtube.\" + 0.012*\"11\"\n",
            "topic #6 (0.033): 0.035*\"tax\" + 0.022*\"#p2\" + 0.022*\"gop\" + 0.020*\"cuts\" + 0.018*\"...\" + 0.016*\"#tcot\" + 0.013*\"--\" + 0.013*\"senate\" + 0.012*\"vote\" + 0.011*\"bush\"\n",
            "topic #3 (0.033): 0.042*\"...\" + 0.025*\"-\" + 0.018*\"wikileaks\" + 0.012*\"by\" + 0.011*\"us\" + 0.008*\"new\" + 0.008*\"#wikileaks\" + 0.007*\"with\" + 0.007*\"from\" + 0.007*\"u.s.\"\n",
            "topic diff=0.079891, rho=0.235702\n",
            "bound: at document #0\n",
            "-14.897 per-word bound, 30505.0 perplexity estimate based on a held-out corpus of 1063 documents with 81213 words\n",
            "PROGRESS: pass 0, at document #37063/37063\n",
            "performing inference on a chunk of 1063 documents\n",
            "1039/1063 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 1063 documents into a model of 37063 documents\n",
            "topic #14 (0.033): 0.028*\"@cnnbrk:\" + 0.016*\"french\" + 0.014*\"ma\" + 0.013*\"/via\" + 0.010*\"photo:\" + 0.010*\"promote\" + 0.009*\"alaska\" + 0.007*\"women's\" + 0.007*\"everywhere\" + 0.007*\"prayer\"\n",
            "topic #7 (0.033): 0.028*\"rosa\" + 0.025*\"lol.\" + 0.023*\"parks\" + 0.017*\"park\" + 0.017*\"dat\" + 0.016*\"55\" + 0.015*\"ago\" + 0.015*\"#blog\" + 0.015*\"her\" + 0.013*\"seat\"\n",
            "topic #3 (0.033): 0.042*\"...\" + 0.024*\"-\" + 0.020*\"wikileaks\" + 0.014*\"by\" + 0.011*\"us\" + 0.010*\"#wikileaks\" + 0.008*\"new\" + 0.008*\"from\" + 0.008*\"with\" + 0.007*\"assange\"\n",
            "topic #17 (0.033): 0.054*\"via\" + 0.030*\"|\" + 0.025*\"by\" + 0.020*\"-\" + 0.020*\"top\" + 0.020*\"@addthis\" + 0.018*\"daily\" + 0.018*\"?\" + 0.013*\"stories\" + 0.012*\"today\"\n",
            "topic #6 (0.033): 0.036*\"tax\" + 0.023*\"#p2\" + 0.021*\"gop\" + 0.019*\"cuts\" + 0.018*\"...\" + 0.017*\"#tcot\" + 0.012*\"--\" + 0.012*\"senate\" + 0.012*\"vote\" + 0.011*\"bush\"\n",
            "topic diff=0.077561, rho=0.229416\n",
            "PROGRESS: pass 1, at document #2000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1979/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #4 (0.033): 0.087*\"twitter's\" + 0.038*\"featured\" + 0.010*\"morning!\" + 0.009*\"taylor\" + 0.008*\"@nytimes:\" + 0.008*\"wonderful\" + 0.007*\"plus\" + 0.007*\"fewer\" + 0.007*\"economy\" + 0.006*\"jobs\"\n",
            "topic #28 (0.033): 0.020*\"...\" + 0.019*\"-\" + 0.012*\">>\" + 0.009*\"free\" + 0.009*\"air\" + 0.008*\"as\" + 0.007*\"@breakingnews:\" + 0.007*\"at\" + 0.006*\"are\" + 0.006*\"with\"\n",
            "topic #11 (0.033): 0.025*\"dubai\" + 0.022*\"van\" + 0.019*\"darfur\" + 0.017*\"pm\" + 0.011*\"een\" + 0.011*\"er\" + 0.011*\"promo\" + 0.011*\"op\" + 0.010*\"het\" + 0.009*\"11\"\n",
            "topic #9 (0.033): 0.090*\"/\" + 0.014*\"????????????????????????????\" + 0.011*\"teen\" + 0.010*\"equipment\" + 0.009*\"somewhere\" + 0.009*\"importance\" + 0.009*\"magazine:\" + 0.008*\"???????????????????????????????????????\" + 0.008*\"uh\" + 0.007*\"flow\"\n",
            "topic #7 (0.033): 0.025*\"rosa\" + 0.022*\"parks\" + 0.020*\"lol.\" + 0.015*\"park\" + 0.014*\"55\" + 0.014*\"dat\" + 0.014*\"her\" + 0.013*\"ago\" + 0.012*\"#blog\" + 0.012*\"#nowplaying\"\n",
            "topic diff=0.074415, rho=0.220694\n",
            "PROGRESS: pass 1, at document #4000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1964/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #21 (0.033): 0.023*\"(@\" + 0.021*\"$\" + 0.011*\"year's\" + 0.011*\"shuttle\" + 0.011*\"atlanta\" + 0.010*\"space\" + 0.010*\"victoria\" + 0.010*\"ha\" + 0.010*\"knowing\" + 0.008*\"tourist\"\n",
            "topic #15 (0.033): 0.026*\"at\" + 0.013*\"from\" + 0.013*\"&\" + 0.012*\"...\" + 0.011*\"new\" + 0.009*\"snow\" + 0.008*\"-\" + 0.007*\"by\" + 0.006*\"as\" + 0.005*\"nyc\"\n",
            "topic #4 (0.033): 0.076*\"twitter's\" + 0.033*\"featured\" + 0.009*\"morning!\" + 0.009*\"taylor\" + 0.008*\"@nytimes:\" + 0.007*\"wonderful\" + 0.006*\"plus\" + 0.006*\"jobs\" + 0.006*\"economy\" + 0.006*\"fewer\"\n",
            "topic #24 (0.033): 0.033*\";-)\" + 0.014*\"ones\" + 0.011*\"wine\" + 0.010*\"this?\" + 0.007*\"roads\" + 0.007*\"trailer\" + 0.007*\"//\" + 0.006*\"az\" + 0.006*\"wow\" + 0.006*\"care.\"\n",
            "topic #7 (0.033): 0.025*\"rosa\" + 0.022*\"parks\" + 0.021*\"lol.\" + 0.016*\"her\" + 0.015*\"55\" + 0.014*\"park\" + 0.013*\"ago\" + 0.012*\"dat\" + 0.012*\"seat\" + 0.011*\"years\"\n",
            "topic diff=0.097029, rho=0.220694\n",
            "PROGRESS: pass 1, at document #6000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1963/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #11 (0.033): 0.039*\":-)\" + 0.028*\"cnn\" + 0.025*\"pm\" + 0.023*\"update\" + 0.018*\"van\" + 0.017*\"dubai\" + 0.012*\"darfur\" + 0.011*\"11\" + 0.009*\"promo\" + 0.009*\"er\"\n",
            "topic #29 (0.033): 0.016*\"they\" + 0.013*\"are\" + 0.012*\"what\" + 0.012*\"that\" + 0.011*\"who\" + 0.011*\"about\" + 0.010*\"you\" + 0.009*\"not\" + 0.009*\"#dadt\" + 0.009*\"more\"\n",
            "topic #15 (0.033): 0.026*\"at\" + 0.014*\"&\" + 0.014*\"from\" + 0.011*\"new\" + 0.011*\"...\" + 0.009*\"snow\" + 0.008*\"-\" + 0.007*\"by\" + 0.006*\"as\" + 0.005*\"dec\"\n",
            "topic #4 (0.033): 0.065*\"twitter's\" + 0.029*\"featured\" + 0.009*\"taylor\" + 0.008*\"morning!\" + 0.007*\"@nytimes:\" + 0.007*\"wonderful\" + 0.006*\"jobs\" + 0.006*\"plus\" + 0.006*\"fewer\" + 0.006*\"economy\"\n",
            "topic #2 (0.033): 0.031*\"it\" + 0.016*\"be\" + 0.015*\"my\" + 0.014*\"you\" + 0.011*\"lol\" + 0.011*\"not\" + 0.010*\"just\" + 0.009*\"with\" + 0.009*\"that\" + 0.009*\"going\"\n",
            "topic diff=0.091557, rho=0.220694\n",
            "PROGRESS: pass 1, at document #8000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1961/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #6 (0.033): 0.041*\"tax\" + 0.022*\"cuts\" + 0.021*\"#p2\" + 0.018*\"gop\" + 0.018*\"#tcot\" + 0.013*\"senate\" + 0.012*\"...\" + 0.011*\"vote\" + 0.011*\"obama\" + 0.010*\"--\"\n",
            "topic #4 (0.033): 0.054*\"twitter's\" + 0.023*\"featured\" + 0.010*\"morning!\" + 0.009*\"wonderful\" + 0.008*\"plus\" + 0.008*\"jobs\" + 0.008*\"taylor\" + 0.007*\"@nytimes:\" + 0.007*\"economy\" + 0.006*\"39,000\"\n",
            "topic #8 (0.033): 0.053*\"!\" + 0.012*\"post:\" + 0.009*\"me:\" + 0.009*\"new\" + 0.009*\"#business\" + 0.007*\"#economist\" + 0.007*\"@thedailybeast:\" + 0.006*\"#photo\" + 0.006*\"sundance\" + 0.006*\"angels\"\n",
            "topic #7 (0.033): 0.032*\"lol.\" + 0.025*\"rosa\" + 0.023*\"parks\" + 0.016*\"her\" + 0.013*\"park\" + 0.013*\"55\" + 0.012*\"ago\" + 0.011*\"seat\" + 0.011*\"years\" + 0.009*\"#nowplaying\"\n",
            "topic #13 (0.033): 0.046*\"delivered\" + 0.017*\"hanukkah\" + 0.015*\"(\" + 0.013*\"happy\" + 0.013*\")\" + 0.012*\"robert\" + 0.009*\"pattinson\" + 0.009*\"une\" + 0.008*\"kristen\" + 0.007*\",\"\n",
            "topic diff=0.060179, rho=0.220694\n",
            "PROGRESS: pass 1, at document #10000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1965/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #19 (0.033): 0.052*\"~\" + 0.018*\"other.\" + 0.012*\"each\" + 0.008*\"dress\" + 0.007*\"....\" + 0.006*\"china,\" + 0.006*\"blow\" + 0.005*\"turned\" + 0.005*\"centre\" + 0.004*\"friendly\"\n",
            "topic #17 (0.033): 0.057*\"via\" + 0.030*\"|\" + 0.025*\"by\" + 0.022*\"top\" + 0.020*\"-\" + 0.019*\"daily\" + 0.018*\"?\" + 0.015*\"stories\" + 0.014*\"today\" + 0.012*\"out!\"\n",
            "topic #20 (0.033): 0.073*\"#fb\" + 0.019*\"#ukuncut\" + 0.017*\"trust\" + 0.016*\"#football\" + 0.012*\"ron\" + 0.008*\"..\" + 0.007*\"chicago\" + 0.007*\"ford\" + 0.007*\"rose\" + 0.007*\"brawl\"\n",
            "topic #11 (0.033): 0.030*\":-)\" + 0.026*\"pm\" + 0.023*\"cnn\" + 0.018*\"update\" + 0.017*\"van\" + 0.012*\"dubai\" + 0.011*\"11\" + 0.010*\"op\" + 0.009*\"een\" + 0.009*\"tune\"\n",
            "topic #0 (0.033): 0.055*\"your\" + 0.034*\"you\" + 0.019*\"&\" + 0.015*\"our\" + 0.015*\"tweet\" + 0.014*\"this\" + 0.011*\"best\" + 0.010*\"christmas\" + 0.009*\"holiday\" + 0.009*\"love\"\n",
            "topic diff=0.076291, rho=0.220694\n",
            "PROGRESS: pass 1, at document #12000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1972/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #18 (0.033): 0.022*\"you\" + 0.015*\"that\" + 0.014*\"my\" + 0.012*\"have\" + 0.011*\"are\" + 0.010*\"it\" + 0.009*\"be\" + 0.009*\"...\" + 0.008*\"not\" + 0.008*\"with\"\n",
            "topic #7 (0.033): 0.038*\"lol.\" + 0.025*\"rosa\" + 0.019*\"parks\" + 0.018*\"her\" + 0.014*\"park\" + 0.014*\"years\" + 0.013*\"ago\" + 0.012*\"#england2018\" + 0.012*\"55\" + 0.011*\"#nowplaying\"\n",
            "topic #8 (0.033): 0.050*\"!\" + 0.016*\"post:\" + 0.011*\"@thedailybeast:\" + 0.008*\"me:\" + 0.008*\"new\" + 0.007*\"#business\" + 0.006*\"nj\" + 0.006*\"large\" + 0.006*\"#india\" + 0.006*\"#economist\"\n",
            "topic #24 (0.033): 0.025*\";-)\" + 0.015*\"blows\" + 0.014*\"ones\" + 0.013*\"this?\" + 0.009*\"wine\" + 0.009*\"wow\" + 0.008*\"discovered\" + 0.007*\"roads\" + 0.007*\"trailer\" + 0.007*\"to:\"\n",
            "topic #0 (0.033): 0.055*\"your\" + 0.033*\"you\" + 0.019*\"&\" + 0.016*\"our\" + 0.014*\"this\" + 0.014*\"tweet\" + 0.011*\"best\" + 0.010*\"christmas\" + 0.010*\"holiday\" + 0.009*\"love\"\n",
            "topic diff=0.067642, rho=0.220694\n",
            "PROGRESS: pass 1, at document #14000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1962/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #12 (0.033): 0.044*\":\" + 0.026*\"la\" + 0.024*\"le\" + 0.021*\"de\" + 0.017*\"les\" + 0.017*\"et\" + 0.015*\"-\" + 0.014*\"di\" + 0.012*\"à\" + 0.012*\"des\"\n",
            "topic #18 (0.033): 0.021*\"you\" + 0.014*\"my\" + 0.014*\"that\" + 0.012*\"have\" + 0.011*\"are\" + 0.010*\"it\" + 0.009*\"be\" + 0.009*\"...\" + 0.008*\"not\" + 0.008*\"with\"\n",
            "topic #20 (0.033): 0.064*\"#fb\" + 0.019*\"trust\" + 0.018*\"#ukuncut\" + 0.014*\"ron\" + 0.013*\"#football\" + 0.009*\"rose\" + 0.009*\"chicago\" + 0.008*\"figure\" + 0.007*\"ford\" + 0.007*\"santo\"\n",
            "topic #11 (0.033): 0.028*\":-)\" + 0.026*\"pm\" + 0.020*\"cnn\" + 0.019*\"van\" + 0.017*\"update\" + 0.012*\"tune\" + 0.011*\"het\" + 0.010*\"11\" + 0.009*\"een\" + 0.009*\"dubai\"\n",
            "topic #16 (0.033): 0.068*\"-\" + 0.027*\"bbc:\" + 0.019*\"social\" + 0.014*\"iphone\" + 0.012*\"media\" + 0.011*\"ranking\" + 0.010*\"facebook\" + 0.009*\"@mashable:\" + 0.008*\"app\" + 0.008*\"week\"\n",
            "topic diff=0.054988, rho=0.220694\n",
            "PROGRESS: pass 1, at document #16000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1970/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #1 (0.033): 0.030*\"-\" + 0.027*\"...\" + 0.026*\"at\" + 0.015*\"this\" + 0.015*\"with\" + 0.010*\"out\" + 0.009*\"from\" + 0.009*\"new\" + 0.008*\"check\" + 0.006*\"&\"\n",
            "topic #4 (0.033): 0.024*\"twitter's\" + 0.012*\"featured\" + 0.012*\"morning!\" + 0.011*\"jobs\" + 0.010*\"plus\" + 0.009*\"rate\" + 0.009*\"economy\" + 0.008*\"wonderful\" + 0.008*\"@nytimes:\" + 0.007*\"39,000\"\n",
            "topic #9 (0.033): 0.085*\"/\" + 0.019*\"2000\" + 0.012*\"????????????????????????????\" + 0.011*\"teen\" + 0.010*\"flow\" + 0.008*\"soup\" + 0.008*\"waldo\" + 0.007*\"uh\" + 0.007*\"importance\" + 0.007*\"pepper\"\n",
            "topic #10 (0.033): 0.012*\"–\" + 0.007*\"#iranelection\" + 0.007*\"#iran\" + 0.006*\"rights\" + 0.006*\"freedom\" + 0.005*\"@mparent77772:\" + 0.005*\"full\" + 0.005*\"#israel\" + 0.005*\"health\" + 0.005*\"itunes\"\n",
            "topic #6 (0.033): 0.040*\"tax\" + 0.023*\"gop\" + 0.022*\"#p2\" + 0.022*\"cuts\" + 0.016*\"#tcot\" + 0.014*\"senate\" + 0.013*\"bush\" + 0.013*\"--\" + 0.012*\"vote\" + 0.012*\"obama\"\n",
            "topic diff=0.052901, rho=0.220694\n",
            "PROGRESS: pass 1, at document #18000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "1975/2000 documents converged within 50 iterations\n",
            "updating topics\n",
            "merging changes from 2000 documents into a model of 37063 documents\n",
            "topic #8 (0.033): 0.056*\"!\" + 0.015*\"post:\" + 0.010*\"#business\" + 0.009*\"@thedailybeast:\" + 0.008*\"me:\" + 0.008*\"angels\" + 0.007*\"new\" + 0.007*\"village\" + 0.006*\"nj\" + 0.006*\"tonight:\"\n",
            "topic #4 (0.033): 0.020*\"twitter's\" + 0.012*\"morning!\" + 0.012*\"jobs\" + 0.011*\"featured\" + 0.010*\"economy\" + 0.009*\"plus\" + 0.009*\"rate\" + 0.008*\"wonderful\" + 0.008*\"39,000\" + 0.007*\"fewer\"\n",
            "topic #25 (0.033): 0.039*\"world\" + 0.037*\"aids\" + 0.026*\"(cont)\" + 0.024*\"day\" + 0.019*\"&\" + 0.018*\"...\" + 0.016*\"today\" + 0.015*\"#ff\" + 0.011*\"hiv\" + 0.008*\"day.\"\n",
            "topic #18 (0.033): 0.022*\"you\" + 0.015*\"my\" + 0.014*\"that\" + 0.012*\"have\" + 0.011*\"are\" + 0.011*\"it\" + 0.010*\"be\" + 0.009*\"...\" + 0.008*\"not\" + 0.008*\"we\"\n",
            "topic #29 (0.033): 0.016*\"they\" + 0.014*\"what\" + 0.013*\"are\" + 0.012*\"about\" + 0.012*\"that\" + 0.012*\"who\" + 0.011*\"he\" + 0.010*\"more\" + 0.010*\"not\" + 0.010*\"you\"\n",
            "topic diff=0.038259, rho=0.220694\n",
            "bound: at document #0\n",
            "-15.272 per-word bound, 39564.0 perplexity estimate based on a held-out corpus of 2000 documents with 97368 words\n",
            "PROGRESS: pass 1, at document #20000/37063\n",
            "performing inference on a chunk of 2000 documents\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 48, in RunPipeline\n",
            "    dictionary = gensim.corpora.Dictionary.load(f\"{params.tml['path2save']}/{params.tml['library']}_{params.tml['num_topics']}topics_TopicModelingDictionary.mm\")\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/utils.py\", line 435, in load\n",
            "    obj = unpickle(fname)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/utils.py\", line 1395, in unpickle\n",
            "    with open(fname, 'rb') as f:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\", line 184, in open\n",
            "    newline=newline,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\", line 363, in _shortcut_open\n",
            "    return _builtin_open(local_path, mode, buffering=buffering, **open_kwargs)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/1/tml/gensim_30topics_TopicModelingDictionary.mm'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 95, in <module>\n",
            "    c = RunPipeline()\n",
            "  File \"main.py\", line 57, in RunPipeline\n",
            "    path_2_save_tml=params.tml['path2save'])\n",
            "  File \"/content/SEERa/SEERa/src/tml/TopicModeling.py\", line 22, in topic_modeling\n",
            "    lda_model = gensim.models.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=5)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\", line 519, in __init__\n",
            "    self.update(corpus, chunks_as_numpy=use_numpy)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\", line 980, in update\n",
            "    gammat = self.do_estep(chunk, other)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\", line 742, in do_estep\n",
            "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\", line 685, in inference\n",
            "    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
            "  File \"<__array_function__ internals>\", line 6, in dot\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}