{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --config python3 #3.6 due to dynamimcgem"
      ],
      "metadata": {
        "id": "faeGOx7swFSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c571dc2-e8b6-4cf6-d230-00f07efe5c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.7   2         auto mode\n",
            "  1            /usr/bin/python3.6   1         manual mode\n",
            "  2            /usr/bin/python3.7   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/bin/python3.6 to provide /usr/bin/python3 (python3) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3-pip\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install ipykernel"
      ],
      "metadata": {
        "id": "hMhgG7g5OYpo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01f43bb9-2fee-4496-8a91-cfa282f751cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pkg-resources python3-secretstorage python3-setuptools python3-six\n",
            "  python3-wheel python3-xdg\n",
            "Suggested packages:\n",
            "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
            "  python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
            "  python3-six python3-wheel python3-xdg\n",
            "0 upgraded, 15 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 2,882 kB of archives.\n",
            "After this operation, 8,886 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.5 [1,653 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.4 [220 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.5 [114 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-xdg all 0.25-4ubuntu1.1 [31.3 kB]\n",
            "Fetched 2,882 kB in 0s (16.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 15.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../01-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../02-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../03-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../04-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../05-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../06-python3-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../07-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../08-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../09-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../10-python3-pip_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../11-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../12-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../13-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../14-python3-xdg_0.25-4ubuntu1.1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pip\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/6d/6463d49a933f547439d6b5b98b46af8742cc03ae83543e4d7688c2420f8b/pip-21.3.1-py3-none-any.whl (1.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.7MB 846kB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 9.0.1\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "Successfully installed pip-21.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\n",
            "     |████████████████████████████████| 121 kB 23.7 MB/s            \n",
            "\u001b[?25hCollecting tornado>=4.2\n",
            "  Downloading tornado-6.1-cp36-cp36m-manylinux2010_x86_64.whl (427 kB)\n",
            "     |████████████████████████████████| 427 kB 69.6 MB/s            \n",
            "\u001b[?25hCollecting ipython>=5.0.0\n",
            "  Downloading ipython-7.16.3-py3-none-any.whl (783 kB)\n",
            "     |████████████████████████████████| 783 kB 70.6 MB/s            \n",
            "\u001b[?25hCollecting jupyter-client\n",
            "  Downloading jupyter_client-7.1.2-py3-none-any.whl (130 kB)\n",
            "     |████████████████████████████████| 130 kB 80.3 MB/s            \n",
            "\u001b[?25hCollecting traitlets>=4.1.0\n",
            "  Downloading traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
            "     |████████████████████████████████| 75 kB 4.9 MB/s             \n",
            "\u001b[?25hCollecting ipython-genutils\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.31-py3-none-any.whl (382 kB)\n",
            "     |████████████████████████████████| 382 kB 53.3 MB/s            \n",
            "\u001b[?25hCollecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython>=5.0.0->ipykernel) (39.0.1)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting jedi<=0.17.2,>=0.10\n",
            "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 68.8 MB/s            \n",
            "\u001b[?25hCollecting pygments\n",
            "  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 67.4 MB/s            \n",
            "\u001b[?25hCollecting pexpect\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "     |████████████████████████████████| 59 kB 7.8 MB/s             \n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from traitlets>=4.1.0->ipykernel) (1.11.0)\n",
            "Collecting jupyter-core>=4.6.0\n",
            "  Downloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\n",
            "     |████████████████████████████████| 86 kB 7.4 MB/s             \n",
            "\u001b[?25hCollecting pyzmq>=13\n",
            "  Downloading pyzmq-24.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 65.9 MB/s            \n",
            "\u001b[?25hCollecting python-dateutil>=2.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     |████████████████████████████████| 247 kB 77.8 MB/s            \n",
            "\u001b[?25hCollecting nest-asyncio>=1.5\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting parso<0.8.0,>=0.7.0\n",
            "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
            "     |████████████████████████████████| 109 kB 59.5 MB/s            \n",
            "\u001b[?25hCollecting wcwidth\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting ptyprocess>=0.5\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ipython-genutils, decorator, wcwidth, traitlets, ptyprocess, parso, tornado, pyzmq, python-dateutil, pygments, prompt-toolkit, pickleshare, pexpect, nest-asyncio, jupyter-core, jedi, entrypoints, backcall, jupyter-client, ipython, ipykernel\n",
            "Successfully installed backcall-0.2.0 decorator-5.1.1 entrypoints-0.4 ipykernel-5.5.6 ipython-7.16.3 ipython-genutils-0.2.0 jedi-0.17.2 jupyter-client-7.1.2 jupyter-core-4.9.2 nest-asyncio-1.5.6 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.31 ptyprocess-0.7.0 pygments-2.13.0 python-dateutil-2.8.2 pyzmq-24.0.1 tornado-6.1 traitlets-4.3.3 wcwidth-0.2.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "ipython_genutils",
                  "pexpect",
                  "pickleshare",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%rm -R SEERa\n",
        "!git clone https://github.com/fani-lab/SEERa.git\n",
        "%cd SEERa/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "cTadusMKXPRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "193fd646-f63e-4f20-a10b-a2a85f16451b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SEERa'...\n",
            "remote: Enumerating objects: 2133, done.\u001b[K\n",
            "remote: Counting objects: 100% (368/368), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 2133 (delta 221), reused 348 (delta 209), pack-reused 1765\u001b[K\n",
            "Receiving objects: 100% (2133/2133), 175.59 MiB | 28.88 MiB/s, done.\n",
            "Resolving deltas: 100% (1221/1221), done.\n",
            "/content/SEERa\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "     |████████████████████████████████| 312 kB 23.0 MB/s            \n",
            "\u001b[?25hCollecting matplotlib==3.0.1\n",
            "  Downloading matplotlib-3.0.1-cp36-cp36m-manylinux1_x86_64.whl (12.9 MB)\n",
            "     |████████████████████████████████| 12.9 MB 58.2 MB/s            \n",
            "\u001b[?25hCollecting tensorflow==1.11.0\n",
            "  Downloading tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0 MB)\n",
            "     |████████████████████████████████| 63.0 MB 1.2 MB/s             \n",
            "\u001b[?25hCollecting h5py==2.8.0\n",
            "  Downloading h5py-2.8.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
            "     |████████████████████████████████| 2.8 MB 63.2 MB/s            \n",
            "\u001b[?25hCollecting Cython>=0.29\n",
            "  Downloading Cython-0.29.32-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n",
            "     |████████████████████████████████| 2.0 MB 43.5 MB/s            \n",
            "\u001b[?25hCollecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
            "     |████████████████████████████████| 24.2 MB 1.3 MB/s             \n",
            "\u001b[?25hCollecting networkx==2.5.1\n",
            "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 58.7 MB/s            \n",
            "\u001b[?25hCollecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "     |████████████████████████████████| 1.5 MB 66.6 MB/s            \n",
            "\u001b[?25hCollecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "     |████████████████████████████████| 14.8 MB 59.0 MB/s            \n",
            "\u001b[?25hCollecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
            "     |████████████████████████████████| 9.5 MB 62.7 MB/s            \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
            "     |████████████████████████████████| 22.2 MB 1.4 MB/s             \n",
            "\u001b[?25hCollecting scikit-network\n",
            "  Downloading scikit_network-0.20.0-cp36-cp36m-manylinux2010_x86_64.whl (7.5 MB)\n",
            "     |████████████████████████████████| 7.5 MB 55.6 MB/s            \n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
            "     |████████████████████████████████| 25.9 MB 1.2 MB/s             \n",
            "\u001b[?25hCollecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting sklearn==0.0\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tagme==0.1.3\n",
            "  Downloading tagme-0.1.3-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting seaborn==0.9.0\n",
            "  Downloading seaborn-0.9.0-py3-none-any.whl (208 kB)\n",
            "     |████████████████████████████████| 208 kB 73.7 MB/s            \n",
            "\u001b[?25hCollecting newspaper3k==0.2.8\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "     |████████████████████████████████| 211 kB 77.2 MB/s            \n",
            "\u001b[?25hCollecting pytrec-eval-terrier==0.5.2\n",
            "  Downloading pytrec_eval_terrier-0.5.2-cp36-cp36m-manylinux2010_x86_64.whl (286 kB)\n",
            "     |████████████████████████████████| 286 kB 78.5 MB/s            \n",
            "\u001b[?25hCollecting ffmpeg==1.4\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitermplus==0.6.10\n",
            "  Downloading bitermplus-0.6.10.tar.gz (614 kB)\n",
            "     |████████████████████████████████| 614 kB 64.9 MB/s            \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
            "     |████████████████████████████████| 603 kB 71.5 MB/s            \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "     |████████████████████████████████| 42 kB 1.4 MB/s             \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "     |████████████████████████████████| 50 kB 7.8 MB/s             \n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 68.0 MB/s            \n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "     |████████████████████████████████| 98 kB 6.9 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.1->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.11.0->-r requirements.txt (line 5)) (0.30.0)\n",
            "Collecting gast>=0.2.0\n",
            "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "     |████████████████████████████████| 4.6 MB 62.0 MB/s            \n",
            "\u001b[?25hCollecting protobuf>=3.6.0\n",
            "  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 48.2 MB/s            \n",
            "\u001b[?25hCollecting absl-py>=0.1.6\n",
            "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "     |████████████████████████████████| 123 kB 64.4 MB/s            \n",
            "\u001b[?25hCollecting tensorboard<1.12.0,>=1.11.0\n",
            "  Downloading tensorboard-1.11.0-py3-none-any.whl (3.0 MB)\n",
            "     |████████████████████████████████| 3.0 MB 62.0 MB/s            \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/lib/python3/dist-packages (from tensorflow==1.11.0->-r requirements.txt (line 5)) (39.0.1)\n",
            "Collecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
            "     |████████████████████████████████| 58 kB 6.1 MB/s             \n",
            "\u001b[?25hCollecting decorator<5,>=4.3\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
            "     |████████████████████████████████| 309 kB 75.0 MB/s            \n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 7.7 MB/s             \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "     |████████████████████████████████| 78 kB 8.7 MB/s             \n",
            "\u001b[?25hCollecting regex\n",
            "  Downloading regex-2022.9.13-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n",
            "     |████████████████████████████████| 756 kB 66.3 MB/s            \n",
            "\u001b[?25hCollecting pytz>=2017.2\n",
            "  Downloading pytz-2022.4-py2.py3-none-any.whl (500 kB)\n",
            "     |████████████████████████████████| 500 kB 75.4 MB/s            \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "     |████████████████████████████████| 63 kB 2.0 MB/s             \n",
            "\u001b[?25hCollecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "     |████████████████████████████████| 829 kB 70.5 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting beautifulsoup4>=4.4.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "     |████████████████████████████████| 128 kB 74.2 MB/s            \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pillow>=3.3.0\n",
            "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "     |████████████████████████████████| 3.1 MB 40.4 MB/s            \n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n",
            "     |████████████████████████████████| 87 kB 6.0 MB/s             \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "     |████████████████████████████████| 81 kB 9.7 MB/s             \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "     |████████████████████████████████| 7.4 MB 61.0 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lxml>=3.6.0\n",
            "  Downloading lxml-4.9.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "     |████████████████████████████████| 6.4 MB 38.3 MB/s            \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
            "     |████████████████████████████████| 161 kB 69.8 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->tagme==0.1.3->-r requirements.txt (line 20)) (2.6)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "     |████████████████████████████████| 140 kB 75.7 MB/s            \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting werkzeug>=0.11.10\n",
            "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
            "     |████████████████████████████████| 289 kB 75.9 MB/s            \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 7.8 MB/s             \n",
            "\u001b[?25hCollecting filelock>=3.0.8\n",
            "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
            "Collecting importlib-resources\n",
            "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: sklearn, ffmpeg, bitermplus, tinysegmenter, feedfinder2, jieba3k, termcolor, future, sgmllib3k\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=2397 sha256=ae97dc975d9c90952463313a926650e3cb8042739cbc7964cfd00332e2f0b1ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6812 sha256=354886258985f733ec522823a1b222de8f3ae512a75923ac13bd91754c2dc30d\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/0d/3c/5a9cbae53044e993a211358ddae115ce34bcbc29f7a3bd9155\n",
            "  Building wheel for bitermplus (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitermplus: filename=bitermplus-0.6.10-cp36-cp36m-linux_x86_64.whl size=937419 sha256=33d8aab9405cd9bf01bd3c6665af0a5d979a2a02157beb43a7c22fadc175328f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/10/a0/501b36efc73676c6ba0c2b1b51010803ae76ad668c980334af\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=14366 sha256=44255efc0508f5f5c9cdf31da0dc003baea196124d13195433d3186277afb970\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/ca/c1/1e27eacc3fd0cca25b5383253141ba07694e926a10c3bdc549\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=4726 sha256=a5ce3040c31066915865cb0f0ae1085dd34b2cf9da3410a1be2daef2a7b22165\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/4b/1d/fefcdd940e06e465f1df6f7ee0d56c931bbeed14328748cfc6\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7399104 sha256=630487ba38192ee7c3bcbfa73543e76e97e9a438b67f2548c84fc00ab3acbb8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/f5/11/d67e35ef63e5397aa052f4c84ed5fc6a65bbd9ceb9089cc93b\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=5679 sha256=7d6bcdb25d1f2438279efc435c10c61eb98db2e1281c5018295870767ae51479\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=493275 sha256=dd9aea5a9fd699971900ff1751a8c51c38e850d63870e36c8f7ad04c35ffe498\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=7082 sha256=ff7aaafcb5f5bdc868539ea8fdef6f7848a0305b87a2dfd5340139b019bd5a92\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/cb/26/83c0b63161dc478ded6db8d83a148d32b6cce8606043c2023b\n",
            "Successfully built sklearn ffmpeg bitermplus tinysegmenter feedfinder2 jieba3k termcolor future sgmllib3k\n",
            "Installing collected packages: zipp, urllib3, typing-extensions, charset-normalizer, certifi, soupsieve, six, requests, numpy, importlib-resources, importlib-metadata, dataclasses, werkzeug, tqdm, threadpoolctl, sgmllib3k, scipy, requests-file, regex, pytz, pyparsing, protobuf, markdown, kiwisolver, joblib, h5py, grpcio, filelock, cycler, click, beautifulsoup4, tldextract, tinysegmenter, termcolor, tensorboard, smart-open, scikit-learn, pyyaml, Pillow, pandas, nltk, matplotlib, lxml, keras-preprocessing, keras-applications, jieba3k, gast, future, feedparser, feedfinder2, decorator, Cython, cssselect, astor, absl-py, tensorflow, tagme, sklearn, seaborn, scikit-network, pytrec-eval-terrier, newspaper3k, networkx, Keras, gensim, ffmpeg, bitermplus\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.11.0\n",
            "    Uninstalling six-1.11.0:\n",
            "      Successfully uninstalled six-1.11.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "Successfully installed Cython-0.29.32 Keras-2.2.4 Pillow-8.4.0 absl-py-1.2.0 astor-0.8.1 beautifulsoup4-4.11.1 bitermplus-0.6.10 certifi-2022.9.24 charset-normalizer-2.0.12 click-8.0.4 cssselect-1.1.0 cycler-0.11.0 dataclasses-0.8 decorator-4.4.2 feedfinder2-0.0.4 feedparser-6.0.10 ffmpeg-1.4 filelock-3.4.1 future-0.18.2 gast-0.5.3 gensim-3.8.3 grpcio-1.48.2 h5py-2.8.0 importlib-metadata-4.8.3 importlib-resources-5.4.0 jieba3k-0.35.1 joblib-1.1.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 kiwisolver-1.3.1 lxml-4.9.1 markdown-3.3.7 matplotlib-3.0.1 networkx-2.5.1 newspaper3k-0.2.8 nltk-3.6.2 numpy-1.19.5 pandas-1.1.5 protobuf-3.19.6 pyparsing-3.0.9 pytrec-eval-terrier-0.5.2 pytz-2022.4 pyyaml-6.0 regex-2022.9.13 requests-2.27.1 requests-file-1.5.1 scikit-learn-0.24.2 scikit-network-0.20.0 scipy-1.5.4 seaborn-0.9.0 sgmllib3k-1.0.0 six-1.16.0 sklearn-0.0 smart-open-6.2.0 soupsieve-2.3.2.post1 tagme-0.1.3 tensorboard-1.11.0 tensorflow-1.11.0 termcolor-1.1.0 threadpoolctl-3.1.0 tinysegmenter-0.3 tldextract-3.1.2 tqdm-4.64.1 typing-extensions-4.1.1 urllib3-1.26.12 werkzeug-2.0.3 zipp-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor",
                  "certifi",
                  "cycler",
                  "decorator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you need to install the following library from its source in your machine (not colab), and set an environment variable for that to be able to use it:\n",
        "\n",
        "- [MAchine Learning for LanguagE Toolkit (mallet)](/http://mallet.cs.umass.edu/index.php) as a requirement in tml "
      ],
      "metadata": {
        "id": "WK6NaQcrtuLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/palash1992/DynamicGEM.git\n",
        "%cd DynamicGEM/\n",
        "!python setup.py install\n",
        "!pip install tensorflow==1.11.0 --force-reinstall  #may be needed\n",
        "%cd .."
      ],
      "metadata": {
        "id": "RrkSxnOJ_2qD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61c26de1-dd7b-424f-f9ab-c08e86f9b606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DynamicGEM'...\n",
            "remote: Enumerating objects: 1769, done.\u001b[K\n",
            "remote: Total 1769 (delta 0), reused 0 (delta 0), pack-reused 1769\u001b[K\n",
            "Receiving objects: 100% (1769/1769), 8.89 MiB | 29.67 MiB/s, done.\n",
            "Resolving deltas: 100% (1091/1091), done.\n",
            "/content/SEERa/DynamicGEM\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating dynamicgem.egg-info\n",
            "writing dynamicgem.egg-info/PKG-INFO\n",
            "writing dependency_links to dynamicgem.egg-info/dependency_links.txt\n",
            "writing requirements to dynamicgem.egg-info/requires.txt\n",
            "writing top-level names to dynamicgem.egg-info/top_level.txt\n",
            "writing manifest file 'dynamicgem.egg-info/SOURCES.txt'\n",
            "reading manifest file 'dynamicgem.egg-info/SOURCES.txt'\n",
            "writing manifest file 'dynamicgem.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/dynamicgem\n",
            "copying dynamicgem/version.py -> build/lib/dynamicgem\n",
            "copying dynamicgem/__init__.py -> build/lib/dynamicgem\n",
            "creating build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/ts_utils.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/embed_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/__init__.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/graph_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/plot_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/evaluation_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/fig_util.py -> build/lib/dynamicgem/utils\n",
            "copying dynamicgem/utils/dataprep_util.py -> build/lib/dynamicgem/utils\n",
            "creating build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/static_military_call_graph_v1.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/createHepTHCollabNet_nx.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/dynamic_military_call_graph_v1.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/static_military_call_graph_v2.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/SBM_node_migration.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/dynamic_military_call_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/static_military_call_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/dynamic_SBM_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/__init__.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/testgraphgen.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/SBM_graph.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/motivation.py -> build/lib/dynamicgem/graph_generation\n",
            "copying dynamicgem/graph_generation/getAS_nx.py -> build/lib/dynamicgem/graph_generation\n",
            "creating build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/evaluate_graph_reconstruction.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/visualize_embedding.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/metrics.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/evaluate_link_prediction.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/__init__.py -> build/lib/dynamicgem/evaluation\n",
            "copying dynamicgem/evaluation/standardTest.py -> build/lib/dynamicgem/evaluation\n",
            "creating build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/plot_static_embedding.py -> build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/plot_dynamic_sbm_embedding.py -> build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/__init__.py -> build/lib/dynamicgem/visualization\n",
            "copying dynamicgem/visualization/plot_dynamic_embedding.py -> build/lib/dynamicgem/visualization\n",
            "creating build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynRNN.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/TIMERS.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dnn_utils.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynSDNE.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynamicTriad.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/ae_static.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/__init__.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynAE.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/static_graph_embedding.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/graphFac_dynamic.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynAERNN.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/dynamic_graph_embedding.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/sdne_utils.py -> build/lib/dynamicgem/embedding\n",
            "copying dynamicgem/embedding/sdne_dynamic.py -> build/lib/dynamicgem/embedding\n",
            "creating build/lib/dynamicgem/dynamictriad\n",
            "copying dynamicgem/dynamictriad/__init__.py -> build/lib/dynamicgem/dynamictriad\n",
            "copying dynamicgem/dynamictriad/__main__.py -> build/lib/dynamicgem/dynamictriad\n",
            "creating build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/mygraph_utils.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/gconfig.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/__init__.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/graphtool_utils.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/utils.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/gconv.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "copying dynamicgem/dynamictriad/core/utils_py.py -> build/lib/dynamicgem/dynamictriad/core\n",
            "creating build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/academic2adjlist.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/__init__.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/test.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "copying dynamicgem/dynamictriad/scripts/stdtests.py -> build/lib/dynamicgem/dynamictriad/scripts\n",
            "creating build/lib/dynamicgem/dynamictriad/core/kerasext\n",
            "copying dynamicgem/dynamictriad/core/kerasext/__init__.py -> build/lib/dynamicgem/dynamictriad/core/kerasext\n",
            "creating build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/adjlist.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/__init__.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/dataset_utils.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "copying dynamicgem/dynamictriad/core/dataset/citation.py -> build/lib/dynamicgem/dynamictriad/core/dataset\n",
            "creating build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "copying dynamicgem/dynamictriad/core/algorithm/embutils.py -> build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "copying dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py -> build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "copying dynamicgem/dynamictriad/core/algorithm/__init__.py -> build/lib/dynamicgem/dynamictriad/core/algorithm\n",
            "creating build/lib/dynamicgem/dynamictriad/core/cython_src\n",
            "copying dynamicgem/dynamictriad/core/cython_src/__init__.py -> build/lib/dynamicgem/dynamictriad/core/cython_src\n",
            "creating build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "creating build/lib/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying dynamicgem/dynamictriad/core/kerasext/debug/__init__.py -> build/lib/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "creating build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py -> build/lib/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/ts_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/embed_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/graph_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/plot_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/evaluation_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/fig_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/dataprep_util.py -> build/bdist.linux-x86_64/egg/dynamicgem/utils\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph_v1.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/createHepTHCollabNet_nx.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_military_call_graph_v1.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph_v2.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/SBM_node_migration.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_military_call_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_SBM_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/testgraphgen.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/SBM_graph.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/motivation.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/getAS_nx.py -> build/bdist.linux-x86_64/egg/dynamicgem/graph_generation\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/evaluate_graph_reconstruction.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/visualize_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/metrics.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/evaluate_link_prediction.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/standardTest.py -> build/bdist.linux-x86_64/egg/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/version.py -> build/bdist.linux-x86_64/egg/dynamicgem\n",
            "copying build/lib/dynamicgem/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_static_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_dynamic_sbm_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_dynamic_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/visualization\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynRNN.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/TIMERS.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dnn_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynSDNE.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynamicTriad.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/ae_static.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynAE.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/static_graph_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/graphFac_dynamic.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynAERNN.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynamic_graph_embedding.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/sdne_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/sdne_dynamic.py -> build/bdist.linux-x86_64/egg/dynamicgem/embedding\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/mygraph_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/debug/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/adjlist.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/dataset_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/citation.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/gconfig.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/graphtool_utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/embutils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/utils.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/gconv.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/cython_src\n",
            "copying build/lib/dynamicgem/dynamictriad/core/cython_src/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/cython_src\n",
            "copying build/lib/dynamicgem/dynamictriad/core/utils_py.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core\n",
            "creating build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/academic2adjlist.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/test.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/stdtests.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/__init__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad\n",
            "copying build/lib/dynamicgem/dynamictriad/__main__.py -> build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/ts_utils.py to ts_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/embed_util.py to embed_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/graph_util.py to graph_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/plot_util.py to plot_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/evaluation_util.py to evaluation_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/fig_util.py to fig_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/utils/dataprep_util.py to dataprep_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/static_military_call_graph_v1.py to static_military_call_graph_v1.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/createHepTHCollabNet_nx.py to createHepTHCollabNet_nx.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/dynamic_military_call_graph_v1.py to dynamic_military_call_graph_v1.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/static_military_call_graph_v2.py to static_military_call_graph_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/SBM_node_migration.py to SBM_node_migration.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/dynamic_military_call_graph.py to dynamic_military_call_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/static_military_call_graph.py to static_military_call_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/dynamic_SBM_graph.py to dynamic_SBM_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/testgraphgen.py to testgraphgen.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/SBM_graph.py to SBM_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/motivation.py to motivation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/graph_generation/getAS_nx.py to getAS_nx.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/evaluate_graph_reconstruction.py to evaluate_graph_reconstruction.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/visualize_embedding.py to visualize_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/evaluate_link_prediction.py to evaluate_link_prediction.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/evaluation/standardTest.py to standardTest.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/version.py to version.cpython-36.pyc\n",
            "Sorry: IndentationError: unexpected indent (version.py, line 3)\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/plot_static_embedding.py to plot_static_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/plot_dynamic_sbm_embedding.py to plot_dynamic_sbm_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/visualization/plot_dynamic_embedding.py to plot_dynamic_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynRNN.py to dynRNN.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/TIMERS.py to TIMERS.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dnn_utils.py to dnn_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynSDNE.py to dynSDNE.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynamicTriad.py to dynamicTriad.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/ae_static.py to ae_static.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynAE.py to dynAE.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/static_graph_embedding.py to static_graph_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/graphFac_dynamic.py to graphFac_dynamic.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynAERNN.py to dynAERNN.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/dynamic_graph_embedding.py to dynamic_graph_embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/sdne_utils.py to sdne_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/embedding/sdne_dynamic.py to sdne_dynamic.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/mygraph_utils.py to mygraph_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py to tensorflow_patches.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py to theano_patches.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py to finite_number_check.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/kerasext/debug/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/adjlist.py to adjlist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/dataset_utils.py to dataset_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/dataset/citation.py to citation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/gconfig.py to gconfig.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/graphtool_utils.py to graphtool_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py to sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py to pos_neg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py to pos_neg_tri.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/embutils.py to embutils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py to dynamic_triad.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/algorithm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/gconv.py to gconv.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/cython_src/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/core/utils_py.py to utils_py.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/academic2adjlist.py to academic2adjlist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/test.py to test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/scripts/stdtests.py to stdtests.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicgem/dynamictriad/__main__.py to __main__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicgem.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "dynamicgem.dynamictriad.core.__pycache__.__init__.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.core.__pycache__.utils.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.scripts.__pycache__.academic2adjlist.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.scripts.__pycache__.stdtests.cpython-36: module references __file__\n",
            "dynamicgem.dynamictriad.scripts.__pycache__.test.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.TIMERS.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.ae_static.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.dynSDNE.cpython-36: module references __file__\n",
            "dynamicgem.embedding.__pycache__.dynamicTriad.cpython-36: module references __file__\n",
            "dynamicgem.evaluation.__pycache__.standardTest.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/dynamicgem-1.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing dynamicgem-1.0.0-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem-1.0.0-py3.6.egg\n",
            "Extracting dynamicgem-1.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Sorry: IndentationError: unexpected indent (version.py, line 3)\n",
            "Adding dynamicgem 1.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/dynamicgem-1.0.0-py3.6.egg\n",
            "Processing dependencies for dynamicgem==1.0.0\n",
            "Searching for tensorflow==1.11.0\n",
            "Best match: tensorflow 1.11.0\n",
            "Adding tensorflow 1.11.0 to easy-install.pth file\n",
            "Installing freeze_graph script to /usr/local/bin\n",
            "Installing saved_model_cli script to /usr/local/bin\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "Installing tflite_convert script to /usr/local/bin\n",
            "Installing toco script to /usr/local/bin\n",
            "Installing toco_from_protos script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for sklearn==0.0\n",
            "Best match: sklearn 0.0\n",
            "Adding sklearn 0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for seaborn==0.9.0\n",
            "Best match: seaborn 0.9.0\n",
            "Adding seaborn 0.9.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.5.4\n",
            "Best match: scipy 1.5.4\n",
            "Adding scipy 1.5.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scikit-learn==0.24.2\n",
            "Best match: scikit-learn 0.24.2\n",
            "Adding scikit-learn 0.24.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pandas==1.1.5\n",
            "Best match: pandas 1.1.5\n",
            "Adding pandas 1.1.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for networkx==2.5.1\n",
            "Best match: networkx 2.5.1\n",
            "Adding networkx 2.5.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.0.1\n",
            "Best match: matplotlib 3.0.1\n",
            "Adding matplotlib 3.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for joblib==1.1.1\n",
            "Best match: joblib 1.1.1\n",
            "Adding joblib 1.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.8.0\n",
            "Best match: h5py 2.8.0\n",
            "Adding h5py 2.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras==2.2.4\n",
            "Best match: Keras 2.2.4\n",
            "Adding Keras 2.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Cython==0.29.32\n",
            "Best match: Cython 0.29.32\n",
            "Adding Cython 0.29.32 to easy-install.pth file\n",
            "Installing cygdb script to /usr/local/bin\n",
            "Installing cython script to /usr/local/bin\n",
            "Installing cythonize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==1.2.0\n",
            "Best match: absl-py 1.2.0\n",
            "Adding absl-py 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for protobuf==3.19.6\n",
            "Best match: protobuf 3.19.6\n",
            "Adding protobuf 3.19.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for gast==0.5.3\n",
            "Best match: gast 0.5.3\n",
            "Adding gast 0.5.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard==1.11.0\n",
            "Best match: tensorboard 1.11.0\n",
            "Adding tensorboard 1.11.0 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.1.2\n",
            "Best match: Keras-Preprocessing 1.1.2\n",
            "Adding Keras-Preprocessing 1.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.8\n",
            "Best match: Keras-Applications 1.0.8\n",
            "Adding Keras-Applications 1.0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wheel==0.30.0\n",
            "Best match: wheel 0.30.0\n",
            "Adding wheel 0.30.0 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/lib/python3/dist-packages\n",
            "Searching for grpcio==1.48.2\n",
            "Best match: grpcio 1.48.2\n",
            "Adding grpcio 1.48.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for astor==0.8.1\n",
            "Best match: astor 0.8.1\n",
            "Adding astor 0.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==39.0.1\n",
            "Best match: setuptools 39.0.1\n",
            "Adding setuptools 39.0.1 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "\n",
            "Using /usr/lib/python3/dist-packages\n",
            "Searching for threadpoolctl==3.1.0\n",
            "Best match: threadpoolctl 3.1.0\n",
            "Adding threadpoolctl 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2022.4\n",
            "Best match: pytz 2022.4\n",
            "Adding pytz 2022.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for decorator==4.4.2\n",
            "Best match: decorator 4.4.2\n",
            "Adding decorator 4.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.11.0\n",
            "Best match: cycler 0.11.0\n",
            "Adding cycler 0.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==3.0.9\n",
            "Best match: pyparsing 3.0.9\n",
            "Adding pyparsing 3.0.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.3.1\n",
            "Best match: kiwisolver 1.3.1\n",
            "Adding kiwisolver 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==6.0\n",
            "Best match: PyYAML 6.0\n",
            "Adding PyYAML 6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==2.0.3\n",
            "Best match: Werkzeug 2.0.3\n",
            "Adding Werkzeug 2.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.3.7\n",
            "Best match: Markdown 3.3.7\n",
            "Adding Markdown 3.3.7 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for dataclasses==0.8\n",
            "Best match: dataclasses 0.8\n",
            "Adding dataclasses 0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==4.8.3\n",
            "Best match: importlib-metadata 4.8.3\n",
            "Adding importlib-metadata 4.8.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for typing-extensions==4.1.1\n",
            "Best match: typing-extensions 4.1.1\n",
            "Adding typing-extensions 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.6.0\n",
            "Best match: zipp 3.6.0\n",
            "Adding zipp 3.6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for dynamicgem==1.0.0\n",
            "/usr/lib/python3/dist-packages/setuptools/dist.py:406: UserWarning: The version specified ('R2017a') is an invalid version, this may not work as expected with newer versions of setuptools, pip, and PyPI. Please see PEP 440 for more details.\n",
            "  \"details.\" % self.metadata.version\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib/TIMERS_ALL\n",
            "copying dynamicgem/TIMERS/TIMERS_ALL/for_redistribution_files_only/TIMERS_ALL/__init__.py -> build/lib/TIMERS_ALL\n",
            "copying dynamicgem/TIMERS/TIMERS_ALL/for_redistribution_files_only/TIMERS_ALL/TIMERS_ALL.ctf -> build/lib/TIMERS_ALL\n",
            "running install_lib\n",
            "creating /usr/local/lib/python3.6/dist-packages/TIMERS_ALL\n",
            "copying build/lib/TIMERS_ALL/TIMERS_ALL.ctf -> /usr/local/lib/python3.6/dist-packages/TIMERS_ALL\n",
            "copying build/lib/TIMERS_ALL/__init__.py -> /usr/local/lib/python3.6/dist-packages/TIMERS_ALL\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/ts_utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/embed_util.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/graph_util.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/plot_util.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/evaluation_util.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/fig_util.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "copying build/lib/dynamicgem/utils/dataprep_util.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/utils\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph_v1.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/createHepTHCollabNet_nx.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_military_call_graph_v1.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph_v2.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/SBM_node_migration.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_military_call_graph.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/static_military_call_graph.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/dynamic_SBM_graph.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/testgraphgen.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/SBM_graph.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/motivation.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "copying build/lib/dynamicgem/graph_generation/getAS_nx.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/evaluate_graph_reconstruction.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/visualize_embedding.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/metrics.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/evaluate_link_prediction.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/evaluation/standardTest.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation\n",
            "copying build/lib/dynamicgem/version.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem\n",
            "copying build/lib/dynamicgem/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_static_embedding.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_dynamic_sbm_embedding.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization\n",
            "copying build/lib/dynamicgem/visualization/plot_dynamic_embedding.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynRNN.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/TIMERS.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dnn_utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynSDNE.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynamicTriad.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/ae_static.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynAE.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/static_graph_embedding.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/graphFac_dynamic.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynAERNN.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/dynamic_graph_embedding.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/sdne_utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "copying build/lib/dynamicgem/embedding/sdne_dynamic.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/mygraph_utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "copying build/lib/dynamicgem/dynamictriad/core/kerasext/debug/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/debug\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/adjlist.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/dataset_utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/dataset/citation.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset\n",
            "copying build/lib/dynamicgem/dynamictriad/core/gconfig.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/graphtool_utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/embutils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/algorithm/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm\n",
            "copying build/lib/dynamicgem/dynamictriad/core/utils.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "copying build/lib/dynamicgem/dynamictriad/core/gconv.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/cython_src\n",
            "copying build/lib/dynamicgem/dynamictriad/core/cython_src/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/cython_src\n",
            "copying build/lib/dynamicgem/dynamictriad/core/utils_py.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/academic2adjlist.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/test.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/scripts/stdtests.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts\n",
            "copying build/lib/dynamicgem/dynamictriad/__init__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad\n",
            "copying build/lib/dynamicgem/dynamictriad/__main__.py -> /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/TIMERS_ALL/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/ts_utils.py to ts_utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/embed_util.py to embed_util.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/graph_util.py to graph_util.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/plot_util.py to plot_util.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/evaluation_util.py to evaluation_util.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/fig_util.py to fig_util.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/utils/dataprep_util.py to dataprep_util.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/static_military_call_graph_v1.py to static_military_call_graph_v1.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/createHepTHCollabNet_nx.py to createHepTHCollabNet_nx.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/dynamic_military_call_graph_v1.py to dynamic_military_call_graph_v1.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/static_military_call_graph_v2.py to static_military_call_graph_v2.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/SBM_node_migration.py to SBM_node_migration.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/dynamic_military_call_graph.py to dynamic_military_call_graph.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/static_military_call_graph.py to static_military_call_graph.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/dynamic_SBM_graph.py to dynamic_SBM_graph.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/testgraphgen.py to testgraphgen.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/SBM_graph.py to SBM_graph.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/motivation.py to motivation.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/graph_generation/getAS_nx.py to getAS_nx.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation/evaluate_graph_reconstruction.py to evaluate_graph_reconstruction.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation/visualize_embedding.py to visualize_embedding.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation/evaluate_link_prediction.py to evaluate_link_prediction.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/evaluation/standardTest.py to standardTest.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/version.py to version.cpython-36.pyc\n",
            "Sorry: IndentationError: unexpected indent (version.py, line 3)\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization/plot_static_embedding.py to plot_static_embedding.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization/plot_dynamic_sbm_embedding.py to plot_dynamic_sbm_embedding.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/visualization/plot_dynamic_embedding.py to plot_dynamic_embedding.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynRNN.py to dynRNN.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/TIMERS.py to TIMERS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py to dnn_utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynSDNE.py to dynSDNE.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynamicTriad.py to dynamicTriad.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/ae_static.py to ae_static.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynAE.py to dynAE.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/static_graph_embedding.py to static_graph_embedding.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/graphFac_dynamic.py to graphFac_dynamic.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynAERNN.py to dynAERNN.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynamic_graph_embedding.py to dynamic_graph_embedding.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/sdne_utils.py to sdne_utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/sdne_dynamic.py to sdne_dynamic.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/mygraph_utils.py to mygraph_utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/tensorflow_patches.py to tensorflow_patches.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/keras_backend_patches/theano_patches.py to theano_patches.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/debug/finite_number_check.py to finite_number_check.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/kerasext/debug/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset/adjlist.py to adjlist.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset/dataset_utils.py to dataset_utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/dataset/citation.py to citation.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/gconfig.py to gconfig.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/graphtool_utils.py to graphtool_utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers/sampler.py to sampler.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg.py to pos_neg.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/samplers/pos_neg_tri.py to pos_neg_tri.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/embutils.py to embutils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/dynamic_triad.py to dynamic_triad.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/algorithm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/gconv.py to gconv.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/cython_src/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/core/utils_py.py to utils_py.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts/academic2adjlist.py to academic2adjlist.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts/test.py to test.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/scripts/stdtests.py to stdtests.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/dynamicgem/dynamictriad/__main__.py to __main__.cpython-36.pyc\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating matlabruntimeforpython.egg-info\n",
            "writing matlabruntimeforpython.egg-info/PKG-INFO\n",
            "writing dependency_links to matlabruntimeforpython.egg-info/dependency_links.txt\n",
            "writing top-level names to matlabruntimeforpython.egg-info/top_level.txt\n",
            "writing manifest file 'matlabruntimeforpython.egg-info/SOURCES.txt'\n",
            "reading manifest file 'matlabruntimeforpython.egg-info/SOURCES.txt'\n",
            "writing manifest file 'matlabruntimeforpython.egg-info/SOURCES.txt'\n",
            "Copying matlabruntimeforpython.egg-info to /usr/local/lib/python3.6/dist-packages/matlabruntimeforpython-R2017a.egg-info\n",
            "removing 'build/lib' (and everything under it)\n",
            "removing 'build/bdist.linux-x86_64' (and everything under it)\n",
            "'build/scripts-3.6' does not exist -- can't clean it\n",
            "removing 'build'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.11.0\n",
            "  Using cached tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "Collecting absl-py>=0.1.6\n",
            "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "Collecting tensorboard<1.12.0,>=1.11.0\n",
            "  Using cached tensorboard-1.11.0-py3-none-any.whl (3.0 MB)\n",
            "Collecting protobuf>=3.6.0\n",
            "  Using cached protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting keras-preprocessing>=1.0.3\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting gast>=0.2.0\n",
            "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Using cached grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Collecting wheel>=0.26\n",
            "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Collecting setuptools<=39.1.0\n",
            "  Downloading setuptools-39.1.0-py2.py3-none-any.whl (566 kB)\n",
            "     |████████████████████████████████| 566 kB 26.8 MB/s            \n",
            "\u001b[?25hCollecting keras-applications>=1.0.5\n",
            "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Collecting six>=1.10.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting astor>=0.6.0\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting h5py\n",
            "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
            "     |████████████████████████████████| 4.0 MB 61.4 MB/s            \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
            "Collecting werkzeug>=0.11.10\n",
            "  Using cached Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
            "Collecting dataclasses\n",
            "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Collecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Installing collected packages: zipp, typing-extensions, six, numpy, importlib-metadata, dataclasses, cached-property, wheel, werkzeug, protobuf, markdown, h5py, grpcio, termcolor, tensorboard, setuptools, keras-preprocessing, keras-applications, gast, astor, absl-py, tensorflow\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.6.0\n",
            "    Uninstalling zipp-3.6.0:\n",
            "      Successfully uninstalled zipp-3.6.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.3\n",
            "    Uninstalling importlib-metadata-4.8.3:\n",
            "      Successfully uninstalled importlib-metadata-4.8.3\n",
            "  Attempting uninstall: dataclasses\n",
            "    Found existing installation: dataclasses 0.8\n",
            "    Uninstalling dataclasses-0.8:\n",
            "      Successfully uninstalled dataclasses-0.8\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.30.0\n",
            "    Uninstalling wheel-0.30.0:\n",
            "      Successfully uninstalled wheel-0.30.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.0.3\n",
            "    Uninstalling Werkzeug-2.0.3:\n",
            "      Successfully uninstalled Werkzeug-2.0.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.7\n",
            "    Uninstalling Markdown-3.3.7:\n",
            "      Successfully uninstalled Markdown-3.3.7\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.48.2\n",
            "    Uninstalling grpcio-1.48.2:\n",
            "      Successfully uninstalled grpcio-1.48.2\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.11.0\n",
            "    Uninstalling tensorboard-1.11.0:\n",
            "      Successfully uninstalled tensorboard-1.11.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 39.0.1\n",
            "    Uninstalling setuptools-39.0.1:\n",
            "      Successfully uninstalled setuptools-39.0.1\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: keras-applications\n",
            "    Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.2.0\n",
            "    Uninstalling absl-py-1.2.0:\n",
            "      Successfully uninstalled absl-py-1.2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.11.0\n",
            "    Uninstalling tensorflow-1.11.0:\n",
            "      Successfully uninstalled tensorflow-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dynamicgem 1.0.0 requires h5py==2.8.0, but you have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-1.2.0 astor-0.8.1 cached-property-1.5.2 dataclasses-0.8 gast-0.5.3 grpcio-1.48.2 h5py-3.1.0 importlib-metadata-4.8.3 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.7 numpy-1.19.5 protobuf-3.19.6 setuptools-39.1.0 six-1.16.0 tensorboard-1.11.0 tensorflow-1.11.0 termcolor-1.1.0 typing-extensions-4.1.1 werkzeug-2.0.3 wheel-0.37.1 zipp-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SEERa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rwalk/gsdmm.git\n",
        "%cd gsdmm\n",
        "!python setup.py install\n",
        "%cd .."
      ],
      "metadata": {
        "id": "f9nbJZbKqpwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c23d85-bd17-4b97-a3cf-40a01bc9c480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gsdmm'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Total 62 (delta 0), reused 0 (delta 0), pack-reused 62\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "/content/SEERa/gsdmm\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating gsdmm.egg-info\n",
            "writing gsdmm.egg-info/PKG-INFO\n",
            "writing dependency_links to gsdmm.egg-info/dependency_links.txt\n",
            "writing requirements to gsdmm.egg-info/requires.txt\n",
            "writing top-level names to gsdmm.egg-info/top_level.txt\n",
            "writing manifest file 'gsdmm.egg-info/SOURCES.txt'\n",
            "reading manifest file 'gsdmm.egg-info/SOURCES.txt'\n",
            "writing manifest file 'gsdmm.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/gsdmm\n",
            "copying gsdmm/__init__.py -> build/lib/gsdmm\n",
            "copying gsdmm/mgp.py -> build/lib/gsdmm\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/gsdmm\n",
            "copying build/lib/gsdmm/__init__.py -> build/bdist.linux-x86_64/egg/gsdmm\n",
            "copying build/lib/gsdmm/mgp.py -> build/bdist.linux-x86_64/egg/gsdmm\n",
            "byte-compiling build/bdist.linux-x86_64/egg/gsdmm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/gsdmm/mgp.py to mgp.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gsdmm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gsdmm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gsdmm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gsdmm.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gsdmm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/gsdmm-0.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing gsdmm-0.1-py3.6.egg\n",
            "Copying gsdmm-0.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding gsdmm 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/gsdmm-0.1-py3.6.egg\n",
            "Processing dependencies for gsdmm==0.1\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for gsdmm==0.1\n",
            "/content/SEERa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd src\n",
        "#!python -u main.py -r toy -t lda.gensim lda.mallet gsdmm btm -g AE DynAE DynAERNN\n",
        "!python -u main.py -r toy -t lda.gensim gsdmm btm -g AE DynAE DynAERNN"
      ],
      "metadata": {
        "id": "GFfsQl5qtaIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691c7cb3-b46d-491d-b3ee-a3d2932492e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 558, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042258\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 559, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042220\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 560, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042182\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 561, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042145\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 562, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042108\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 563, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042070\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 564, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.042033\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 565, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041996\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 566, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041959\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 567, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041922\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 568, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041885\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 569, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041849\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 570, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041812\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 571, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041776\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 572, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041739\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 573, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041703\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 574, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041667\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 575, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041631\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 576, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041595\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 577, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041559\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 578, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041523\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 579, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041487\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 580, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041451\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 581, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041416\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 582, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041380\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 583, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041345\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 584, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041310\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 585, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041274\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 586, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041239\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 587, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041204\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 588, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041169\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 589, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041135\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 590, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041100\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 591, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041065\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 592, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.041030\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 593, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040996\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 594, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040962\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 595, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040927\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 596, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040893\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 597, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040859\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 598, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040825\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 599, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040791\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 600, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040757\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 601, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040723\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 602, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040689\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 603, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040656\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 604, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040622\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 605, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040589\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 606, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040555\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 607, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040522\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 608, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040489\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 609, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040456\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 610, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040423\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 611, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040390\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 612, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040357\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 613, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040324\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 614, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040291\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 615, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040258\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 616, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040226\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 617, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040193\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 618, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040161\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 619, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040129\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 620, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040096\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 621, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040064\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 622, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040032\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 623, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.040000\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 624, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039968\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 625, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039936\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 626, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039904\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 627, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039873\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 628, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039841\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 629, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039809\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 630, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039778\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 631, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039746\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 632, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039715\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 633, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039684\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 634, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039653\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 635, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039621\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 636, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039590\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 637, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039559\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 638, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039528\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 639, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039498\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 640, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039467\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 641, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039436\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 642, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039406\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 643, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039375\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 644, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039344\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 645, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039314\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 646, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039284\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 647, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039253\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 648, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039223\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 649, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039193\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 650, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039163\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 651, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039133\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 652, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039103\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 653, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039073\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 654, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039043\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 655, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.039014\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 656, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038984\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 657, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038954\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 658, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038925\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 659, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038895\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 660, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038866\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 661, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038837\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 662, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038808\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 663, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038778\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 664, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038749\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 665, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038720\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 666, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038691\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 667, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038662\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 668, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038633\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 669, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038605\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 670, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038576\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 671, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038547\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 672, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038519\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 673, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038490\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 674, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038462\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 675, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038433\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 676, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038405\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 677, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038376\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 678, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038348\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 679, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038320\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 680, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038292\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 681, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038264\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 682, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038236\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 683, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038208\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 684, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038180\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 685, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038152\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 686, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038125\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 687, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038097\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 688, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038069\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 689, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038042\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 690, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.038014\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 691, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037987\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 692, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037959\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 693, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037932\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 694, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037905\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 695, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037878\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 696, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037851\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 697, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037823\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 698, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037796\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 699, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037769\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 700, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037743\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 701, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037716\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 702, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037689\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 703, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037662\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 704, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037635\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 705, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037609\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 706, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037582\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 707, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037556\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 708, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037529\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 709, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037503\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 710, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037477\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 711, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037450\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 712, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037424\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 713, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037398\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 714, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037372\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 715, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037346\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 716, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037320\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 717, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037294\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 718, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037268\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 719, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037242\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 720, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037216\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 721, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037190\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 722, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037165\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 723, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037139\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 724, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037113\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 725, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037088\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 726, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037062\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 727, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037037\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 728, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.037012\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 729, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036986\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 730, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036961\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 731, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036936\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 732, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036911\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 733, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036886\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 734, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036860\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 735, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036835\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 736, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036811\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 737, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036786\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 738, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036761\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 739, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036736\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 740, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036711\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 741, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036686\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 742, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036662\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 743, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036637\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 744, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036613\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 745, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036588\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 746, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036564\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 747, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036539\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 748, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036515\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 749, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036491\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 750, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036466\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 751, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036442\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 752, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036418\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 753, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036394\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 754, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036370\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 755, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036346\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 756, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036322\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 757, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036298\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 758, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036274\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 759, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036250\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 760, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036226\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 761, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036202\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 762, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036179\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 763, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036155\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 764, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036131\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 765, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036108\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 766, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036084\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 767, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036061\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 768, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036037\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 769, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.036014\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 770, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035991\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 771, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035968\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 772, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035944\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 773, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035921\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 774, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035898\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 775, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035875\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 776, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035852\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 777, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035829\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 778, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035806\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 779, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035783\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 780, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035760\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 781, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035737\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 782, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035714\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 783, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035692\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 784, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035669\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 785, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035646\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 786, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035624\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 787, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035601\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 788, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035578\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 789, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035556\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 790, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035533\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 791, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035511\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 792, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035489\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 793, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035466\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 794, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035444\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 795, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035422\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 796, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035400\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 797, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035377\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 798, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035355\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 799, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035333\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 800, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035311\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 801, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035289\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 802, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035267\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 803, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035245\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 804, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035223\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 805, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035202\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 806, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035180\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 807, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035158\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 808, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035136\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 809, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035115\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 810, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035093\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 811, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035072\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 812, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035050\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 813, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035028\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 814, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.035007\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 815, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034986\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 816, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034964\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 817, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034943\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 818, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034922\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 819, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034900\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 820, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034879\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 821, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034858\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 822, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034837\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 823, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034816\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 824, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034794\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 825, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034773\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 826, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034752\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 827, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034731\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 828, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034711\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 829, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034690\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 830, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034669\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 831, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034648\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 832, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034627\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 833, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034606\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 834, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034586\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 835, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034565\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 836, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034544\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 837, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034524\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 838, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034503\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 839, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034483\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 840, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034462\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 841, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034442\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 842, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034421\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 843, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034401\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 844, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034381\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 845, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034360\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 846, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034340\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 847, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034320\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 848, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034300\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 849, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034280\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 850, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034259\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 851, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034239\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 852, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034219\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 853, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034199\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 854, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034179\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 855, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034159\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 856, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034139\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 857, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034120\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 858, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034100\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 859, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034080\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 860, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034060\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 861, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034040\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 862, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034021\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 863, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.034001\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 864, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033981\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 865, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033962\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 866, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033942\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 867, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033923\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 868, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033903\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 869, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033884\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 870, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033864\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 871, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033845\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 872, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033826\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 873, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033806\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 874, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033787\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 875, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033768\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 876, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033748\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 877, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033729\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 878, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033710\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 879, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033691\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 880, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033672\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 881, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033653\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 882, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033634\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 883, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033615\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 884, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033596\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 885, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033577\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 886, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033558\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 887, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033539\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 888, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033520\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 889, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033501\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 890, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033482\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 891, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033464\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 892, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033445\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 893, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033426\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 894, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033408\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 895, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033389\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 896, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033370\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 897, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033352\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 898, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033333\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 899, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033315\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 900, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033296\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 901, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033278\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 902, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033260\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 903, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033241\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 904, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033223\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 905, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033204\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 906, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033186\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 907, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033168\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 908, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033150\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 909, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033131\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 910, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033113\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 911, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033095\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 912, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033077\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 913, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033059\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 914, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033041\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 915, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033023\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 916, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.033005\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 917, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032987\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 918, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032969\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 919, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032951\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 920, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032933\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 921, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032915\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 922, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032898\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 923, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032880\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 924, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032862\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 925, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032844\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 926, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032827\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 927, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032809\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 928, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032791\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 929, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032774\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 930, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032756\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 931, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032739\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 932, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032721\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 933, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032703\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 934, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032686\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 935, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032669\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 936, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032651\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 937, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032634\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 938, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032616\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 939, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032599\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 940, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032582\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 941, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032564\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 942, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032547\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 943, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032530\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 944, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032513\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 945, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032496\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 946, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032478\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 947, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032461\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 948, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032444\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 949, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032427\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 950, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032410\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 951, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032393\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 952, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032376\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 953, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032359\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 954, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032342\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 955, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032325\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 956, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032309\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 957, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032292\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 958, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032275\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 959, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032258\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 960, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032241\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 961, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032225\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 962, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032208\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 963, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032191\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 964, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032174\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 965, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032158\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 966, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032141\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 967, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032125\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 968, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032108\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 969, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032092\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 970, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032075\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 971, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032059\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 972, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032042\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 973, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032026\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 974, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.032009\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 975, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031993\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 976, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031976\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 977, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031960\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 978, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031944\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 979, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031928\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 980, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031911\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 981, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031895\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 982, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031879\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 983, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031863\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 984, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031846\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 985, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031830\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 986, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031814\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 987, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031798\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 988, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031782\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 989, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031766\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 990, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031750\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 991, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031734\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 992, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031718\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 993, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031702\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 994, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031686\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 995, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031670\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 996, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031654\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 997, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031639\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 998, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031623\n",
            "bound: at document #0\n",
            "-2.570 per-word bound, 5.9 perplexity estimate based on a held-out corpus of 180 documents with 5832 words\n",
            "PROGRESS: pass 999, at document #180/180\n",
            "performing inference on a chunk of 180 documents\n",
            "180/180 documents converged within 1000 iterations\n",
            "updating topics\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "topic diff=0.000000, rho=0.031607\n",
            "saving LdaState object under ../output/toy/lda.gensim.dynaernn/tml/3Topics.model.state, separately None\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3Topics.model.state'}\n",
            "saved ../output/toy/lda.gensim.dynaernn/tml/3Topics.model.state\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3Topics.model.id2word'}\n",
            "saving LdaModel object under ../output/toy/lda.gensim.dynaernn/tml/3Topics.model, separately ['expElogbeta', 'sstats']\n",
            "storing np array 'expElogbeta' to ../output/toy/lda.gensim.dynaernn/tml/3Topics.model.expElogbeta.npy\n",
            "not storing attribute dispatcher\n",
            "not storing attribute state\n",
            "not storing attribute id2word\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3Topics.model'}\n",
            "saved ../output/toy/lda.gensim.dynaernn/tml/3Topics.model\n",
            "topic #0 (0.333): 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "topic #1 (0.333): 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "topic #2 (0.333): 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "GENSIM Topic: 0 \n",
            "Words: 0.102*\"climbing\" + 0.102*\"boxing\" + 0.099*\"football\" + 0.099*\"swimming\" + 0.099*\"basketball\" + 0.099*\"baseball\" + 0.099*\"volleyball\" + 0.099*\"hiking\" + 0.099*\"running\" + 0.099*\"biking\"\n",
            "GENSIM Topic: 1 \n",
            "Words: 0.075*\"discord\" + 0.075*\"snapchat\" + 0.075*\"whatsapp\" + 0.075*\"twitter\" + 0.075*\"tiktok\" + 0.075*\"zoom\" + 0.075*\"skype\" + 0.075*\"linkedin\" + 0.075*\"instagram\" + 0.075*\"facebook\"\n",
            "GENSIM Topic: 2 \n",
            "Words: 0.111*\"mouse\" + 0.111*\"monitor\" + 0.111*\"sony\" + 0.111*\"samsung\" + 0.111*\"dell\" + 0.111*\"apple\" + 0.111*\"digital\" + 0.111*\"microsoft\" + 0.111*\"keyboard\" + 0.000*\"volleyball\"\n",
            "The currently set model 'LdaModel(num_terms=29, num_topics=3, decay=0.5, chunksize=2000)' may be inconsistent with the newly set topics\n",
            "2.2. Quality of topics ...\n",
            "(MeanCoherence): (-1.8255856577886507)\n",
            "(#Topic, Topic Coherences): (3, [-0.17027520791926362, 3.0000446571375978e-12, -5.306481765449688])\n",
            "Time Elapsed: 62.12760543823242\n",
            "\n",
            "3. UML: Temporal Graph Creation ...\n",
            "##################################################\n",
            "3.1. Loading users' graph stream from ../output/toy/lda.gensim.dynaernn/uml/graphs/graphs.pkl ...\n",
            "3.1. Loading users' graph stream failed! Generating the graph stream ...\n",
            "60 users have twitted in 2010-12-01\n",
            "UserSimilarity: UsersTopicInterests.npy is saved for day:2010-12-01 with shape: (60, 3)\n",
            "UserSimilarity: A graph is being created for day: 2010-12-01 with 60 users\n",
            "UserSimilarity: Number of users per day: 60\n",
            "UserSimilarity: Graphs are written in \"graphs\" directory\n",
            "UsersGraph: There are 60 users on 2010-12-01\n",
            "60 users have twitted in 2010-12-02\n",
            "UserSimilarity: UsersTopicInterests.npy is saved for day:2010-12-02 with shape: (60, 3)\n",
            "UserSimilarity: A graph is being created for day: 2010-12-02 with 60 users\n",
            "UserSimilarity: Number of users per day: 60\n",
            "UserSimilarity: Graphs are written in \"graphs\" directory\n",
            "UsersGraph: There are 60 users on 2010-12-02\n",
            "60 users have twitted in 2010-12-03\n",
            "UserSimilarity: UsersTopicInterests.npy is saved for day:2010-12-03 with shape: (60, 3)\n",
            "UserSimilarity: A graph is being created for day: 2010-12-03 with 60 users\n",
            "UserSimilarity: Number of users per day: 60\n",
            "UserSimilarity: Graphs are written in \"graphs\" directory\n",
            "UsersGraph: There are 60 users on 2010-12-03\n",
            "(#Graphs): (3)\n",
            "Time Elapsed: 0.49970555305480957\n",
            "\n",
            "4. GEL: Temporal Graph Embedding ...\n",
            "##################################################\n",
            "4.1. Loading embeddings ...\n",
            "4.1. Loading embeddings failed! Training dynaernn ...\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:491: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(500, activation=<keras.lay..., kernel_regularizer=<keras.reg...)`\n",
            "  W_regularizer=Reg.l1_l2(l1=nu1, l2=nu2))(y[i])\n",
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:491: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, activation=<keras.lay..., kernel_regularizer=<keras.reg...)`\n",
            "  W_regularizer=Reg.l1_l2(l1=nu1, l2=nu2))(y[i])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:493: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=<keras.lay..., kernel_regularizer=<keras.reg...)`\n",
            "  W_regularizer=Reg.l1_l2(l1=nu1, l2=nu2))(y[K - 1])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:495: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  encoder = Model(input=x, output=y[K])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:533: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, activation=<keras.lay..., kernel_regularizer=<keras.reg...)`\n",
            "  W_regularizer=Reg.l1_l2(l1=nu1, l2=nu2))(y_hat[i + 1])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:533: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(500, activation=<keras.lay..., kernel_regularizer=<keras.reg...)`\n",
            "  W_regularizer=Reg.l1_l2(l1=nu1, l2=nu2))(y_hat[i + 1])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:535: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, activation=<keras.lay..., kernel_regularizer=<keras.reg...)`\n",
            "  W_regularizer=Reg.l1_l2(l1=nu1, l2=nu2))(y_hat[1])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:540: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  decoder = Model(input=y, output=x_hat)\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dnn_utils.py:885: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
            "  autoencoder = Model(input=x_in, output=[x_hat, y])\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynAERNN.py:159: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"su...)`\n",
            "  self._model = Model(input=[x_in, x_pred], output=x_diff)\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynAERNN.py:179: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  verbose=1\n",
            "/usr/local/lib/python3.6/dist-packages/dynamicgem/embedding/dynAERNN.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., verbose=1, steps_per_epoch=1, epochs=10)`\n",
            "  verbose=1\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 299.7134\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0308\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0333\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.0363\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.0395\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0425\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0451\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0472\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0486\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.0493\n",
            "(#Embeddings, #Dimension) : (60, 64)\n",
            "Time Elapsed: 6.633436679840088\n",
            "\n",
            "5. Community Prediction ...\n",
            "##################################################\n",
            "5.1. Loading future user communities ...\n",
            "Loading future user communities failed! Predicting future user communities ...\n",
            "5.1. Inter-User Similarity Prediction ...\n",
            "5.2. Future Graph Prediction ...\n",
            "(#Nodes/Users, #Edges): (60, 630)\n",
            "5.3. Future Community Prediction ...\n",
            "Starting with 60 nodes.\n",
            "Aggregation 1 completed with 3 clusters and  0.6333334445953369 increment.\n",
            "Aggregation 2 completed with 3 clusters and  0.0 increment.\n",
            "(#Future Communities, Communities Sizes) : (3, [20, 20, 20])\n",
            "(#Future Communities with less then 10 members) : (0)\n",
            "Cluster 0 has 20 users. Topic 3 is the favorite topic for 66.66666666666666% of users.\n",
            "Cluster 2 has 20 users. Topic 1 is the favorite topic for 66.66666666666666% of users.\n",
            "Cluster 1 has 20 users. Topic 2 is the favorite topic for 66.66666666666666% of users.\n",
            "Time Elapsed: 0.1367473602294922\n",
            "\n",
            "6. Application: News Recommendation ...\n",
            "##################################################\n",
            "6.1 Loading news articles ...\n",
            "6.2 Inferring news articles' topics ...\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "loading Dictionary object from ../output/toy/lda.gensim.dynaernn/tml/3TopicsDictionary.mm\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'rb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3TopicsDictionary.mm'}\n",
            "loaded ../output/toy/lda.gensim.dynaernn/tml/3TopicsDictionary.mm\n",
            "Loading lda.gensim model ...\n",
            "loading LdaModel object from ../output/toy/lda.gensim.dynaernn/tml/3Topics.model\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'rb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3Topics.model'}\n",
            "loading expElogbeta from ../output/toy/lda.gensim.dynaernn/tml/3Topics.model.expElogbeta.npy with mmap=None\n",
            "setting ignored attribute dispatcher to None\n",
            "setting ignored attribute state to None\n",
            "setting ignored attribute id2word to None\n",
            "loaded ../output/toy/lda.gensim.dynaernn/tml/3Topics.model\n",
            "loading LdaState object from ../output/toy/lda.gensim.dynaernn/tml/3Topics.model.state\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'rb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3Topics.model.state'}\n",
            "loaded ../output/toy/lda.gensim.dynaernn/tml/3Topics.model.state\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'rb', 'uri': '../output/toy/lda.gensim.dynaernn/tml/3Topics.model.id2word'}\n",
            "6.3 Recommending news articles to future communities ...\n",
            "6.4 Evaluating recommended news articles on future time interval 2010-12-04...\n",
            "Calling pytrec_eval for {'map_cut', 'success', 'ndcg_cut'} at [1:1:100] cutoffs ...\n",
            "Averaging ...\n",
            "Time Elapsed: 0.1928706169128418\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running pipeline for gsdmm and ae ....\n",
            "\n",
            "1. DAL: Temporal Document Creation from Social Posts ...\n",
            "##################################################\n",
            "1.1. Loading saved temporal documents from  ../output/toy/gsdmm.ae/Documents.csv in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)...\n",
            "1.1. Loading temporal documents failed! Creating temporal documents ...\n",
            "1.2. Loading social posts ...\n",
            "(#Posts): (540)\n",
            "1.3. Creating temporal documents in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)\n",
            "100% 540/540 [00:00<00:00, 29315.61it/s]\n",
            "(#ProcessedDocuments, #Documents, #Users, #TimeIntervals): (180,180,60,3)\n",
            "Time Elapsed: 0.05175971984863281\n",
            "\n",
            "2. TML: Topic Modeling ...\n",
            "##################################################\n",
            "2.1. Loading saved topic model failed! Training a model ...\n",
            "(#Topics, Model): (3, gsdmm)\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...) from 180 documents (total 5832 corpus positions)\n",
            "discarding 0 tokens: []...\n",
            "keeping 29 tokens which were in no less than 2 and no more than 108 (=60.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...)\n",
            "saving Dictionary object under ../output/toy/gsdmm.ae/tml/3TopicsDictionary.mm, separately None\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/toy/gsdmm.ae/tml/3TopicsDictionary.mm'}\n",
            "saved ../output/toy/gsdmm.ae/tml/3TopicsDictionary.mm\n",
            "In stage 0: transferred 110 clusters with 3 clusters populated\n",
            "In stage 1: transferred 5 clusters with 3 clusters populated\n",
            "In stage 2: transferred 0 clusters with 3 clusters populated\n",
            "In stage 3: transferred 0 clusters with 3 clusters populated\n",
            "In stage 4: transferred 0 clusters with 3 clusters populated\n",
            "In stage 5: transferred 0 clusters with 3 clusters populated\n",
            "In stage 6: transferred 0 clusters with 3 clusters populated\n",
            "In stage 7: transferred 0 clusters with 3 clusters populated\n",
            "In stage 8: transferred 0 clusters with 3 clusters populated\n",
            "In stage 9: transferred 0 clusters with 3 clusters populated\n",
            "In stage 10: transferred 0 clusters with 3 clusters populated\n",
            "In stage 11: transferred 0 clusters with 3 clusters populated\n",
            "In stage 12: transferred 0 clusters with 3 clusters populated\n",
            "In stage 13: transferred 0 clusters with 3 clusters populated\n",
            "In stage 14: transferred 0 clusters with 3 clusters populated\n",
            "In stage 15: transferred 0 clusters with 3 clusters populated\n",
            "In stage 16: transferred 0 clusters with 3 clusters populated\n",
            "In stage 17: transferred 0 clusters with 3 clusters populated\n",
            "In stage 18: transferred 0 clusters with 3 clusters populated\n",
            "In stage 19: transferred 0 clusters with 3 clusters populated\n",
            "In stage 20: transferred 0 clusters with 3 clusters populated\n",
            "In stage 21: transferred 0 clusters with 3 clusters populated\n",
            "In stage 22: transferred 0 clusters with 3 clusters populated\n",
            "In stage 23: transferred 0 clusters with 3 clusters populated\n",
            "In stage 24: transferred 0 clusters with 3 clusters populated\n",
            "In stage 25: transferred 0 clusters with 3 clusters populated\n",
            "In stage 26: transferred 0 clusters with 3 clusters populated\n",
            "Converged.  Breaking out.\n",
            "2.2. Quality of topics ...\n",
            "(MeanCoherence): (None)\n",
            "(#Topic, Topic Coherences): (3, None)\n",
            "Time Elapsed: 0.6101298332214355\n",
            "\n",
            "3. UML: Temporal Graph Creation ...\n",
            "##################################################\n",
            "3.1. Loading users' graph stream from ../output/toy/gsdmm.ae/uml/graphs/graphs.pkl ...\n",
            "3.1. Loading users' graph stream failed! Generating the graph stream ...\n",
            "60 users have twitted in 2010-12-01\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 104, in main\n",
            "    graphs = pd.read_pickle(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\", line 169, in read_pickle\n",
            "    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 499, in get_handle\n",
            "    f = open(path_or_buf, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/toy/gsdmm.ae/uml/graphs/graphs.pkl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 175, in run\n",
            "    main()\n",
            "  File \"main.py\", line 111, in main\n",
            "    just_one=Params.tml['justOne'], binary=Params.tml['binary'], threshold=Params.tml['threshold'])\n",
            "  File \"/content/SEERa/src/uml/UserSimilarities.py\", line 34, in main\n",
            "    else: d2t = tm.doc2topics(lda_model, dictionary.doc2bow(doc.split()), threshold=threshold, just_one=just_one, binary=binary)\n",
            "  File \"/content/SEERa/src/tml/TopicModeling.py\", line 184, in doc2topics\n",
            "    if t_temp >= threshold:\n",
            "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running pipeline for gsdmm and dynae ....\n",
            "\n",
            "1. DAL: Temporal Document Creation from Social Posts ...\n",
            "##################################################\n",
            "1.1. Loading saved temporal documents from  ../output/toy/gsdmm.dynae/Documents.csv in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)...\n",
            "1.1. Loading temporal documents failed! Creating temporal documents ...\n",
            "1.2. Loading social posts ...\n",
            "(#Posts): (540)\n",
            "1.3. Creating temporal documents in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)\n",
            "100% 540/540 [00:00<00:00, 22457.01it/s]\n",
            "(#ProcessedDocuments, #Documents, #Users, #TimeIntervals): (180,180,60,3)\n",
            "Time Elapsed: 0.06284904479980469\n",
            "\n",
            "2. TML: Topic Modeling ...\n",
            "##################################################\n",
            "2.1. Loading saved topic model failed! Training a model ...\n",
            "(#Topics, Model): (3, gsdmm)\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...) from 180 documents (total 5832 corpus positions)\n",
            "discarding 0 tokens: []...\n",
            "keeping 29 tokens which were in no less than 2 and no more than 108 (=60.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...)\n",
            "saving Dictionary object under ../output/toy/gsdmm.dynae/tml/3TopicsDictionary.mm, separately None\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/toy/gsdmm.dynae/tml/3TopicsDictionary.mm'}\n",
            "saved ../output/toy/gsdmm.dynae/tml/3TopicsDictionary.mm\n",
            "In stage 0: transferred 117 clusters with 3 clusters populated\n",
            "In stage 1: transferred 11 clusters with 3 clusters populated\n",
            "In stage 2: transferred 0 clusters with 3 clusters populated\n",
            "In stage 3: transferred 0 clusters with 3 clusters populated\n",
            "In stage 4: transferred 0 clusters with 3 clusters populated\n",
            "In stage 5: transferred 0 clusters with 3 clusters populated\n",
            "In stage 6: transferred 0 clusters with 3 clusters populated\n",
            "In stage 7: transferred 0 clusters with 3 clusters populated\n",
            "In stage 8: transferred 0 clusters with 3 clusters populated\n",
            "In stage 9: transferred 0 clusters with 3 clusters populated\n",
            "In stage 10: transferred 0 clusters with 3 clusters populated\n",
            "In stage 11: transferred 0 clusters with 3 clusters populated\n",
            "In stage 12: transferred 0 clusters with 3 clusters populated\n",
            "In stage 13: transferred 0 clusters with 3 clusters populated\n",
            "In stage 14: transferred 0 clusters with 3 clusters populated\n",
            "In stage 15: transferred 0 clusters with 3 clusters populated\n",
            "In stage 16: transferred 0 clusters with 3 clusters populated\n",
            "In stage 17: transferred 0 clusters with 3 clusters populated\n",
            "In stage 18: transferred 0 clusters with 3 clusters populated\n",
            "In stage 19: transferred 0 clusters with 3 clusters populated\n",
            "In stage 20: transferred 0 clusters with 3 clusters populated\n",
            "In stage 21: transferred 0 clusters with 3 clusters populated\n",
            "In stage 22: transferred 0 clusters with 3 clusters populated\n",
            "In stage 23: transferred 0 clusters with 3 clusters populated\n",
            "In stage 24: transferred 0 clusters with 3 clusters populated\n",
            "In stage 25: transferred 0 clusters with 3 clusters populated\n",
            "In stage 26: transferred 0 clusters with 3 clusters populated\n",
            "Converged.  Breaking out.\n",
            "2.2. Quality of topics ...\n",
            "(MeanCoherence): (None)\n",
            "(#Topic, Topic Coherences): (3, None)\n",
            "Time Elapsed: 0.616208553314209\n",
            "\n",
            "3. UML: Temporal Graph Creation ...\n",
            "##################################################\n",
            "3.1. Loading users' graph stream from ../output/toy/gsdmm.dynae/uml/graphs/graphs.pkl ...\n",
            "3.1. Loading users' graph stream failed! Generating the graph stream ...\n",
            "60 users have twitted in 2010-12-01\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 104, in main\n",
            "    graphs = pd.read_pickle(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\", line 169, in read_pickle\n",
            "    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 499, in get_handle\n",
            "    f = open(path_or_buf, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/toy/gsdmm.dynae/uml/graphs/graphs.pkl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 175, in run\n",
            "    main()\n",
            "  File \"main.py\", line 111, in main\n",
            "    just_one=Params.tml['justOne'], binary=Params.tml['binary'], threshold=Params.tml['threshold'])\n",
            "  File \"/content/SEERa/src/uml/UserSimilarities.py\", line 34, in main\n",
            "    else: d2t = tm.doc2topics(lda_model, dictionary.doc2bow(doc.split()), threshold=threshold, just_one=just_one, binary=binary)\n",
            "  File \"/content/SEERa/src/tml/TopicModeling.py\", line 184, in doc2topics\n",
            "    if t_temp >= threshold:\n",
            "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running pipeline for gsdmm and dynaernn ....\n",
            "\n",
            "1. DAL: Temporal Document Creation from Social Posts ...\n",
            "##################################################\n",
            "1.1. Loading saved temporal documents from  ../output/toy/gsdmm.dynaernn/Documents.csv in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)...\n",
            "1.1. Loading temporal documents failed! Creating temporal documents ...\n",
            "1.2. Loading social posts ...\n",
            "(#Posts): (540)\n",
            "1.3. Creating temporal documents in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)\n",
            "100% 540/540 [00:00<00:00, 27288.24it/s]\n",
            "(#ProcessedDocuments, #Documents, #Users, #TimeIntervals): (180,180,60,3)\n",
            "Time Elapsed: 0.05606722831726074\n",
            "\n",
            "2. TML: Topic Modeling ...\n",
            "##################################################\n",
            "2.1. Loading saved topic model failed! Training a model ...\n",
            "(#Topics, Model): (3, gsdmm)\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...) from 180 documents (total 5832 corpus positions)\n",
            "discarding 0 tokens: []...\n",
            "keeping 29 tokens which were in no less than 2 and no more than 108 (=60.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...)\n",
            "saving Dictionary object under ../output/toy/gsdmm.dynaernn/tml/3TopicsDictionary.mm, separately None\n",
            "{'transport_params': None, 'compression': 'infer_from_extension', 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/toy/gsdmm.dynaernn/tml/3TopicsDictionary.mm'}\n",
            "saved ../output/toy/gsdmm.dynaernn/tml/3TopicsDictionary.mm\n",
            "In stage 0: transferred 112 clusters with 3 clusters populated\n",
            "In stage 1: transferred 10 clusters with 3 clusters populated\n",
            "In stage 2: transferred 0 clusters with 3 clusters populated\n",
            "In stage 3: transferred 0 clusters with 3 clusters populated\n",
            "In stage 4: transferred 0 clusters with 3 clusters populated\n",
            "In stage 5: transferred 0 clusters with 3 clusters populated\n",
            "In stage 6: transferred 0 clusters with 3 clusters populated\n",
            "In stage 7: transferred 0 clusters with 3 clusters populated\n",
            "In stage 8: transferred 0 clusters with 3 clusters populated\n",
            "In stage 9: transferred 0 clusters with 3 clusters populated\n",
            "In stage 10: transferred 0 clusters with 3 clusters populated\n",
            "In stage 11: transferred 0 clusters with 3 clusters populated\n",
            "In stage 12: transferred 0 clusters with 3 clusters populated\n",
            "In stage 13: transferred 0 clusters with 3 clusters populated\n",
            "In stage 14: transferred 0 clusters with 3 clusters populated\n",
            "In stage 15: transferred 0 clusters with 3 clusters populated\n",
            "In stage 16: transferred 0 clusters with 3 clusters populated\n",
            "In stage 17: transferred 0 clusters with 3 clusters populated\n",
            "In stage 18: transferred 0 clusters with 3 clusters populated\n",
            "In stage 19: transferred 0 clusters with 3 clusters populated\n",
            "In stage 20: transferred 0 clusters with 3 clusters populated\n",
            "In stage 21: transferred 0 clusters with 3 clusters populated\n",
            "In stage 22: transferred 0 clusters with 3 clusters populated\n",
            "In stage 23: transferred 0 clusters with 3 clusters populated\n",
            "In stage 24: transferred 0 clusters with 3 clusters populated\n",
            "In stage 25: transferred 0 clusters with 3 clusters populated\n",
            "In stage 26: transferred 0 clusters with 3 clusters populated\n",
            "Converged.  Breaking out.\n",
            "2.2. Quality of topics ...\n",
            "(MeanCoherence): (None)\n",
            "(#Topic, Topic Coherences): (3, None)\n",
            "Time Elapsed: 0.5852224826812744\n",
            "\n",
            "3. UML: Temporal Graph Creation ...\n",
            "##################################################\n",
            "3.1. Loading users' graph stream from ../output/toy/gsdmm.dynaernn/uml/graphs/graphs.pkl ...\n",
            "3.1. Loading users' graph stream failed! Generating the graph stream ...\n",
            "60 users have twitted in 2010-12-01\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 104, in main\n",
            "    graphs = pd.read_pickle(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\", line 169, in read_pickle\n",
            "    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 499, in get_handle\n",
            "    f = open(path_or_buf, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/toy/gsdmm.dynaernn/uml/graphs/graphs.pkl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 175, in run\n",
            "    main()\n",
            "  File \"main.py\", line 111, in main\n",
            "    just_one=Params.tml['justOne'], binary=Params.tml['binary'], threshold=Params.tml['threshold'])\n",
            "  File \"/content/SEERa/src/uml/UserSimilarities.py\", line 34, in main\n",
            "    else: d2t = tm.doc2topics(lda_model, dictionary.doc2bow(doc.split()), threshold=threshold, just_one=just_one, binary=binary)\n",
            "  File \"/content/SEERa/src/tml/TopicModeling.py\", line 184, in doc2topics\n",
            "    if t_temp >= threshold:\n",
            "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running pipeline for btm and ae ....\n",
            "\n",
            "1. DAL: Temporal Document Creation from Social Posts ...\n",
            "##################################################\n",
            "1.1. Loading saved temporal documents from  ../output/toy/btm.ae/Documents.csv in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)...\n",
            "1.1. Loading temporal documents failed! Creating temporal documents ...\n",
            "1.2. Loading social posts ...\n",
            "(#Posts): (540)\n",
            "1.3. Creating temporal documents in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)\n",
            "100% 540/540 [00:00<00:00, 30925.95it/s]\n",
            "(#ProcessedDocuments, #Documents, #Users, #TimeIntervals): (180,180,60,3)\n",
            "Time Elapsed: 0.04837656021118164\n",
            "\n",
            "2. TML: Topic Modeling ...\n",
            "##################################################\n",
            "2.1. Loading saved topic model failed! Training a model ...\n",
            "(#Topics, Model): (3, btm)\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...) from 180 documents (total 5832 corpus positions)\n",
            "discarding 0 tokens: []...\n",
            "keeping 29 tokens which were in no less than 2 and no more than 108 (=60.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...)\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 70, in main\n",
            "    tml_model = pd.read_pickle(path_mdl)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\", line 169, in read_pickle\n",
            "    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 499, in get_handle\n",
            "    f = open(path_or_buf, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/toy/btm.ae/tml/3Topics.pkl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 175, in run\n",
            "    main()\n",
            "  File \"main.py\", line 90, in main\n",
            "    path_2_save_tml=Params.tml['path2save'])\n",
            "  File \"/content/SEERa/src/tml/TopicModeling.py\", line 106, in topic_modeling\n",
            "    doc_word_frequency, dictionary, vocab_dict = btm.get_words_freqs(processed_docs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bitermplus/_util.py\", line 50, in get_words_freqs\n",
            "    words = np.array(vec.get_feature_names_out())\n",
            "AttributeError: 'CountVectorizer' object has no attribute 'get_feature_names_out'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running pipeline for btm and dynae ....\n",
            "\n",
            "1. DAL: Temporal Document Creation from Social Posts ...\n",
            "##################################################\n",
            "1.1. Loading saved temporal documents from  ../output/toy/btm.dynae/Documents.csv in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)...\n",
            "1.1. Loading temporal documents failed! Creating temporal documents ...\n",
            "1.2. Loading social posts ...\n",
            "(#Posts): (540)\n",
            "1.3. Creating temporal documents in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)\n",
            "100% 540/540 [00:00<00:00, 20908.02it/s]\n",
            "(#ProcessedDocuments, #Documents, #Users, #TimeIntervals): (180,180,60,3)\n",
            "Time Elapsed: 0.056299448013305664\n",
            "\n",
            "2. TML: Topic Modeling ...\n",
            "##################################################\n",
            "2.1. Loading saved topic model failed! Training a model ...\n",
            "(#Topics, Model): (3, btm)\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...) from 180 documents (total 5832 corpus positions)\n",
            "discarding 0 tokens: []...\n",
            "keeping 29 tokens which were in no less than 2 and no more than 108 (=60.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...)\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 70, in main\n",
            "    tml_model = pd.read_pickle(path_mdl)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\", line 169, in read_pickle\n",
            "    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 499, in get_handle\n",
            "    f = open(path_or_buf, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/toy/btm.dynae/tml/3Topics.pkl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 175, in run\n",
            "    main()\n",
            "  File \"main.py\", line 90, in main\n",
            "    path_2_save_tml=Params.tml['path2save'])\n",
            "  File \"/content/SEERa/src/tml/TopicModeling.py\", line 106, in topic_modeling\n",
            "    doc_word_frequency, dictionary, vocab_dict = btm.get_words_freqs(processed_docs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bitermplus/_util.py\", line 50, in get_words_freqs\n",
            "    words = np.array(vec.get_feature_names_out())\n",
            "AttributeError: 'CountVectorizer' object has no attribute 'get_feature_names_out'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running pipeline for btm and dynaernn ....\n",
            "\n",
            "1. DAL: Temporal Document Creation from Social Posts ...\n",
            "##################################################\n",
            "1.1. Loading saved temporal documents from  ../output/toy/btm.dynaernn/Documents.csv in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)...\n",
            "1.1. Loading temporal documents failed! Creating temporal documents ...\n",
            "1.2. Loading social posts ...\n",
            "(#Posts): (540)\n",
            "1.3. Creating temporal documents in which \n",
            "(User, Time) a document is concat of user's posts in each 1 day(s)\n",
            "100% 540/540 [00:00<00:00, 30541.46it/s]\n",
            "(#ProcessedDocuments, #Documents, #Users, #TimeIntervals): (180,180,60,3)\n",
            "Time Elapsed: 0.048494577407836914\n",
            "\n",
            "2. TML: Topic Modeling ...\n",
            "##################################################\n",
            "2.1. Loading saved topic model failed! Training a model ...\n",
            "(#Topics, Model): (3, btm)\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...) from 180 documents (total 5832 corpus positions)\n",
            "discarding 0 tokens: []...\n",
            "keeping 29 tokens which were in no less than 2 and no more than 108 (=60.0%) documents\n",
            "rebuilding dictionary, shrinking gaps\n",
            "resulting dictionary: Dictionary(29 unique tokens: ['apple', 'dell', 'digital', 'keyboard', 'microsoft']...)\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 70, in main\n",
            "    tml_model = pd.read_pickle(path_mdl)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\", line 169, in read_pickle\n",
            "    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 499, in get_handle\n",
            "    f = open(path_or_buf, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../output/toy/btm.dynaernn/tml/3Topics.pkl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 175, in run\n",
            "    main()\n",
            "  File \"main.py\", line 90, in main\n",
            "    path_2_save_tml=Params.tml['path2save'])\n",
            "  File \"/content/SEERa/src/tml/TopicModeling.py\", line 106, in topic_modeling\n",
            "    doc_word_frequency, dictionary, vocab_dict = btm.get_words_freqs(processed_docs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bitermplus/_util.py\", line 50, in get_words_freqs\n",
            "    words = np.array(vec.get_feature_names_out())\n",
            "AttributeError: 'CountVectorizer' object has no attribute 'get_feature_names_out'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}