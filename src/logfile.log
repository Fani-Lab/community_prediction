'pattern' package not found; tag filters are not available for English
CACHEDIR=C:\Users\sorou\.matplotlib
Using fontManager instance from C:\Users\sorou\.matplotlib\fontlist-v300.json
Loaded backend module://backend_interagg version unknown.
Main: UserSimilarities ...
DataReader: Connection created
DataReader: 63744 rows returned
DataReader: Connection closed
DataPreperation: userModeling=True, timeModeling=True, preProcessing=False, TagME=False
DataPreperation: 100000 sampled from the end of dataset (sorted by creationTime)
DataPreperation: Length of the dataset after applying groupby: 10788 

UserSimilarity: Processed docs shape: (10788,)
UserSimilarity: Topic modeling ...
TopicModeling: num_topics=5,  filterExtremes=True, library=gensim
adding document #0 to Dictionary(0 unique tokens: [])
adding document #10000 to Dictionary(48161 unique tokens: ['Beethoven', "Beethoven's 5th", 'Swan Lake', 'holiday', 'party']...)
built Dictionary(49968 unique tokens: ['Beethoven', "Beethoven's 5th", 'Swan Lake', 'holiday', 'party']...) from 10788 documents (total 234804 corpus positions)
discarding 0 tokens: []...
keeping 49968 tokens which were in no less than 1 and no more than 2157 (=20.0%) documents
rebuilding dictionary, shrinking gaps
resulting dictionary: Dictionary(49968 unique tokens: ['Beethoven', "Beethoven's 5th", 'Swan Lake', 'holiday', 'party']...)
using symmetric alpha at 0.2
using symmetric eta at 0.2
using serial LDA version on this node
running online (multi-pass) LDA training, 5 topics, 5 passes over the supplied corpus of 10788 documents, updating model once every 2000 documents, evaluating perplexity every 10788 documents, iterating 50x with a convergence threshold of 0.001000
PROGRESS: pass 0, at document #2000/10788
performing inference on a chunk of 2000 documents
1640/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.004*"lol" + 0.003*"WikiLeaks" + 0.003*"will" + 0.002*"Google" + 0.002*"iPhone" + 0.002*"Facebook" + 0.002*"House" + 0.002*"iPad" + 0.002*"Twitter" + 0.002*"U.S"
topic #1 (0.200): 0.004*"CNN" + 0.003*"Larry King" + 0.002*"U.S" + 0.002*"Obama" + 0.002*"lol" + 0.002*"Follow Friday" + 0.002*"will" + 0.002*"Yahoo" + 0.002*"Twitter" + 0.002*"WikiLeaks"
topic #2 (0.200): 0.004*"Facebook" + 0.003*"Delicious" + 0.003*"Christmas" + 0.003*"Apple" + 0.002*"video" + 0.002*"Wikileaks" + 0.002*"U.S" + 0.002*"Yahoo" + 0.002*"CNN" + 0.002*"lol"
topic #3 (0.200): 0.007*"Twitter" + 0.004*"Facebook" + 0.004*"Yahoo" + 0.004*"CNN" + 0.004*"will" + 0.003*"US" + 0.003*"House" + 0.003*"WikiLeaks" + 0.003*"Obama" + 0.002*"Christmas"
topic #4 (0.200): 0.004*"Obama" + 0.004*"Facebook" + 0.003*"Twitter" + 0.003*"Yahoo" + 0.003*"bill" + 0.003*"GOP" + 0.003*"House" + 0.002*"US" + 0.002*"will" + 0.002*"today"
topic diff=3.896832, rho=1.000000
PROGRESS: pass 0, at document #4000/10788
performing inference on a chunk of 2000 documents
1755/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.008*"lol" + 0.003*"time" + 0.003*"will" + 0.002*"WikiLeaks" + 0.002*"LOL" + 0.002*"Christmas" + 0.002*"people" + 0.002*"video" + 0.002*"Google" + 0.002*"who"
topic #1 (0.200): 0.005*"NYT" + 0.004*"Roll Call" + 0.003*"Larry King" + 0.003*"CNN" + 0.003*"lol" + 0.003*"Obama" + 0.003*"will" + 0.002*"U.S" + 0.002*"LAT" + 0.002*"who"
topic #2 (0.200): 0.005*"video" + 0.004*"request" + 0.004*"Facebook" + 0.004*"Christmas" + 0.003*"YouTube" + 0.003*"lol" + 0.003*"livestream" + 0.003*"tcot" + 0.003*"people" + 0.003*"chat room"
topic #3 (0.200): 0.007*"YouTube" + 0.005*"Twitter" + 0.004*"will" + 0.003*"WikiLeaks" + 0.003*"RHOBH" + 0.003*"Facebook" + 0.003*"House" + 0.003*"US" + 0.003*"Senate" + 0.003*"CNN"
topic #4 (0.200): 0.005*"Obama" + 0.004*"GOP" + 0.003*"Sen" + 0.003*"Facebook" + 0.003*"bill" + 0.003*"will" + 0.003*"today" + 0.003*"Senate" + 0.002*"House" + 0.002*"DADT"
topic diff=0.996477, rho=0.707107
PROGRESS: pass 0, at document #6000/10788
performing inference on a chunk of 2000 documents
1867/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.010*"lol" + 0.003*"time" + 0.002*"God" + 0.002*"will" + 0.002*"Christmas" + 0.002*"people" + 0.002*"LOL" + 0.002*"photography" + 0.002*"who" + 0.002*"can"
topic #1 (0.200): 0.004*"Larry King" + 0.003*"Follow Friday" + 0.003*"CNN" + 0.003*"lol" + 0.002*"will" + 0.002*"NYT" + 0.002*"good" + 0.002*"Christmas" + 0.002*"U.S" + 0.002*"Morgan Freeman"
topic #2 (0.200): 0.005*"video" + 0.004*"Christmas" + 0.003*"Facebook" + 0.003*"people" + 0.003*"lol" + 0.003*"today" + 0.002*"who" + 0.002*"you" + 0.002*"Amtrak" + 0.002*"will"
topic #3 (0.200): 0.005*"Twitter" + 0.004*"WikiLeaks" + 0.004*"will" + 0.003*"YouTube" + 0.003*"US" + 0.002*"China" + 0.002*"Facebook" + 0.002*"CNN" + 0.002*"India" + 0.002*"House"
topic #4 (0.200): 0.005*"GOP" + 0.004*"Obama" + 0.004*"pf" + 0.003*"will" + 0.003*"bill" + 0.003*"today" + 0.003*"Senate" + 0.003*"Christmas" + 0.002*"DADT" + 0.002*"Facebook"
topic diff=0.887025, rho=0.577350
PROGRESS: pass 0, at document #8000/10788
performing inference on a chunk of 2000 documents
1903/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.012*"lol" + 0.004*"VE" + 0.003*"Barinas" + 0.003*"time" + 0.002*"Venezuela" + 0.002*"Vla" + 0.002*"Christmas" + 0.002*"will" + 0.002*"lmao" + 0.002*"people"
topic #1 (0.200): 0.004*"Larry King" + 0.004*"Follow Friday" + 0.003*"CNN" + 0.003*"U.S" + 0.002*"will" + 0.002*"Christmas" + 0.002*"lol" + 0.002*"Morgan Freeman" + 0.002*"us" + 0.002*"who"
topic #2 (0.200): 0.004*"video" + 0.004*"Christmas" + 0.003*"Facebook" + 0.003*"people" + 0.003*"lol" + 0.003*"who" + 0.002*"today" + 0.002*"you" + 0.002*"tcot" + 0.002*"pour"
topic #3 (0.200): 0.005*"Twitter" + 0.004*"US" + 0.004*"WikiLeaks" + 0.003*"will" + 0.003*"India" + 0.003*"China" + 0.003*"Assange" + 0.003*"Wikileaks" + 0.003*"auspol" + 0.003*"Julian Assange"
topic #4 (0.200): 0.005*"Obama" + 0.005*"GOP" + 0.004*"para" + 0.003*"bill" + 0.003*"DADT" + 0.003*"will" + 0.003*"today" + 0.003*"Senate" + 0.003*"Christmas" + 0.003*"tcot"
topic diff=1.025281, rho=0.500000
PROGRESS: pass 0, at document #10000/10788
performing inference on a chunk of 2000 documents
1901/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.012*"lol" + 0.003*"time" + 0.003*"God" + 0.003*"Como" + 0.002*"poco" + 0.002*"lentamente" + 0.002*"poco a poco" + 0.002*"pasion" + 0.002*"VE" + 0.002*"will"
topic #1 (0.200): 0.006*"friends" + 0.005*"Family" + 0.005*"Concert" + 0.005*"Biblical" + 0.005*"Presbyterian" + 0.004*"Fundraising" + 0.004*"Larry King" + 0.003*"Breaking News" + 0.003*"CNN" + 0.003*"Follow Friday"
topic #2 (0.200): 0.004*"video" + 0.004*"UR" + 0.004*"Christmas" + 0.003*"Facebook" + 0.003*"people" + 0.003*"who" + 0.003*"Amtrak" + 0.003*"Home" + 0.003*"Twitter" + 0.002*"tweet"
topic #3 (0.200): 0.010*"Twitter" + 0.006*"US" + 0.006*"tweet" + 0.005*"WikiLeaks" + 0.005*"India" + 0.004*"China" + 0.004*"Wikileaks" + 0.003*"Julian Assange" + 0.003*"will" + 0.003*"Pakistan"
topic #4 (0.200): 0.006*"GOP" + 0.005*"para" + 0.004*"Obama" + 0.004*"bill" + 0.003*"un" + 0.003*"Senate" + 0.003*"will" + 0.003*"Christmas" + 0.003*"DADT" + 0.003*"today"
topic diff=1.029382, rho=0.447214
bound: at document #0
-11.118 per-word bound, 2222.5 perplexity estimate based on a held-out corpus of 788 documents with 20452 words
PROGRESS: pass 0, at document #10788/10788
performing inference on a chunk of 788 documents
766/788 documents converged within 50 iterations
updating topics
merging changes from 788 documents into a model of 10788 documents
topic #0 (0.200): 0.011*"lol" + 0.010*"Como" + 0.003*"Venezuela" + 0.003*"time" + 0.003*"VE" + 0.003*"God" + 0.002*"ur" + 0.002*"will" + 0.002*"you" + 0.002*"people"
topic #1 (0.200): 0.004*"friends" + 0.003*"CNN" + 0.003*"Follow Friday" + 0.003*"Family" + 0.003*"will" + 0.003*"Concert" + 0.003*"Biblical" + 0.003*"Presbyterian" + 0.003*"Fundraising" + 0.003*"Larry King"
topic #2 (0.200): 0.006*"video" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"who" + 0.003*"Twitter" + 0.003*"you" + 0.002*"today" + 0.002*"app" + 0.002*"UR"
topic #3 (0.200): 0.009*"Twitter" + 0.006*"US" + 0.006*"Sudan" + 0.005*"WikiLeaks" + 0.004*"WORLD" + 0.004*"via        s" + 0.004*"LE" + 0.004*"India" + 0.004*"tweet" + 0.004*"Peace"
topic #4 (0.200): 0.007*"GOP" + 0.006*"para" + 0.005*"un" + 0.004*"bill" + 0.004*"Obama" + 0.004*"Senate" + 0.004*"Venezuela" + 0.004*"will" + 0.003*"House" + 0.003*"Christmas"
topic diff=0.691359, rho=0.408248
PROGRESS: pass 1, at document #2000/10788
performing inference on a chunk of 2000 documents
1959/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.012*"lol" + 0.007*"Como" + 0.003*"time" + 0.002*"Venezuela" + 0.002*"God" + 0.002*"VE" + 0.002*"will" + 0.002*"people" + 0.002*"ur" + 0.002*"you"
topic #1 (0.200): 0.003*"CNN" + 0.003*"Follow Friday" + 0.003*"Larry King" + 0.003*"U.S" + 0.003*"friends" + 0.003*"will" + 0.002*"Morgan Freeman" + 0.002*"Family" + 0.002*"Concert" + 0.002*"Christmas"
topic #2 (0.200): 0.006*"video" + 0.004*"Facebook" + 0.004*"Christmas" + 0.004*"people" + 0.003*"who" + 0.003*"Twitter" + 0.003*"app" + 0.002*"you" + 0.002*"today" + 0.002*"Delicious"
topic #3 (0.200): 0.009*"Twitter" + 0.006*"US" + 0.005*"WikiLeaks" + 0.004*"Sudan" + 0.004*"China" + 0.004*"India" + 0.004*"Wikileaks" + 0.003*"Julian Assange" + 0.003*"will" + 0.003*"Assange"
topic #4 (0.200): 0.007*"GOP" + 0.005*"para" + 0.005*"Obama" + 0.005*"un" + 0.005*"bill" + 0.004*"Senate" + 0.003*"House" + 0.003*"will" + 0.003*"tcot" + 0.003*"DADT"
topic diff=0.450458, rho=0.367756
PROGRESS: pass 1, at document #4000/10788
performing inference on a chunk of 2000 documents
1986/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.013*"lol" + 0.005*"Como" + 0.003*"time" + 0.003*"LOL" + 0.002*"ur" + 0.002*"will" + 0.002*"God" + 0.002*"you" + 0.002*"Christmas" + 0.002*"people"
topic #1 (0.200): 0.004*"NYT" + 0.004*"Larry King" + 0.003*"CNN" + 0.003*"U.S" + 0.003*"will" + 0.003*"Follow Friday" + 0.002*"Morgan Freeman" + 0.002*"LAT" + 0.002*"Roll Call" + 0.002*"friends"
topic #2 (0.200): 0.007*"video" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.004*"YouTube" + 0.003*"who" + 0.003*"request" + 0.003*"today" + 0.002*"livestream" + 0.002*"time"
topic #3 (0.200): 0.008*"Twitter" + 0.006*"WikiLeaks" + 0.005*"US" + 0.004*"Wikileaks" + 0.004*"India" + 0.004*"Julian Assange" + 0.004*"China" + 0.003*"will" + 0.003*"YouTube" + 0.003*"Assange"
topic #4 (0.200): 0.007*"GOP" + 0.006*"Obama" + 0.005*"Senate" + 0.005*"bill" + 0.004*"para" + 0.004*"tcot" + 0.004*"House" + 0.004*"will" + 0.004*"un" + 0.003*"vote"
topic diff=0.467244, rho=0.367756
PROGRESS: pass 1, at document #6000/10788
performing inference on a chunk of 2000 documents
1990/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.003*"Como" + 0.003*"time" + 0.003*"God" + 0.002*"LOL" + 0.002*"ur" + 0.002*"you" + 0.002*"Christmas" + 0.002*"will" + 0.002*"can"
topic #1 (0.200): 0.004*"Larry King" + 0.004*"Follow Friday" + 0.003*"CNN" + 0.003*"will" + 0.003*"NYT" + 0.003*"U.S" + 0.002*"Morgan Freeman" + 0.002*"Haiti" + 0.002*"Christmas" + 0.002*"good"
topic #2 (0.200): 0.006*"video" + 0.004*"Facebook" + 0.004*"Christmas" + 0.004*"people" + 0.003*"who" + 0.003*"today" + 0.003*"YouTube" + 0.003*"you" + 0.002*"time" + 0.002*"Twitter"
topic #3 (0.200): 0.007*"Twitter" + 0.006*"WikiLeaks" + 0.005*"US" + 0.004*"China" + 0.004*"Julian Assange" + 0.004*"Wikileaks" + 0.003*"India" + 0.003*"Assange" + 0.003*"will" + 0.003*"Facebook"
topic #4 (0.200): 0.007*"GOP" + 0.005*"Obama" + 0.005*"Senate" + 0.005*"bill" + 0.004*"pf" + 0.004*"will" + 0.004*"tcot" + 0.003*"para" + 0.003*"House" + 0.003*"vote"
topic diff=0.450793, rho=0.367756
PROGRESS: pass 1, at document #8000/10788
performing inference on a chunk of 2000 documents
1984/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.003*"VE" + 0.003*"time" + 0.002*"lmao" + 0.002*"LOL" + 0.002*"ur" + 0.002*"you" + 0.002*"Lol" + 0.002*"can" + 0.002*"Christmas"
topic #1 (0.200): 0.005*"Larry King" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"U.S" + 0.003*"will" + 0.002*"Morgan Freeman" + 0.002*"NYT" + 0.002*"Christmas" + 0.002*"Blake Edwards" + 0.002*"us"
topic #2 (0.200): 0.006*"video" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"people" + 0.003*"who" + 0.002*"you" + 0.002*"time" + 0.002*"today" + 0.002*"Twitter" + 0.002*"YouTube"
topic #3 (0.200): 0.006*"Twitter" + 0.006*"WikiLeaks" + 0.005*"US" + 0.004*"Julian Assange" + 0.004*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.003*"Assange" + 0.003*"will" + 0.003*"Japan"
topic #4 (0.200): 0.006*"GOP" + 0.006*"Obama" + 0.005*"bill" + 0.005*"Senate" + 0.004*"para" + 0.004*"tcot" + 0.004*"un" + 0.004*"House" + 0.004*"DADT" + 0.003*"will"
topic diff=0.521652, rho=0.367756
PROGRESS: pass 1, at document #10000/10788
performing inference on a chunk of 2000 documents
1987/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.004*"Como" + 0.003*"ur" + 0.003*"time" + 0.003*"Lol" + 0.003*"God" + 0.003*"poco" + 0.003*"lentamente" + 0.003*"you" + 0.003*"poco a poco"
topic #1 (0.200): 0.006*"friends" + 0.005*"Family" + 0.004*"Concert" + 0.004*"Presbyterian" + 0.004*"Biblical" + 0.004*"Larry King" + 0.004*"Fundraising" + 0.003*"Follow Friday" + 0.003*"CNN" + 0.003*"Breaking News"
topic #2 (0.200): 0.005*"video" + 0.004*"Christmas" + 0.004*"Twitter" + 0.004*"Facebook" + 0.004*"people" + 0.003*"UR" + 0.003*"tweet" + 0.003*"who" + 0.003*"time" + 0.003*"you"
topic #3 (0.200): 0.009*"Twitter" + 0.006*"US" + 0.006*"WikiLeaks" + 0.005*"India" + 0.004*"China" + 0.004*"tweet" + 0.004*"Julian Assange" + 0.004*"Wikileaks" + 0.004*"Pakistan" + 0.003*"Assange"
topic #4 (0.200): 0.007*"GOP" + 0.005*"para" + 0.005*"Obama" + 0.005*"bill" + 0.004*"Senate" + 0.004*"un" + 0.004*"tcot" + 0.003*"House" + 0.003*"DADT" + 0.003*"will"
topic diff=0.566574, rho=0.367756
bound: at document #0
-10.285 per-word bound, 1248.1 perplexity estimate based on a held-out corpus of 788 documents with 20452 words
PROGRESS: pass 1, at document #10788/10788
performing inference on a chunk of 788 documents
786/788 documents converged within 50 iterations
updating topics
merging changes from 788 documents into a model of 10788 documents
topic #0 (0.200): 0.013*"lol" + 0.010*"Como" + 0.003*"ur" + 0.003*"time" + 0.003*"you" + 0.003*"God" + 0.003*"VE" + 0.003*"Lol" + 0.002*"can" + 0.002*"LOL"
topic #1 (0.200): 0.004*"friends" + 0.003*"Follow Friday" + 0.003*"CNN" + 0.003*"Family" + 0.003*"Larry King" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.003*"U.S" + 0.003*"Concert" + 0.003*"Presbyterian"
topic #2 (0.200): 0.007*"video" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Facebook" + 0.004*"Twitter" + 0.003*"hindi" + 0.003*"who" + 0.003*"tweet" + 0.003*"you" + 0.003*"today"
topic #3 (0.200): 0.008*"Twitter" + 0.007*"US" + 0.006*"Sudan" + 0.006*"WikiLeaks" + 0.005*"Peace" + 0.005*"Julian Assange" + 0.004*"WORLD" + 0.004*"China" + 0.004*"via        s" + 0.004*"India"
topic #4 (0.200): 0.008*"GOP" + 0.006*"para" + 0.006*"un" + 0.005*"Senate" + 0.005*"bill" + 0.005*"Venezuela" + 0.005*"Obama" + 0.004*"House" + 0.004*"tcot" + 0.004*"will"
topic diff=0.409746, rho=0.367756
PROGRESS: pass 2, at document #2000/10788
performing inference on a chunk of 2000 documents
1981/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.013*"lol" + 0.008*"Como" + 0.003*"ur" + 0.003*"time" + 0.003*"you" + 0.003*"God" + 0.002*"VE" + 0.002*"LOL" + 0.002*"will" + 0.002*"Lol"
topic #1 (0.200): 0.004*"CNN" + 0.004*"Larry King" + 0.004*"Follow Friday" + 0.003*"friends" + 0.003*"U.S" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.003*"Family" + 0.002*"Concert" + 0.002*"Biblical"
topic #2 (0.200): 0.006*"video" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Twitter" + 0.003*"you" + 0.003*"who" + 0.003*"app" + 0.003*"today" + 0.003*"tweet"
topic #3 (0.200): 0.008*"Twitter" + 0.006*"US" + 0.006*"WikiLeaks" + 0.004*"China" + 0.004*"Julian Assange" + 0.004*"Sudan" + 0.004*"Wikileaks" + 0.004*"India" + 0.003*"Pakistan" + 0.003*"Facebook"
topic #4 (0.200): 0.007*"GOP" + 0.006*"para" + 0.006*"Obama" + 0.006*"Senate" + 0.005*"un" + 0.005*"bill" + 0.004*"House" + 0.004*"tcot" + 0.004*"Venezuela" + 0.004*"will"
topic diff=0.311570, rho=0.345156
PROGRESS: pass 2, at document #4000/10788
performing inference on a chunk of 2000 documents
1987/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.005*"Como" + 0.004*"LOL" + 0.003*"ur" + 0.003*"time" + 0.003*"you" + 0.002*"will" + 0.002*"God" + 0.002*"Lol" + 0.002*"can"
topic #1 (0.200): 0.004*"Larry King" + 0.004*"NYT" + 0.004*"CNN" + 0.003*"U.S" + 0.003*"will" + 0.003*"Follow Friday" + 0.003*"Morgan Freeman" + 0.002*"LAT" + 0.002*"friends" + 0.002*"Roll Call"
topic #2 (0.200): 0.008*"video" + 0.005*"YouTube" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.003*"Twitter" + 0.003*"time" + 0.003*"who" + 0.003*"today" + 0.003*"you"
topic #3 (0.200): 0.007*"Twitter" + 0.006*"WikiLeaks" + 0.006*"US" + 0.004*"Wikileaks" + 0.004*"Julian Assange" + 0.004*"China" + 0.004*"India" + 0.003*"Facebook" + 0.003*"Pakistan" + 0.003*"Sudan"
topic #4 (0.200): 0.007*"GOP" + 0.006*"Obama" + 0.006*"Senate" + 0.006*"bill" + 0.005*"tcot" + 0.004*"House" + 0.004*"para" + 0.004*"un" + 0.004*"will" + 0.004*"vote"
topic diff=0.324058, rho=0.345156
PROGRESS: pass 2, at document #6000/10788
performing inference on a chunk of 2000 documents
1988/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.004*"Como" + 0.003*"ur" + 0.003*"LOL" + 0.003*"you" + 0.003*"time" + 0.003*"God" + 0.002*"Lol" + 0.002*"can" + 0.002*"Christmas"
topic #1 (0.200): 0.005*"Larry King" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"NYT" + 0.003*"will" + 0.003*"U.S" + 0.003*"Morgan Freeman" + 0.002*"Haiti" + 0.002*"friends" + 0.002*"Blake Edwards"
topic #2 (0.200): 0.007*"video" + 0.004*"Facebook" + 0.004*"Christmas" + 0.004*"people" + 0.003*"YouTube" + 0.003*"Twitter" + 0.003*"today" + 0.003*"who" + 0.003*"you" + 0.003*"time"
topic #3 (0.200): 0.007*"Twitter" + 0.006*"WikiLeaks" + 0.006*"US" + 0.004*"China" + 0.004*"Julian Assange" + 0.004*"Wikileaks" + 0.004*"India" + 0.003*"Assange" + 0.003*"Facebook" + 0.003*"Pakistan"
topic #4 (0.200): 0.007*"GOP" + 0.006*"Obama" + 0.006*"Senate" + 0.006*"bill" + 0.004*"tcot" + 0.004*"House" + 0.004*"pf" + 0.004*"will" + 0.004*"para" + 0.004*"vote"
topic diff=0.318562, rho=0.345156
PROGRESS: pass 2, at document #8000/10788
performing inference on a chunk of 2000 documents
1987/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.016*"lol" + 0.003*"VE" + 0.003*"ur" + 0.003*"you" + 0.003*"LOL" + 0.003*"Lol" + 0.003*"time" + 0.003*"lmao" + 0.002*"can" + 0.002*"Como"
topic #1 (0.200): 0.005*"Larry King" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"U.S" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.003*"NYT" + 0.002*"Blake Edwards" + 0.002*"Christmas" + 0.002*"friends"
topic #2 (0.200): 0.006*"video" + 0.004*"Christmas" + 0.004*"Facebook" + 0.004*"people" + 0.003*"Twitter" + 0.003*"who" + 0.003*"time" + 0.003*"today" + 0.003*"YouTube" + 0.003*"you"
topic #3 (0.200): 0.006*"WikiLeaks" + 0.006*"US" + 0.006*"Twitter" + 0.005*"Julian Assange" + 0.004*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.003*"Assange" + 0.003*"Japan" + 0.003*"Pakistan"
topic #4 (0.200): 0.006*"GOP" + 0.006*"Obama" + 0.005*"Senate" + 0.005*"bill" + 0.005*"tcot" + 0.005*"para" + 0.004*"un" + 0.004*"House" + 0.004*"Venezuela" + 0.004*"will"
topic diff=0.379030, rho=0.345156
PROGRESS: pass 2, at document #10000/10788
performing inference on a chunk of 2000 documents
1985/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.004*"Como" + 0.004*"ur" + 0.003*"Lol" + 0.003*"you" + 0.003*"time" + 0.003*"LOL" + 0.003*"God" + 0.003*"can" + 0.002*"poco"
topic #1 (0.200): 0.006*"friends" + 0.005*"Family" + 0.005*"Larry King" + 0.004*"Concert" + 0.004*"Biblical" + 0.004*"Presbyterian" + 0.004*"Fundraising" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"Breaking News"
topic #2 (0.200): 0.006*"video" + 0.006*"Twitter" + 0.005*"tweet" + 0.004*"Christmas" + 0.004*"Facebook" + 0.004*"people" + 0.003*"UR" + 0.003*"who" + 0.003*"time" + 0.003*"today"
topic #3 (0.200): 0.008*"Twitter" + 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"China" + 0.005*"Julian Assange" + 0.005*"India" + 0.004*"Wikileaks" + 0.004*"Pakistan" + 0.003*"Assange" + 0.003*"via"
topic #4 (0.200): 0.007*"GOP" + 0.006*"para" + 0.005*"Obama" + 0.005*"bill" + 0.005*"Senate" + 0.005*"un" + 0.004*"tcot" + 0.004*"House" + 0.003*"Venezuela" + 0.003*"DADT"
topic diff=0.424680, rho=0.345156
bound: at document #0
-10.107 per-word bound, 1102.6 perplexity estimate based on a held-out corpus of 788 documents with 20452 words
PROGRESS: pass 2, at document #10788/10788
performing inference on a chunk of 788 documents
786/788 documents converged within 50 iterations
updating topics
merging changes from 788 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.010*"Como" + 0.004*"ur" + 0.003*"you" + 0.003*"time" + 0.003*"Lol" + 0.003*"God" + 0.003*"VE" + 0.003*"LOL" + 0.002*"can"
topic #1 (0.200): 0.004*"friends" + 0.004*"Follow Friday" + 0.004*"Larry King" + 0.004*"CNN" + 0.004*"Family" + 0.003*"will" + 0.003*"U.S" + 0.003*"Morgan Freeman" + 0.003*"Concert" + 0.003*"Biblical"
topic #2 (0.200): 0.007*"video" + 0.006*"Twitter" + 0.004*"people" + 0.004*"tweet" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"hindi" + 0.003*"who" + 0.003*"today" + 0.003*"you"
topic #3 (0.200): 0.007*"Twitter" + 0.007*"US" + 0.006*"WikiLeaks" + 0.006*"Sudan" + 0.005*"Julian Assange" + 0.005*"Peace" + 0.004*"China" + 0.004*"India" + 0.004*"Wikileaks" + 0.004*"WORLD"
topic #4 (0.200): 0.008*"GOP" + 0.006*"para" + 0.006*"un" + 0.006*"Senate" + 0.006*"bill" + 0.005*"Venezuela" + 0.005*"Obama" + 0.004*"House" + 0.004*"tcot" + 0.004*"will"
topic diff=0.304283, rho=0.345156
PROGRESS: pass 3, at document #2000/10788
performing inference on a chunk of 2000 documents
1986/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.008*"Como" + 0.004*"ur" + 0.003*"you" + 0.003*"time" + 0.003*"LOL" + 0.003*"God" + 0.003*"Lol" + 0.002*"VE" + 0.002*"can"
topic #1 (0.200): 0.004*"Larry King" + 0.004*"CNN" + 0.004*"Follow Friday" + 0.003*"friends" + 0.003*"U.S" + 0.003*"will" + 0.003*"Family" + 0.003*"Morgan Freeman" + 0.002*"Concert" + 0.002*"London"
topic #2 (0.200): 0.007*"video" + 0.005*"Twitter" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.004*"tweet" + 0.003*"today" + 0.003*"who" + 0.003*"you" + 0.003*"app"
topic #3 (0.200): 0.007*"Twitter" + 0.006*"US" + 0.006*"WikiLeaks" + 0.005*"Julian Assange" + 0.004*"China" + 0.004*"Sudan" + 0.004*"Wikileaks" + 0.004*"Facebook" + 0.004*"India" + 0.004*"Pakistan"
topic #4 (0.200): 0.008*"GOP" + 0.006*"Senate" + 0.006*"bill" + 0.006*"para" + 0.006*"Obama" + 0.006*"un" + 0.005*"House" + 0.004*"tcot" + 0.004*"Venezuela" + 0.004*"will"
topic diff=0.250330, rho=0.326268
PROGRESS: pass 3, at document #4000/10788
performing inference on a chunk of 2000 documents
1990/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.006*"Como" + 0.004*"LOL" + 0.004*"ur" + 0.003*"you" + 0.003*"time" + 0.003*"Lol" + 0.002*"God" + 0.002*"will" + 0.002*"can"
topic #1 (0.200): 0.004*"Larry King" + 0.004*"NYT" + 0.004*"CNN" + 0.003*"U.S" + 0.003*"Follow Friday" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.003*"friends" + 0.002*"LAT" + 0.002*"Roll Call"
topic #2 (0.200): 0.008*"video" + 0.005*"YouTube" + 0.005*"Twitter" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.003*"tweet" + 0.003*"today" + 0.003*"time" + 0.003*"who"
topic #3 (0.200): 0.006*"WikiLeaks" + 0.006*"Twitter" + 0.006*"US" + 0.005*"Julian Assange" + 0.004*"Wikileaks" + 0.004*"China" + 0.004*"India" + 0.004*"Facebook" + 0.004*"Pakistan" + 0.003*"Assange"
topic #4 (0.200): 0.007*"GOP" + 0.007*"Obama" + 0.006*"Senate" + 0.006*"bill" + 0.005*"tcot" + 0.005*"House" + 0.005*"para" + 0.004*"un" + 0.004*"will" + 0.004*"vote"
topic diff=0.254423, rho=0.326268
PROGRESS: pass 3, at document #6000/10788
performing inference on a chunk of 2000 documents
1990/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.004*"Como" + 0.004*"ur" + 0.004*"LOL" + 0.003*"you" + 0.003*"time" + 0.003*"God" + 0.003*"Lol" + 0.003*"can" + 0.002*"Christmas"
topic #1 (0.200): 0.005*"Larry King" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"NYT" + 0.003*"U.S" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.002*"friends" + 0.002*"Haiti" + 0.002*"Blake Edwards"
topic #2 (0.200): 0.007*"video" + 0.004*"Twitter" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.004*"YouTube" + 0.003*"today" + 0.003*"who" + 0.003*"you" + 0.003*"time"
topic #3 (0.200): 0.006*"WikiLeaks" + 0.006*"Twitter" + 0.006*"US" + 0.005*"Julian Assange" + 0.005*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.003*"Facebook" + 0.003*"Pakistan" + 0.003*"Assange"
topic #4 (0.200): 0.007*"GOP" + 0.006*"Senate" + 0.006*"Obama" + 0.006*"bill" + 0.005*"tcot" + 0.005*"House" + 0.004*"will" + 0.004*"para" + 0.004*"pf" + 0.004*"un"
topic diff=0.252382, rho=0.326268
PROGRESS: pass 3, at document #8000/10788
performing inference on a chunk of 2000 documents
1992/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.017*"lol" + 0.004*"ur" + 0.003*"you" + 0.003*"LOL" + 0.003*"VE" + 0.003*"Lol" + 0.003*"time" + 0.003*"lmao" + 0.003*"can" + 0.003*"Como"
topic #1 (0.200): 0.005*"Larry King" + 0.005*"Follow Friday" + 0.004*"CNN" + 0.003*"U.S" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.003*"NYT" + 0.002*"Blake Edwards" + 0.002*"friends" + 0.002*"Christmas"
topic #2 (0.200): 0.006*"video" + 0.004*"Christmas" + 0.004*"Twitter" + 0.004*"Facebook" + 0.004*"people" + 0.003*"YouTube" + 0.003*"today" + 0.003*"who" + 0.003*"time" + 0.003*"you"
topic #3 (0.200): 0.006*"WikiLeaks" + 0.006*"US" + 0.005*"Twitter" + 0.005*"Julian Assange" + 0.004*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.003*"Assange" + 0.003*"Pakistan" + 0.003*"via"
topic #4 (0.200): 0.007*"GOP" + 0.006*"Obama" + 0.006*"Senate" + 0.006*"bill" + 0.005*"tcot" + 0.005*"para" + 0.005*"un" + 0.005*"House" + 0.004*"Venezuela" + 0.004*"will"
topic diff=0.302918, rho=0.326268
PROGRESS: pass 3, at document #10000/10788
performing inference on a chunk of 2000 documents
1990/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.016*"lol" + 0.004*"ur" + 0.004*"Como" + 0.004*"Lol" + 0.004*"you" + 0.003*"LOL" + 0.003*"time" + 0.003*"can" + 0.003*"God" + 0.002*"poco"
topic #1 (0.200): 0.006*"friends" + 0.005*"Family" + 0.005*"Larry King" + 0.004*"Concert" + 0.004*"Presbyterian" + 0.004*"Biblical" + 0.004*"Fundraising" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"U.S"
topic #2 (0.200): 0.009*"Twitter" + 0.007*"tweet" + 0.006*"video" + 0.004*"Christmas" + 0.004*"Facebook" + 0.004*"people" + 0.003*"UR" + 0.003*"who" + 0.003*"time" + 0.003*"today"
topic #3 (0.200): 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"Twitter" + 0.005*"Julian Assange" + 0.005*"China" + 0.005*"India" + 0.004*"Wikileaks" + 0.004*"Pakistan" + 0.003*"Assange" + 0.003*"via"
topic #4 (0.200): 0.007*"GOP" + 0.006*"para" + 0.006*"Obama" + 0.005*"bill" + 0.005*"Senate" + 0.005*"un" + 0.005*"tcot" + 0.004*"House" + 0.004*"Venezuela" + 0.003*"DADT"
topic diff=0.346856, rho=0.326268
bound: at document #0
-10.006 per-word bound, 1028.4 perplexity estimate based on a held-out corpus of 788 documents with 20452 words
PROGRESS: pass 3, at document #10788/10788
performing inference on a chunk of 788 documents
785/788 documents converged within 50 iterations
updating topics
merging changes from 788 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.010*"Como" + 0.004*"ur" + 0.004*"you" + 0.003*"Lol" + 0.003*"time" + 0.003*"God" + 0.003*"LOL" + 0.003*"VE" + 0.003*"can"
topic #1 (0.200): 0.004*"friends" + 0.004*"Larry King" + 0.004*"Follow Friday" + 0.004*"Family" + 0.003*"CNN" + 0.003*"will" + 0.003*"U.S" + 0.003*"London" + 0.003*"Concert" + 0.003*"Morgan Freeman"
topic #2 (0.200): 0.008*"Twitter" + 0.007*"video" + 0.006*"tweet" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"hindi" + 0.003*"today" + 0.003*"who" + 0.003*"you"
topic #3 (0.200): 0.007*"US" + 0.006*"WikiLeaks" + 0.006*"Sudan" + 0.005*"Julian Assange" + 0.005*"Peace" + 0.005*"Twitter" + 0.004*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.004*"WORLD"
topic #4 (0.200): 0.008*"GOP" + 0.006*"para" + 0.006*"un" + 0.006*"Senate" + 0.006*"bill" + 0.006*"Venezuela" + 0.005*"Obama" + 0.004*"House" + 0.004*"tcot" + 0.004*"will"
topic diff=0.250310, rho=0.326268
PROGRESS: pass 4, at document #2000/10788
performing inference on a chunk of 2000 documents
1978/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.008*"Como" + 0.004*"ur" + 0.003*"you" + 0.003*"time" + 0.003*"LOL" + 0.003*"Lol" + 0.003*"God" + 0.002*"can" + 0.002*"VE"
topic #1 (0.200): 0.004*"Larry King" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"friends" + 0.003*"U.S" + 0.003*"Family" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.002*"London" + 0.002*"Concert"
topic #2 (0.200): 0.008*"Twitter" + 0.007*"video" + 0.005*"tweet" + 0.004*"Facebook" + 0.004*"people" + 0.004*"Christmas" + 0.003*"today" + 0.003*"who" + 0.003*"you" + 0.003*"app"
topic #3 (0.200): 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"Twitter" + 0.005*"Julian Assange" + 0.005*"China" + 0.004*"Wikileaks" + 0.004*"Sudan" + 0.004*"Facebook" + 0.004*"Pakistan" + 0.004*"India"
topic #4 (0.200): 0.008*"GOP" + 0.006*"Senate" + 0.006*"bill" + 0.006*"para" + 0.006*"Obama" + 0.006*"un" + 0.005*"House" + 0.005*"tcot" + 0.004*"Venezuela" + 0.004*"will"
topic diff=0.218445, rho=0.310176
PROGRESS: pass 4, at document #4000/10788
performing inference on a chunk of 2000 documents
1985/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.015*"lol" + 0.006*"Como" + 0.004*"LOL" + 0.004*"ur" + 0.004*"you" + 0.003*"time" + 0.003*"Lol" + 0.002*"God" + 0.002*"will" + 0.002*"can"
topic #1 (0.200): 0.005*"Larry King" + 0.004*"NYT" + 0.004*"CNN" + 0.003*"Follow Friday" + 0.003*"U.S" + 0.003*"will" + 0.003*"friends" + 0.003*"Morgan Freeman" + 0.002*"LAT" + 0.002*"Family"
topic #2 (0.200): 0.008*"video" + 0.007*"Twitter" + 0.005*"YouTube" + 0.004*"Facebook" + 0.004*"people" + 0.004*"tweet" + 0.004*"Christmas" + 0.003*"today" + 0.003*"time" + 0.003*"who"
topic #3 (0.200): 0.007*"WikiLeaks" + 0.006*"US" + 0.005*"Julian Assange" + 0.005*"Twitter" + 0.004*"China" + 0.004*"Wikileaks" + 0.004*"Facebook" + 0.004*"India" + 0.004*"Pakistan" + 0.003*"Assange"
topic #4 (0.200): 0.008*"GOP" + 0.007*"Senate" + 0.007*"Obama" + 0.006*"bill" + 0.005*"tcot" + 0.005*"House" + 0.005*"para" + 0.004*"un" + 0.004*"will" + 0.004*"vote"
topic diff=0.216916, rho=0.310176
PROGRESS: pass 4, at document #6000/10788
performing inference on a chunk of 2000 documents
1994/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.016*"lol" + 0.004*"Como" + 0.004*"ur" + 0.004*"LOL" + 0.004*"you" + 0.003*"time" + 0.003*"Lol" + 0.003*"God" + 0.003*"can" + 0.002*"Christmas"
topic #1 (0.200): 0.005*"Larry King" + 0.004*"Follow Friday" + 0.004*"CNN" + 0.003*"NYT" + 0.003*"U.S" + 0.003*"will" + 0.003*"Morgan Freeman" + 0.002*"friends" + 0.002*"Blake Edwards" + 0.002*"Haiti"
topic #2 (0.200): 0.007*"video" + 0.006*"Twitter" + 0.004*"Facebook" + 0.004*"YouTube" + 0.004*"people" + 0.004*"Christmas" + 0.004*"tweet" + 0.003*"today" + 0.003*"who" + 0.003*"you"
topic #3 (0.200): 0.006*"WikiLeaks" + 0.006*"US" + 0.005*"Julian Assange" + 0.005*"China" + 0.004*"Twitter" + 0.004*"Wikileaks" + 0.004*"Facebook" + 0.004*"India" + 0.003*"Pakistan" + 0.003*"Assange"
topic #4 (0.200): 0.007*"GOP" + 0.007*"Senate" + 0.006*"Obama" + 0.006*"bill" + 0.005*"House" + 0.005*"tcot" + 0.004*"para" + 0.004*"will" + 0.004*"pf" + 0.004*"un"
topic diff=0.216322, rho=0.310176
PROGRESS: pass 4, at document #8000/10788
performing inference on a chunk of 2000 documents
1988/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.017*"lol" + 0.004*"ur" + 0.004*"you" + 0.003*"LOL" + 0.003*"Lol" + 0.003*"VE" + 0.003*"time" + 0.003*"Como" + 0.003*"can" + 0.003*"lmao"
topic #1 (0.200): 0.005*"Larry King" + 0.005*"Follow Friday" + 0.004*"CNN" + 0.003*"U.S" + 0.003*"will" + 0.003*"NYT" + 0.003*"Morgan Freeman" + 0.003*"Blake Edwards" + 0.002*"friends" + 0.002*"Christmas"
topic #2 (0.200): 0.007*"video" + 0.006*"Twitter" + 0.004*"Christmas" + 0.004*"Facebook" + 0.004*"people" + 0.003*"YouTube" + 0.003*"tweet" + 0.003*"today" + 0.003*"who" + 0.003*"time"
topic #3 (0.200): 0.006*"WikiLeaks" + 0.006*"US" + 0.005*"Julian Assange" + 0.004*"China" + 0.004*"Wikileaks" + 0.004*"Twitter" + 0.004*"India" + 0.003*"Assange" + 0.003*"Pakistan" + 0.003*"Facebook"
topic #4 (0.200): 0.007*"GOP" + 0.007*"Obama" + 0.006*"Senate" + 0.006*"bill" + 0.005*"tcot" + 0.005*"para" + 0.005*"House" + 0.005*"un" + 0.004*"Venezuela" + 0.004*"will"
topic diff=0.259149, rho=0.310176
PROGRESS: pass 4, at document #10000/10788
performing inference on a chunk of 2000 documents
1991/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 10788 documents
topic #0 (0.200): 0.016*"lol" + 0.005*"ur" + 0.004*"Como" + 0.004*"Lol" + 0.004*"you" + 0.003*"LOL" + 0.003*"time" + 0.003*"can" + 0.003*"God" + 0.002*"poco"
topic #1 (0.200): 0.006*"friends" + 0.005*"Family" + 0.005*"Larry King" + 0.004*"Concert" + 0.004*"Biblical" + 0.004*"Presbyterian" + 0.004*"Fundraising" + 0.004*"Follow Friday" + 0.003*"CNN" + 0.003*"U.S"
topic #2 (0.200): 0.011*"Twitter" + 0.008*"tweet" + 0.006*"video" + 0.004*"Christmas" + 0.004*"Facebook" + 0.004*"people" + 0.003*"now" + 0.003*"UR" + 0.003*"who" + 0.003*"today"
topic #3 (0.200): 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"Julian Assange" + 0.005*"China" + 0.005*"India" + 0.004*"Wikileaks" + 0.004*"Pakistan" + 0.004*"Twitter" + 0.003*"Facebook" + 0.003*"via"
topic #4 (0.200): 0.007*"GOP" + 0.006*"Obama" + 0.006*"para" + 0.006*"Senate" + 0.006*"bill" + 0.005*"un" + 0.005*"tcot" + 0.004*"House" + 0.004*"Venezuela" + 0.003*"DADT"
topic diff=0.301435, rho=0.310176
bound: at document #0
-9.937 per-word bound, 980.4 perplexity estimate based on a held-out corpus of 788 documents with 20452 words
PROGRESS: pass 4, at document #10788/10788
performing inference on a chunk of 788 documents
785/788 documents converged within 50 iterations
updating topics
merging changes from 788 documents into a model of 10788 documents
topic #0 (0.200): 0.014*"lol" + 0.009*"Como" + 0.004*"ur" + 0.004*"you" + 0.003*"Lol" + 0.003*"time" + 0.003*"God" + 0.003*"LOL" + 0.003*"VE" + 0.003*"can"
topic #1 (0.200): 0.004*"friends" + 0.004*"Larry King" + 0.004*"Follow Friday" + 0.004*"Family" + 0.003*"CNN" + 0.003*"London" + 0.003*"will" + 0.003*"Concert" + 0.003*"U.S" + 0.003*"Presbyterian"
topic #2 (0.200): 0.010*"Twitter" + 0.007*"video" + 0.006*"tweet" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"today" + 0.003*"who" + 0.003*"hindi" + 0.003*"you"
topic #3 (0.200): 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"Sudan" + 0.005*"Julian Assange" + 0.005*"Peace" + 0.005*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.004*"Pakistan" + 0.004*"WORLD"
topic #4 (0.200): 0.008*"GOP" + 0.006*"para" + 0.006*"un" + 0.006*"Senate" + 0.006*"Venezuela" + 0.006*"bill" + 0.005*"Obama" + 0.005*"House" + 0.004*"tcot" + 0.004*"will"
topic diff=0.219453, rho=0.310176
saving LdaState object under ../output/58/tml/gensim_5topics.model.state, separately None
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/58/tml/gensim_5topics.model.state'}
saved ../output/58/tml/gensim_5topics.model.state
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/58/tml/gensim_5topics.model.id2word'}
saving LdaModel object under ../output/58/tml/gensim_5topics.model, separately ['expElogbeta', 'sstats']
storing np array 'expElogbeta' to ../output/58/tml/gensim_5topics.model.expElogbeta.npy
not storing attribute dispatcher
not storing attribute id2word
not storing attribute state
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/58/tml/gensim_5topics.model'}
saved ../output/58/tml/gensim_5topics.model
topic #0 (0.200): 0.014*"lol" + 0.009*"Como" + 0.004*"ur" + 0.004*"you" + 0.003*"Lol" + 0.003*"time" + 0.003*"God" + 0.003*"LOL" + 0.003*"VE" + 0.003*"can"
topic #1 (0.200): 0.004*"friends" + 0.004*"Larry King" + 0.004*"Follow Friday" + 0.004*"Family" + 0.003*"CNN" + 0.003*"London" + 0.003*"will" + 0.003*"Concert" + 0.003*"U.S" + 0.003*"Presbyterian"
topic #2 (0.200): 0.010*"Twitter" + 0.007*"video" + 0.006*"tweet" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"today" + 0.003*"who" + 0.003*"hindi" + 0.003*"you"
topic #3 (0.200): 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"Sudan" + 0.005*"Julian Assange" + 0.005*"Peace" + 0.005*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.004*"Pakistan" + 0.004*"WORLD"
topic #4 (0.200): 0.008*"GOP" + 0.006*"para" + 0.006*"un" + 0.006*"Senate" + 0.006*"Venezuela" + 0.006*"bill" + 0.005*"Obama" + 0.005*"House" + 0.004*"tcot" + 0.004*"will"
TopicModeling: GENSIM Topic: 0 
Words: 0.014*"lol" + 0.009*"Como" + 0.004*"ur" + 0.004*"you" + 0.003*"Lol" + 0.003*"time" + 0.003*"God" + 0.003*"LOL" + 0.003*"VE" + 0.003*"can"
TopicModeling: GENSIM Topic: 1 
Words: 0.004*"friends" + 0.004*"Larry King" + 0.004*"Follow Friday" + 0.004*"Family" + 0.003*"CNN" + 0.003*"London" + 0.003*"will" + 0.003*"Concert" + 0.003*"U.S" + 0.003*"Presbyterian"
TopicModeling: GENSIM Topic: 2 
Words: 0.010*"Twitter" + 0.007*"video" + 0.006*"tweet" + 0.004*"people" + 0.004*"Christmas" + 0.004*"Facebook" + 0.003*"today" + 0.003*"who" + 0.003*"hindi" + 0.003*"you"
TopicModeling: GENSIM Topic: 3 
Words: 0.007*"US" + 0.006*"WikiLeaks" + 0.005*"Sudan" + 0.005*"Julian Assange" + 0.005*"Peace" + 0.005*"China" + 0.004*"Wikileaks" + 0.004*"India" + 0.004*"Pakistan" + 0.004*"WORLD"
TopicModeling: GENSIM Topic: 4 
Words: 0.008*"GOP" + 0.006*"para" + 0.006*"un" + 0.006*"Senate" + 0.006*"Venezuela" + 0.006*"bill" + 0.005*"Obama" + 0.005*"House" + 0.004*"tcot" + 0.004*"will"
TopicModeling: Coherences:

TopicModeling: Calculating model coherence:

Setting topics to those of the model: LdaModel(num_terms=49968, num_topics=5, decay=0.5, chunksize=2000)
CorpusAccumulator accumulated stats from 1000 documents
CorpusAccumulator accumulated stats from 2000 documents
CorpusAccumulator accumulated stats from 3000 documents
CorpusAccumulator accumulated stats from 4000 documents
CorpusAccumulator accumulated stats from 5000 documents
CorpusAccumulator accumulated stats from 6000 documents
CorpusAccumulator accumulated stats from 7000 documents
CorpusAccumulator accumulated stats from 8000 documents
CorpusAccumulator accumulated stats from 9000 documents
CorpusAccumulator accumulated stats from 10000 documents
CorpusAccumulator accumulated stats from 1000 documents
CorpusAccumulator accumulated stats from 2000 documents
CorpusAccumulator accumulated stats from 3000 documents
CorpusAccumulator accumulated stats from 4000 documents
CorpusAccumulator accumulated stats from 5000 documents
CorpusAccumulator accumulated stats from 6000 documents
CorpusAccumulator accumulated stats from 7000 documents
CorpusAccumulator accumulated stats from 8000 documents
CorpusAccumulator accumulated stats from 9000 documents
CorpusAccumulator accumulated stats from 10000 documents
TopicModeling: Coherence value is: -4.657868511885745
TopicModeling: Topic coherences are: [-5.6707022517944266, -6.595887569574447, -2.5373837727942847, -2.58786141106487, -2.1722134665811113]
saving Dictionary object under ../output/58/tml/gensim_5topics_TopicModelingDictionary.mm, separately None
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/58/tml/gensim_5topics_TopicModelingDictionary.mm'}
saved ../output/58/tml/gensim_5topics_TopicModelingDictionary.mm
UserSimilarity: Topic modeling done
UserSimilarity: All users size 10788
UserSimilarity: All distinct users:10788
UserSimilarity: users_topic_interests=(10788, 5)
UserSimilarity: Just one topic? False, Binary topic? True, Threshold: 0.2
10788 users has twitted in 2010-12-17 00:00:00
UserSimilarity: 0 / 1
UserSimilarity: UsersTopicInterests.npy is saved for day:0 with shape: (10788, 5)
UsersGraph: There are 10788 users on 0
