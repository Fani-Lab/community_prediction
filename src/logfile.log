CACHEDIR=C:\Users\sorou\.matplotlib
Using fontManager instance from C:\Users\sorou\.matplotlib\fontlist-v330.json
Loaded backend module://backend_interagg version unknown.
Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
Main: UserSimilarities ...
DataReader: Connection created
DataReader: 39968 rows returned
DataReader: Connection closed
DataPreperation: userModeling=True, timeModeling=True,preProcessing=False, TagME=False
DataPreperation: 300 sampled from the end of dataset (sorted by creationTime)
DataPreperation: Length of the dataset after applying groupby: 135 

UserSimilarity: Processed docs shape: (135,)
UserSimilarity: Topic modeling ...
TopicModeling: num_topics=25,  filterExtremes=True, library=gensim
adding document #0 to Dictionary(0 unique tokens: [])
built Dictionary(816 unique tokens: ['Henry W. Longfellow', 'beautiful', 'evil', 'lover', "lover's"]...) from 135 documents (total 1065 corpus positions)
discarding 0 tokens: []...
keeping 816 tokens which were in no less than 1 and no more than 27 (=20.0%) documents
rebuilding dictionary, shrinking gaps
resulting dictionary: Dictionary(816 unique tokens: ['Henry W. Longfellow', 'beautiful', 'evil', 'lover', "lover's"]...)
using symmetric alpha at 0.04
using symmetric eta at 0.04
using serial LDA version on this node
running online (multi-pass) LDA training, 25 topics, 5 passes over the supplied corpus of 135 documents, updating model once every 135 documents, evaluating perplexity every 135 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
bound: at document #0
-42.930 per-word bound, 8376597509191.7 perplexity estimate based on a held-out corpus of 135 documents with 1065 words
PROGRESS: pass 0, at document #135/135
performing inference on a chunk of 135 documents
122/135 documents converged within 50 iterations
updating topics
topic #23 (0.040): 0.023*"fono" + 0.017*"Chopin" + 0.012*"Doors" + 0.011*"banned" + 0.011*"earth" + 0.011*"Cuba" + 0.011*"Wikileaks" + 0.011*"Michael Moore" + 0.011*"evolution" + 0.011*"Story"
topic #5 (0.040): 0.021*"to win" + 0.021*"BBC" + 0.021*"lifetime" + 0.020*"activism" + 0.020*"VAW" + 0.019*"rape" + 0.019*"tcot" + 0.011*"I believe" + 0.011*"Bedfordshire" + 0.011*"Dreamland"
topic #15 (0.040): 0.092*"BBC" + 0.026*"Gallo" + 0.026*"Knicks" + 0.018*"NBA" + 0.011*"snow" + 0.009*"WikiLeaks" + 0.009*"UK" + 0.009*"dunks" + 0.009*"Celts" + 0.009*"That's what I am"
topic #20 (0.040): 0.019*"Private" + 0.019*"Words" + 0.019*"Public / Private" + 0.019*"Public" + 0.019*"hoy" + 0.018*"spiritual" + 0.012*"ones" + 0.011*"sleep" + 0.011*"rituals" + 0.011*"Tarot"
topic #17 (0.040): 0.087*"Marisela" + 0.087*"Escobedo" + 0.022*"burla" + 0.022*"Aquí está" + 0.022*"justicia" + 0.022*"Chihuahua" + 0.022*"Río Pánuco" + 0.022*"U.S" + 0.001*"urges" + 0.001*"Angelina Jolie"
topic diff=20.957458, rho=1.000000
bound: at document #0
-8.045 per-word bound, 264.2 perplexity estimate based on a held-out corpus of 135 documents with 1065 words
PROGRESS: pass 1, at document #135/135
performing inference on a chunk of 135 documents
135/135 documents converged within 50 iterations
updating topics
topic #7 (0.040): 0.019*"spy" + 0.019*"on line" + 0.019*"Pakistan" + 0.019*"drone attacks" + 0.019*"customer.service" + 0.019*"iraq" + 0.019*"ukuncut" + 0.019*"country" + 0.019*"London" + 0.019*"Protest"
topic #2 (0.040): 0.021*"Aid" + 0.021*"Jeff" + 0.021*"State" + 0.021*"Malmberg" + 0.021*"Jobless" + 0.021*"evacuated" + 0.021*"Dallas-Fort Worth International Airport" + 0.021*"DFW Airport" + 0.021*"tcot" + 0.021*"People"
topic #0 (0.040): 0.029*"Brown Betty" + 0.029*"Cloak" + 0.019*"cool" + 0.019*"NOW" + 0.019*"Steve Cooley" + 0.019*"entertainment" + 0.019*"ugly" + 0.019*"War" + 0.019*"Good" + 0.019*"idols"
topic #14 (0.040): 0.012*"fasse" + 0.012*"puisse" + 0.012*"pourrais" + 0.012*"din" + 0.012*"Sued" + 0.012*"est" + 0.012*"pa" + 0.012*"na" + 0.012*"Mortgages" + 0.012*"Bank of America"
topic #22 (0.040): 0.140*"BBC" + 0.021*"snow" + 0.015*"UK" + 0.013*"blah" + 0.013*" blah" + 0.013*"am" + 0.013*"Conservative" + 0.013*"gop" + 0.013*"tcot" + 0.010*"hits"
topic diff=0.212151, rho=0.577350
bound: at document #0
-7.703 per-word bound, 208.3 perplexity estimate based on a held-out corpus of 135 documents with 1065 words
PROGRESS: pass 2, at document #135/135
performing inference on a chunk of 135 documents
135/135 documents converged within 50 iterations
updating topics
topic #15 (0.040): 0.037*"Gallo" + 0.037*"Knicks" + 0.028*"BBC" + 0.025*"NBA" + 0.013*"WikiLeaks" + 0.013*"dunks" + 0.013*"Celts" + 0.013*"That's what I am" + 0.013*"Bron" + 0.013*"Craig Sager"
topic #10 (0.040): 0.038*"Don't Ask Don't Tell" + 0.019*"old" + 0.019*"Lmao" + 0.019*"can" + 0.019*"Keri Hilson" + 0.019*"practice" + 0.019*"Donovan McNabb" + 0.019*"lol" + 0.019*"Repeal" + 0.019*"scout team"
topic #3 (0.040): 0.025*"ppl" + 0.023*"Venezuela" + 0.023*"decree" + 0.023*"lawmakers" + 0.023*"Chavez" + 0.023*"power" + 0.023*"govern" + 0.013*"who" + 0.013*"weeks" + 0.013*"Glaceau"
topic #13 (0.040): 0.027*"una más" + 0.027*"Reyes" + 0.027*"Cd  Juarez" + 0.027*"chick" + 0.027*"fat" + 0.016*"times" + 0.016*"HELP" + 0.016*"Fox News" + 0.016*"alread" + 0.016*"McCain's"
topic #19 (0.040): 0.039*"LeBron" + 0.039*"tonight" + 0.039*"nickname" + 0.039*"basketball" + 0.022*"Sale" + 0.022*"Atomic" + 0.022*"Information" + 0.022*"Warehouse" + 0.022*"Scientists" + 0.022*"Nucleii"
topic diff=0.142372, rho=0.500000
bound: at document #0
-7.553 per-word bound, 187.8 perplexity estimate based on a held-out corpus of 135 documents with 1065 words
PROGRESS: pass 3, at document #135/135
performing inference on a chunk of 135 documents
135/135 documents converged within 50 iterations
updating topics
topic #10 (0.040): 0.038*"Don't Ask Don't Tell" + 0.019*"old" + 0.019*"Lmao" + 0.019*"can" + 0.019*"Keri Hilson" + 0.019*"practice" + 0.019*"lol" + 0.019*"Donovan McNabb" + 0.019*"Repeal" + 0.019*"Song writing"
topic #16 (0.040): 0.024*"tires" + 0.024*"beautiful experience" + 0.024*"Albert Einstein" + 0.024*"world" + 0.024*"Couchman" + 0.024*"Fox News" + 0.024*"as long as I live" + 0.024*"racist" + 0.024*"teabaggers" + 0.024*"thanks in advance"
topic #12 (0.040): 0.029*"This Week at War" + 0.029*"North Korea" + 0.029*"Afghanistan" + 0.001*"wikileaks" + 0.001*"urges" + 0.001*"pourrais" + 0.001*"Angelina Jolie" + 0.001*"prosecute" + 0.001*"tu" + 0.001*"puisse"
topic #0 (0.040): 0.038*"Brown Betty" + 0.038*"Cloak" + 0.021*"cool" + 0.021*"NOW" + 0.021*"entertainment" + 0.021*"ugly" + 0.021*"Steve Cooley" + 0.021*"War" + 0.021*"Good" + 0.021*"idols"
topic #3 (0.040): 0.027*"Venezuela" + 0.027*"lawmakers" + 0.027*"decree" + 0.027*"Chavez" + 0.027*"power" + 0.027*"govern" + 0.025*"ppl" + 0.013*"who" + 0.013*"weeks" + 0.013*"Glaceau"
topic diff=0.105299, rho=0.447214
bound: at document #0
-7.473 per-word bound, 177.6 perplexity estimate based on a held-out corpus of 135 documents with 1065 words
PROGRESS: pass 4, at document #135/135
performing inference on a chunk of 135 documents
135/135 documents converged within 50 iterations
updating topics
topic #0 (0.040): 0.040*"Brown Betty" + 0.040*"Cloak" + 0.021*"cool" + 0.021*"NOW" + 0.021*"entertainment" + 0.021*"ugly" + 0.021*"Steve Cooley" + 0.021*"War" + 0.021*"idols" + 0.021*"OIC"
topic #20 (0.040): 0.028*"spiritual" + 0.015*"Private" + 0.015*"Words" + 0.015*"Public / Private" + 0.015*"Public" + 0.015*"hoy" + 0.014*"ones" + 0.014*"sleep" + 0.014*"rituals" + 0.014*"Tarot"
topic #2 (0.040): 0.023*"Aid" + 0.023*"State" + 0.023*"Jeff" + 0.023*"Jobless" + 0.023*"Malmberg" + 0.023*"evacuated" + 0.023*"Dallas-Fort Worth International Airport" + 0.023*"tcot" + 0.023*"DFW Airport" + 0.023*"People"
topic #17 (0.040): 0.122*"Marisela" + 0.122*"Escobedo" + 0.035*"Aquí está" + 0.035*"Chihuahua" + 0.035*"Río Pánuco" + 0.035*"burla" + 0.019*"justicia" + 0.019*"U.S" + 0.001*"urges" + 0.001*"Angelina Jolie"
topic #3 (0.040): 0.030*"Venezuela" + 0.030*"Chavez" + 0.030*"lawmakers" + 0.030*"decree" + 0.030*"power" + 0.030*"govern" + 0.025*"ppl" + 0.013*"who" + 0.013*"weeks" + 0.013*"Glaceau"
topic diff=0.079385, rho=0.408248
saving LdaState object under ../output/64/tml/gensim_25topics.model.state, separately None
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/64/tml/gensim_25topics.model.state'}
saved ../output/64/tml/gensim_25topics.model.state
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/64/tml/gensim_25topics.model.id2word'}
saving LdaModel object under ../output/64/tml/gensim_25topics.model, separately ['expElogbeta', 'sstats']
storing np array 'expElogbeta' to ../output/64/tml/gensim_25topics.model.expElogbeta.npy
not storing attribute dispatcher
not storing attribute id2word
not storing attribute state
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/64/tml/gensim_25topics.model'}
saved ../output/64/tml/gensim_25topics.model
topic #0 (0.040): 0.040*"Brown Betty" + 0.040*"Cloak" + 0.021*"cool" + 0.021*"NOW" + 0.021*"entertainment" + 0.021*"ugly" + 0.021*"Steve Cooley" + 0.021*"War" + 0.021*"idols" + 0.021*"OIC"
topic #1 (0.040): 0.019*"Interstate Compact" + 0.019*"remember me" + 0.019*"hope" + 0.019*"Technology" + 0.019*"Campus" + 0.019*"Excelsior College" + 0.019*"living" + 0.019*"Pre-Order" + 0.019*"for Dummies" + 0.019*"life"
topic #2 (0.040): 0.023*"Aid" + 0.023*"State" + 0.023*"Jeff" + 0.023*"Jobless" + 0.023*"Malmberg" + 0.023*"evacuated" + 0.023*"Dallas-Fort Worth International Airport" + 0.023*"tcot" + 0.023*"DFW Airport" + 0.023*"People"
topic #3 (0.040): 0.030*"Venezuela" + 0.030*"Chavez" + 0.030*"lawmakers" + 0.030*"decree" + 0.030*"power" + 0.030*"govern" + 0.025*"ppl" + 0.013*"who" + 0.013*"weeks" + 0.013*"Glaceau"
topic #4 (0.040): 0.023*"German" + 0.023*"Scrotum" + 0.023*"weave" + 0.023*"thread" + 0.023*"God" + 0.023*"will" + 0.023*"Proverb" + 0.022*"Snow" + 0.022*"air" + 0.022*"Single"
topic #5 (0.040): 0.029*"to win" + 0.029*"BBC" + 0.029*"lifetime" + 0.020*"activism" + 0.020*"VAW" + 0.020*"rape" + 0.020*"tcot" + 0.010*"I believe" + 0.010*"Bedfordshire" + 0.010*"tonight"
topic #6 (0.040): 0.023*"Lil Wayne" + 0.023*"id" + 0.013*"Captain Beefheart" + 0.012*"Dead Money" + 0.012*"Dead" + 0.012*"To 'Be With You" + 0.012*"David Banner" + 0.012*"Sleep Over" + 0.012*"you" + 0.012*"Lady Gaga"
topic #7 (0.040): 0.019*"spy" + 0.019*"Pakistan" + 0.019*"on line" + 0.019*"drone attacks" + 0.019*"customer.service" + 0.019*"iraq" + 0.019*"country" + 0.019*"ukuncut" + 0.019*"London" + 0.019*"GBC"
topic #8 (0.040): 0.036*"ma" + 0.020*"Elton" + 0.020*"you" + 0.020*"stuff" + 0.020*"Fans" + 0.020*"Rock" + 0.020*"Captain Beefheart" + 0.020*"Liz Phair" + 0.020*"concert" + 0.020*"tweets"
topic #9 (0.040): 0.045*"Captain Beefheart" + 0.036*"Don Van Vliet" + 0.036*"Dies" + 0.010*"Nice" + 0.010*"Artisan" + 0.010*"Simon" + 0.010*"Brindle" + 0.010*"press release" + 0.010*"Marketwire" + 0.010*"Medicine Hat"
topic #10 (0.040): 0.038*"Don't Ask Don't Tell" + 0.019*"old" + 0.019*"Lmao" + 0.019*"can" + 0.019*"Keri Hilson" + 0.019*"lol" + 0.019*"practice" + 0.019*"Repeal" + 0.019*"Donovan McNabb" + 0.019*"Song writing"
topic #11 (0.040): 0.056*"Tibet" + 0.021*"Occupied" + 0.014*"vehicle" + 0.014*"Witness" + 0.007*"trait" + 0.007*"evil" + 0.007*"Henry W. Longfellow" + 0.007*"lover" + 0.007*"think" + 0.007*"object"
topic #12 (0.040): 0.029*"This Week at War" + 0.029*"North Korea" + 0.029*"Afghanistan" + 0.001*"wikileaks" + 0.001*"urges" + 0.001*"pourrais" + 0.001*"Angelina Jolie" + 0.001*"prosecute" + 0.001*"tu" + 0.001*"puisse"
topic #13 (0.040): 0.029*"una más" + 0.029*"Reyes" + 0.029*"Cd  Juarez" + 0.029*"chick" + 0.029*"fat" + 0.015*"times" + 0.015*"HELP" + 0.015*"Fox News" + 0.015*"alread" + 0.015*"McCain's"
topic #14 (0.040): 0.011*"fasse" + 0.011*"puisse" + 0.011*"pourrais" + 0.011*"est" + 0.011*"din" + 0.011*"Sued" + 0.011*"pa" + 0.011*"na" + 0.011*"Mortgages" + 0.011*"Oui"
topic #15 (0.040): 0.040*"Gallo" + 0.040*"Knicks" + 0.027*"NBA" + 0.014*"WikiLeaks" + 0.014*"dunks" + 0.014*"Celts" + 0.014*"That's what I am" + 0.014*"Bron" + 0.014*"Craig Sager" + 0.014*"shooter"
topic #16 (0.040): 0.024*"tires" + 0.024*"beautiful experience" + 0.024*"Albert Einstein" + 0.024*"world" + 0.024*"Couchman" + 0.024*"as long as I live" + 0.024*"Fox News" + 0.024*"racist" + 0.024*"teabaggers" + 0.024*"thanks in advance"
topic #17 (0.040): 0.122*"Marisela" + 0.122*"Escobedo" + 0.035*"Aquí está" + 0.035*"Chihuahua" + 0.035*"Río Pánuco" + 0.035*"burla" + 0.019*"justicia" + 0.019*"U.S" + 0.001*"urges" + 0.001*"Angelina Jolie"
topic #18 (0.040): 0.019*"s voice" + 0.019*"tidal wave" + 0.019*"Ok" + 0.019*"Ireland" + 0.019*"Bosh" + 0.019*"lol" + 0.019*"Holocaust" + 0.019*"who" + 0.019*"Aston Villa" + 0.019*"to win"
topic #19 (0.040): 0.042*"LeBron" + 0.042*"tonight" + 0.042*"nickname" + 0.042*"basketball" + 0.022*"Sale" + 0.022*"Atomic" + 0.022*"Information" + 0.022*"Warehouse" + 0.022*"Scientists" + 0.022*"Nucleii"
topic #20 (0.040): 0.028*"spiritual" + 0.015*"Private" + 0.015*"Words" + 0.015*"Public / Private" + 0.015*"Public" + 0.015*"hoy" + 0.014*"ones" + 0.014*"sleep" + 0.014*"rituals" + 0.014*"Tarot"
topic #21 (0.040): 0.090*"video" + 0.016*"YouTube" + 0.016*"Roaming" + 0.016*"MASH-UP" + 0.016*"Voodoo" + 0.016*"click through" + 0.016*"Rolling In The Deep" + 0.016*"iPhone 4" + 0.016*"prices" + 0.016*"economist"
topic #22 (0.040): 0.152*"BBC" + 0.022*"snow" + 0.017*"UK" + 0.011*"blah" + 0.011*" blah" + 0.011*"am" + 0.011*"Conservative" + 0.011*"gop" + 0.011*"tcot" + 0.011*"hits"
topic #23 (0.040): 0.038*"fono" + 0.020*"Chopin" + 0.019*"Doors" + 0.010*"banned" + 0.010*"earth" + 0.010*"Cuba" + 0.010*"Wikileaks" + 0.010*"Michael Moore" + 0.010*"evolution" + 0.010*"Story"
topic #24 (0.040): 0.027*"Australia" + 0.027*"ready to rock" + 0.014*"U.S" + 0.014*"Congress" + 0.014*"WikiLeaks" + 0.014*"Greed" + 0.014*"Obama" + 0.014*"corruption" + 0.014*"national security" + 0.014*"DOJ"
TopicModeling: GENSIM Topic: 0 
Words: 0.040*"Brown Betty" + 0.040*"Cloak" + 0.021*"cool" + 0.021*"NOW" + 0.021*"entertainment" + 0.021*"ugly" + 0.021*"Steve Cooley" + 0.021*"War" + 0.021*"idols" + 0.021*"OIC"
TopicModeling: GENSIM Topic: 1 
Words: 0.019*"Interstate Compact" + 0.019*"remember me" + 0.019*"hope" + 0.019*"Technology" + 0.019*"Campus" + 0.019*"Excelsior College" + 0.019*"living" + 0.019*"Pre-Order" + 0.019*"for Dummies" + 0.019*"life"
TopicModeling: GENSIM Topic: 2 
Words: 0.023*"Aid" + 0.023*"State" + 0.023*"Jeff" + 0.023*"Jobless" + 0.023*"Malmberg" + 0.023*"evacuated" + 0.023*"Dallas-Fort Worth International Airport" + 0.023*"tcot" + 0.023*"DFW Airport" + 0.023*"People"
TopicModeling: GENSIM Topic: 3 
Words: 0.030*"Venezuela" + 0.030*"Chavez" + 0.030*"lawmakers" + 0.030*"decree" + 0.030*"power" + 0.030*"govern" + 0.025*"ppl" + 0.013*"who" + 0.013*"weeks" + 0.013*"Glaceau"
TopicModeling: GENSIM Topic: 4 
Words: 0.023*"German" + 0.023*"Scrotum" + 0.023*"weave" + 0.023*"thread" + 0.023*"God" + 0.023*"will" + 0.023*"Proverb" + 0.022*"Snow" + 0.022*"air" + 0.022*"Single"
TopicModeling: GENSIM Topic: 5 
Words: 0.029*"to win" + 0.029*"BBC" + 0.029*"lifetime" + 0.020*"activism" + 0.020*"VAW" + 0.020*"rape" + 0.020*"tcot" + 0.010*"I believe" + 0.010*"Bedfordshire" + 0.010*"tonight"
TopicModeling: GENSIM Topic: 6 
Words: 0.023*"Lil Wayne" + 0.023*"id" + 0.013*"Captain Beefheart" + 0.012*"Dead Money" + 0.012*"Dead" + 0.012*"To 'Be With You" + 0.012*"David Banner" + 0.012*"Sleep Over" + 0.012*"you" + 0.012*"Lady Gaga"
TopicModeling: GENSIM Topic: 7 
Words: 0.019*"spy" + 0.019*"Pakistan" + 0.019*"on line" + 0.019*"drone attacks" + 0.019*"customer.service" + 0.019*"iraq" + 0.019*"country" + 0.019*"ukuncut" + 0.019*"London" + 0.019*"GBC"
TopicModeling: GENSIM Topic: 8 
Words: 0.036*"ma" + 0.020*"Elton" + 0.020*"you" + 0.020*"stuff" + 0.020*"Fans" + 0.020*"Rock" + 0.020*"Captain Beefheart" + 0.020*"Liz Phair" + 0.020*"concert" + 0.020*"tweets"
TopicModeling: GENSIM Topic: 9 
Words: 0.045*"Captain Beefheart" + 0.036*"Don Van Vliet" + 0.036*"Dies" + 0.010*"Nice" + 0.010*"Artisan" + 0.010*"Simon" + 0.010*"Brindle" + 0.010*"press release" + 0.010*"Marketwire" + 0.010*"Medicine Hat"
TopicModeling: GENSIM Topic: 10 
Words: 0.038*"Don't Ask Don't Tell" + 0.019*"old" + 0.019*"Lmao" + 0.019*"can" + 0.019*"Keri Hilson" + 0.019*"lol" + 0.019*"practice" + 0.019*"Repeal" + 0.019*"Donovan McNabb" + 0.019*"Song writing"
TopicModeling: GENSIM Topic: 11 
Words: 0.056*"Tibet" + 0.021*"Occupied" + 0.014*"vehicle" + 0.014*"Witness" + 0.007*"trait" + 0.007*"evil" + 0.007*"Henry W. Longfellow" + 0.007*"lover" + 0.007*"think" + 0.007*"object"
TopicModeling: GENSIM Topic: 12 
Words: 0.029*"This Week at War" + 0.029*"North Korea" + 0.029*"Afghanistan" + 0.001*"wikileaks" + 0.001*"urges" + 0.001*"pourrais" + 0.001*"Angelina Jolie" + 0.001*"prosecute" + 0.001*"tu" + 0.001*"puisse"
TopicModeling: GENSIM Topic: 13 
Words: 0.029*"una más" + 0.029*"Reyes" + 0.029*"Cd  Juarez" + 0.029*"chick" + 0.029*"fat" + 0.015*"times" + 0.015*"HELP" + 0.015*"Fox News" + 0.015*"alread" + 0.015*"McCain's"
TopicModeling: GENSIM Topic: 14 
Words: 0.011*"fasse" + 0.011*"puisse" + 0.011*"pourrais" + 0.011*"est" + 0.011*"din" + 0.011*"Sued" + 0.011*"pa" + 0.011*"na" + 0.011*"Mortgages" + 0.011*"Oui"
TopicModeling: GENSIM Topic: 15 
Words: 0.040*"Gallo" + 0.040*"Knicks" + 0.027*"NBA" + 0.014*"WikiLeaks" + 0.014*"dunks" + 0.014*"Celts" + 0.014*"That's what I am" + 0.014*"Bron" + 0.014*"Craig Sager" + 0.014*"shooter"
TopicModeling: GENSIM Topic: 16 
Words: 0.024*"tires" + 0.024*"beautiful experience" + 0.024*"Albert Einstein" + 0.024*"world" + 0.024*"Couchman" + 0.024*"as long as I live" + 0.024*"Fox News" + 0.024*"racist" + 0.024*"teabaggers" + 0.024*"thanks in advance"
TopicModeling: GENSIM Topic: 17 
Words: 0.122*"Marisela" + 0.122*"Escobedo" + 0.035*"Aquí está" + 0.035*"Chihuahua" + 0.035*"Río Pánuco" + 0.035*"burla" + 0.019*"justicia" + 0.019*"U.S" + 0.001*"urges" + 0.001*"Angelina Jolie"
TopicModeling: GENSIM Topic: 18 
Words: 0.019*"s voice" + 0.019*"tidal wave" + 0.019*"Ok" + 0.019*"Ireland" + 0.019*"Bosh" + 0.019*"lol" + 0.019*"Holocaust" + 0.019*"who" + 0.019*"Aston Villa" + 0.019*"to win"
TopicModeling: GENSIM Topic: 19 
Words: 0.042*"LeBron" + 0.042*"tonight" + 0.042*"nickname" + 0.042*"basketball" + 0.022*"Sale" + 0.022*"Atomic" + 0.022*"Information" + 0.022*"Warehouse" + 0.022*"Scientists" + 0.022*"Nucleii"
TopicModeling: GENSIM Topic: 20 
Words: 0.028*"spiritual" + 0.015*"Private" + 0.015*"Words" + 0.015*"Public / Private" + 0.015*"Public" + 0.015*"hoy" + 0.014*"ones" + 0.014*"sleep" + 0.014*"rituals" + 0.014*"Tarot"
TopicModeling: GENSIM Topic: 21 
Words: 0.090*"video" + 0.016*"YouTube" + 0.016*"Roaming" + 0.016*"MASH-UP" + 0.016*"Voodoo" + 0.016*"click through" + 0.016*"Rolling In The Deep" + 0.016*"iPhone 4" + 0.016*"prices" + 0.016*"economist"
TopicModeling: GENSIM Topic: 22 
Words: 0.152*"BBC" + 0.022*"snow" + 0.017*"UK" + 0.011*"blah" + 0.011*" blah" + 0.011*"am" + 0.011*"Conservative" + 0.011*"gop" + 0.011*"tcot" + 0.011*"hits"
TopicModeling: GENSIM Topic: 23 
Words: 0.038*"fono" + 0.020*"Chopin" + 0.019*"Doors" + 0.010*"banned" + 0.010*"earth" + 0.010*"Cuba" + 0.010*"Wikileaks" + 0.010*"Michael Moore" + 0.010*"evolution" + 0.010*"Story"
TopicModeling: GENSIM Topic: 24 
Words: 0.027*"Australia" + 0.027*"ready to rock" + 0.014*"U.S" + 0.014*"Congress" + 0.014*"WikiLeaks" + 0.014*"Greed" + 0.014*"Obama" + 0.014*"corruption" + 0.014*"national security" + 0.014*"DOJ"
TopicModeling: Coherences:

TopicModeling: Calculating model coherence:

Setting topics to those of the model: LdaModel(num_terms=816, num_topics=25, decay=0.5, chunksize=2000)
TopicModeling: Coherence value is: -15.509866914232305
TopicModeling: Topic coherences are: [-14.623501714434843, -16.206740540880197, -15.750551168430825, -12.828392632658955, -13.707426422848481, -18.22012026519523, -0.3117074698997302, 1.350000111608376e-10, -15.24284237231064, -0.7935717450749212, -8.177919790013318, -12.135801317277512, -13.681657614478905, -18.230098229139994, -15.963579557159504, -4.814857071311984, -13.681657614481898, -12.35735585472323, -18.939918188488505, -12.592965770159152, -14.676287514359412, -16.549689164055696, -15.621606014392281, -11.615381461317845, -14.72608980760788]
saving Dictionary object under ../output/64/tml/gensim_25topics_TopicModelingDictionary.mm, separately None
{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'wb', 'uri': '../output/64/tml/gensim_25topics_TopicModelingDictionary.mm'}
saved ../output/64/tml/gensim_25topics_TopicModelingDictionary.mm
UserSimilarity: Topic modeling done
UserSimilarity: All users size 135
UserSimilarity: All distinct users:135
UserSimilarity: users_topic_interests=(135, 25)
UserSimilarity: Just one topic? False, Binary topic? True, Threshold: 0.2
135 users has twitted in 2010-12-17 00:00:00
NumExpr defaulting to 4 threads.
UserSimilarity: 0 / 1
UserSimilarity: UsersTopicInterests.npy is saved for day:0 with shape: (135, 25)
UsersGraph: There are 135 users on 0
UserSimilarity: A graph is being created for 0 with 135 users
UserSimilarity: Number of users per day: [135]
UserSimilarity: Graphs created!
UserSimilarity: Graphs are written in "graphs" directory
Graph Clustering: Louvain clustering for ../output/64/uml/graphs\01.net
nodes: 135 / edges: 512 / isolates: 0
Graph Clustering: Louvain clustering output: 22 clusters. 22 of them are multi-user clusters and rest of them (0) are singleton clusters.

Graph Clustering: Length of multi-user clusters: [20, 11, 8, 8, 8, 7, 7, 6, 6, 6, 5, 5, 5, 5, 5, 4, 4, 3, 3, 3, 3, 2]

Graph Clustering: UserClusters.npy saved.

Cluster 0 has 20 users. Topic 3 is the favorite topic for 55.00000000000001% of users.
Cluster 1 has 11 users. Topic 11 is the favorite topic for 63.63636363636363% of users.
Cluster 2 has 8 users. Topic 24 is the favorite topic for 100.0% of users.
Cluster 3 has 8 users. Topic 22 is the favorite topic for 100.0% of users.
Cluster 4 has 8 users. Topic 13 is the favorite topic for 100.0% of users.
Cluster 5 has 7 users. Topic 5 is the favorite topic for 100.0% of users.
Cluster 6 has 7 users. Topic 4 is the favorite topic for 100.0% of users.
Cluster 7 has 6 users. Topic 1 is the favorite topic for 100.0% of users.
Cluster 8 has 6 users. Topic 15 is the favorite topic for 100.0% of users.
Cluster 9 has 6 users. Topic 6 is the favorite topic for 100.0% of users.
Cluster 10 has 5 users. Topic 8 is the favorite topic for 100.0% of users.
Cluster 11 has 5 users. Topic 21 is the favorite topic for 100.0% of users.
Cluster 12 has 5 users. Topic 14 is the favorite topic for 100.0% of users.
Cluster 13 has 5 users. Topic 23 is the favorite topic for 100.0% of users.
Cluster 14 has 5 users. Topic 18 is the favorite topic for 100.0% of users.
Cluster 15 has 4 users. Topic 0 is the favorite topic for 100.0% of users.
Cluster 16 has 4 users. Topic 10 is the favorite topic for 100.0% of users.
Cluster 17 has 3 users. Topic 16 is the favorite topic for 100.0% of users.
Cluster 18 has 3 users. Topic 20 is the favorite topic for 100.0% of users.
Cluster 19 has 3 users. Topic 2 is the favorite topic for 100.0% of users.
Cluster 20 has 3 users. Topic 19 is the favorite topic for 100.0% of users.
Cluster 21 has 2 users. Topic 7 is the favorite topic for 100.0% of users.
Cluster 22 has 1 users. Topic 12 is the favorite topic for 100.0% of users.
top of axes not in the figure, so title not moved
top of axes not in the figure, so title not moved

NewsTopicExtraction.py:

Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
Calling https://tagme.d4science.org/tagme/tag
